[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH3823 Generalised Linear Models",
    "section": "",
    "text": "Weekly schedule\nItems will be added here week-by-week and so keep checking when you need up-to-date information on what you should be doing."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "MATH3823 Generalised Linear Models",
    "section": "",
    "text": "Some sections will be left as directed reading, but please note that material in all sections is examinable.↩︎"
  },
  {
    "objectID": "0_preface.html",
    "href": "0_preface.html",
    "title": "Overview",
    "section": "",
    "text": "Official Module Description"
  },
  {
    "objectID": "0_preface.html#preface",
    "href": "0_preface.html#preface",
    "title": "Overview",
    "section": "Preface",
    "text": "Preface\nThese lecture notes are produced for the University of Leeds module MATH3823 - Generalised Linear Models for the academic year 2024-25. Please note that this material also forms part of the module MATH5824 - Generalised Linear and Additive Models. They are based on the lecture notes used previously for this module and I am grateful to the previous module leader Robert Aykroyd for sharing his notes and the considerable effort he made in developing these. A PDF version will be made available on the University of Leeds Minerva system. This will be my first year teaching this module, so I encourage you to offer feedback that can be used to improve the content throughout the semester.\nRP Mann, Leeds, January 6, 2025"
  },
  {
    "objectID": "0_preface.html#generative-ai-usage-within-this-module",
    "href": "0_preface.html#generative-ai-usage-within-this-module",
    "title": "Overview",
    "section": "Generative AI usage within this module",
    "text": "Generative AI usage within this module\nThe assessments for this module fall in the red category for using Generative AI which means you must not use Generative AI tools. The purpose and format of the assessments makes it inappropriate or impractical for AI tools to be used.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStatistical ethics and sensitive data\nPlease note that from time to time we will be using data sets from situations which some might perceive as sensitive. All such data sets will, however, be derived from real-world studies which appear in textbooks or in scientific journals. The daily work of many statisticians involves applying their professional skills in a wide variety of situations and as such it is important to include a range of commonly encountered examples in this module. Whenever possible, sensitive topics will be signposted in advance. If you feel that any examples may be personally upsetting then, if possible, please contact the module lecturer in advance. If you are significantly effected by any of these situations, then you can seek support from the Student Counselling and Wellbeing service."
  },
  {
    "objectID": "0_preface.html#module-summary",
    "href": "0_preface.html#module-summary",
    "title": "Overview",
    "section": "Module summary",
    "text": "Module summary\nLinear regression is a tremendously useful statistical technique but is very limited. Generalised linear models extend linear regression in many ways - allowing us to analyse more complex data sets. In this module we will see how to combine continuous and categorical predictors, analyse binomial response data and model count data."
  },
  {
    "objectID": "0_preface.html#objectives",
    "href": "0_preface.html#objectives",
    "title": "Overview",
    "section": "Objectives",
    "text": "Objectives\nOn completion of this module, students should be able to:\n\ncarry out regression analysis with generalised linear models including the use of link functions;\nunderstand the use of deviance in model selection;\nappreciate the problems caused by overdispersion;\nfit and interpret the special cases of log linear models and logistic regression;\nuse a statistical package with real data to fit these models to data and to write a report giving and interpreting the results."
  },
  {
    "objectID": "0_preface.html#syllabus",
    "href": "0_preface.html#syllabus",
    "title": "Overview",
    "section": "Syllabus",
    "text": "Syllabus\nGeneralised linear model; probit model; logistic regression; log linear models."
  },
  {
    "objectID": "0_preface.html#university-module-catalogue",
    "href": "0_preface.html#university-module-catalogue",
    "title": "Overview",
    "section": "University Module Catalogue",
    "text": "University Module Catalogue\nFor any further details, please see MATH3823 Module Catalogue page"
  },
  {
    "objectID": "1_intro.html#overview",
    "href": "1_intro.html#overview",
    "title": "1  Introduction",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nIn previous modules you have studied linear models with a normally distributed error term, such as simple linear regression, multiple linear regression and ANOVA for normally distributed observations. In this module we will study generalised linear models.\nOutline of the module:\n\nRevision of linear models with normal errors.\nIntroduction to generalised linear models, GLMs.\nLogistic regression models.\nLoglinear models, including contingency tables.\n\n\n\n\n\n\n\nImportant\n\n\n\nThis module will make extensive use of \\(\\mathbf{R}\\) and hence it is very important that you are comfortable with its use. If you need some revision, then material is available on Minerva under RStudio Support.\n\n\nThe purpose of a generalised linear model is to describe the dependence of a response variable \\(y\\) on a set of \\(p\\) explanatory variables \\(\\b{x}=(x_1, x_2, \\ldots, x_p)\\) where, conditionally on \\(\\b{x}\\), observation \\(y\\) has a distribution which is not necessarily normal. Note that the normal distribution situation is a special case of the general framework and we will study that in the next Chapter.\nPlease be aware that in this learning material we may use lowercase letters, for example \\(y\\) or \\(y_i,\\) to denote both observed values or random variables, which is being considered should be clear from the context.\n\n\n\n\n\n\nImportant\n\n\n\nThis module will make extensive use of many basic ideas from statistics. If you need some revision, then see Appendix A: Basic material on Minerva under Basic Pre-requisite Material."
  },
  {
    "objectID": "1_intro.html#motivating-example",
    "href": "1_intro.html#motivating-example",
    "title": "1  Introduction",
    "section": "1.2 Motivating example",
    "text": "1.2 Motivating example\nTable 1.1 shows data1 on the number of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide.\n\n\nTable 1.1: Numbers of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide\n\n\n\n\n\n\n\nDose\n\\(x_i\\)\nNo. of beetle\n\\(m_i\\)\nNo. killed\n\\(y_i\\)\n\n\n\n\n1.6907\n59\n6\n\n\n1.7242\n60\n13\n\n\n1.7552\n62\n18\n\n\n1.7842\n56\n28\n\n\n1.8113\n63\n52\n\n\n1.8369\n59\n53\n\n\n1.8610\n62\n61\n\n\n1.8839\n60\n60\n\n\n\n\nFigure 1.1 (a) shows the same data with a linear regression line superimposed. Although this line goes close to the plotted points, we can see some fluctuations around it. More seriously, this is a stupid model: it would predict a mortality rate of greater than 100% at a dose of 1.9 units, and a negative mortality rate at 1.65 units!\n\n\nCode\npar(mar=c(4,4,0,1))\n\nbeetle = read.table(\"https://richardpmann.com/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16, col=\"blue\",\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\n# Fit a linear model\nlm.fit = lm(mortality ~ dose)\nabline(lm.fit, col=\"red\")\n\n# Fit a logisitc model\nplot(dose, mortality, pch=16, col=\"blue\",\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\ny = cbind(beetle$died, beetle$total-beetle$died)\nglm.fit = glm(y ~ dose, family=binomial(link='logit'))\n\noutput.dose = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=output.dose), type=\"response\")\nlines(output.dose, fitted, col=\"red\")\n\n\n\n\n\n\n\n\n(a) Linear model\n\n\n\n\n\n\n\n(b) Logistic model\n\n\n\n\nFigure 1.1: Beetle mortality rates with fitted dose- response curves.\n\n\n\nA more sensible dose–response relationship for the beetle mortality data might be based on the logistic function (to be defined later), as plotted in Figure 1.1 (b). The resulting curve is a closer, more-sensible, fit. Later in this module we will see how this curve was fitted using maximum likelihood estimation for an appropriate generalised linear model.\nThis is an example of a dose-response experiment which are widely used in medical and pharmaceutical situations.\n\n\n\n\n\n\nWarning\n\n\n\nWarning of potentially sensitive material. For further information on dose-response experiments see, for example, www.britannica.com/science/dose-response-relationship."
  },
  {
    "objectID": "1_intro.html#focus-on-correlation-quiz",
    "href": "1_intro.html#focus-on-correlation-quiz",
    "title": "1  Introduction",
    "section": "Focus on correlation quiz",
    "text": "Focus on correlation quiz\nTest your knowledge recall and comprehension to reinforce idea of linear relationships and correlation.\n\n\nFor each situation, choose one of the following statements which you think is most likely to apply.\n\nThe diastolic blood pressure and the weight of patients attending a heart health clinic at the Leeds General Infirmary. positive correlationuncorrelatednegative correlationother\nThe daily stock market closing prices of British Telecom and Virgin Media shares on the London Stock Exchange. positive correlationuncorrelatednegative correlationother\nThe daily rainfall and hours of sunshine collected at a weather monitoring station in the Pennines of Yorkshire. positive correlationuncorrelatednegative correlationother\nThe number of road accidents occurring at a busy roundabout and the UK Retail Prices Index. positive correlationuncorrelatednegative correlationother\n\n\n\n\nClick here to see explanations\n\n\nIt is well documented that people who are over-weight (or at least high BMI) are more likely to have high blood pressure. This is true for systolic and diastolic blood pressure as well as for men and women. It is not certain if the relationship will be linear, but it will lead to a strong positive correlation value. This is likely to be a causal relationship, excess weight causes high blood pressure.\nAlthough these two companies are in the same business sectors, technology and entertainment, it is unlikely that direct competition will be the main factor in the relative behaviour – if it were, then we might expect a negative correlation, that is one does well if the other does badly. Instead, they are both likely to be driven by the same economic and social trends. This is not a causal relationship but both are being driven by an (unseen) third variable. It does, however, lead to a correlation.\nAlthough both weather features, rainfall and sun, can happen at the same time – perhaps leading to a rainbow – and there are very many cases when neither occurs – for example a cloudy but dry day – there is a general pattern that it will not rain when it is sunny and it will not be sunny when it rains. This leads to a negative correlation and a moderate value is likely even if the relationship is not linear. Again, this is not a causal relationship but is being driven, perhaps, by the presence of clouds.\nIt is hard to imagine that there will be a relationship between the number of road accidents and an inflation measure, hence a value of correlation close to zero – though do not expect to ever get a value of exactly zero. It is not completely, inconceivable that the number of road accidents will be higher when the economy is active, but this is unlikely to lead to a substantial correlation value – if you know otherwise, let me know!\n\n\n\n\n\nFrom each of the following scatter plots, choose from the drop down list which correlation value is most likely.\n\nWhat is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.06. What is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.07. What is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.08. What is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.09. What is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.010. What is the most likely correlation for the data below? -1.0-0.7-0.30.0+0.3+0.7+1.0\n\n\n\n\nClick here to see explanations\n\n\nImagine dividing the plot by horizontal and vertical lines at the respective mean values. There would be a majority of points in the top-right and bottom-left indicating a positive correlation, but there are still some on the other quadrants. Hence, +1 is too high and 0.3 is too low, but would be suitable if the points were closer to a line or more dispersed, respectively. For information, the exact value is 0.70.\nAgain, imagine dividing the plot by horizontal and vertical lines at the respective mean values. This time the majority of points would be in the top-left and bottom-right quadrants, and there is moderate spread. A value -1 is too extreme and -0.3 is too close to zero, but would be suitable if the points were closer to a line or more dispersed, respectively. For information, the exact value is -0.60.\nA similar situation to Question 6, but there is noticeably more spread. Although the majority of points are in the top-left and bottom-right quadrants there are substantial numbers in the other quadrants. It would be inaccurate to say that this shows uncorrelated variables as there is a definite negative slope to the pattern. For information, the exact value is -0.30.\nThis is a difficult one as there is a clear relationship, but it is quadratic rather than linear. For information, the exact value is 0.30. Perhaps without the accompanying graph, this correlation value would be misleading.\nDividing the plot by horizontal and vertical lines at the respective mean values leaves very similar numbers of points in al four quadrants. This indicates that the correlation will be close to zero – here is no relationship. For information, the exact value is 0.01.\nAlmost all of the points are in the top-left and bottom-right quadrants indicating a negative correlation. The points are very close to the linear and hence a value close to -1 is likely – such extreme cases are rare. For information, the exact value is -0.995."
  },
  {
    "objectID": "1_intro.html#revision-of-least-squares-estimation",
    "href": "1_intro.html#revision-of-least-squares-estimation",
    "title": "1  Introduction",
    "section": "1.3 Revision of least-squares estimation",
    "text": "1.3 Revision of least-squares estimation\nSuppose that we have \\(n\\) paired data values \\((x_1, y_1),\\dots, (x_n, y_n)\\) and that we believe these are related by a linear model\n\\[\ny_i = \\alpha+\\beta x_i +\\epsilon_i\n\\]\nfor all \\(i\\in \\{1, 2,\\dots,n\\}\\), where \\(\\epsilon_1,\\dots,\\epsilon_n\\) are independent and identically distributed (iid) with \\(\\mbox{E}[\\epsilon_i]=0\\) and \\(\\mbox{Var}[\\epsilon_i]=\\sigma^2\\). The aim will be to find values of the model parameters, \\(\\alpha, \\beta \\text{ and } \\sigma^2\\) using the data. Specifically, we will estimate \\(\\alpha\\) and \\(\\beta\\) using the values which minimize the residual sum of squares (RSS)\n\\[\nRSS(\\alpha, \\beta) = \\sum_{i=1}^n \\left(y_i-(\\alpha+\\beta x_i)\\right)^2.\n\\tag{1.1}\\]\nThis measures how close the data points are around the regression line and hence the resulting estimates, \\(\\hat\\alpha\\) and \\(\\hat\\beta\\), will give us a fitted regression line which is closest to the data.\nFigure 1.2 illustrates this process. The data points are fixed but we are free to choose values for \\(\\alpha\\) and \\(\\beta\\), that is to move the line up and down and to rotate it as needed. In general, the data points, however, do not sit exactly on any line. For any values of \\(\\alpha\\) and \\(\\beta\\) the value on the line \\(y=\\alpha+\\beta x\\) can be calculated and the discrepancy, as measured in the vertical direction, is defined as the residual, \\(r_i=y_i-\\left(\\alpha+\\beta x_i\\right)\\). Then, the residual sum of squares is formed as the sum of the squares of these individual residuals.\n\n\n\n\n\nFigure 1.2: Diagram showing linear regression method\n\n\n\n\nIt can be shown that Equation 1.1 takes its minimum when the parameters are given by\n\\[\n\\hat\\alpha = \\bar y -\\hat\\beta\\bar x, \\quad \\mbox{and} \\quad\n\\hat\\beta = \\frac{s_{xy}}{s^2_x}\n\\tag{1.2}\\]\nwhere \\(\\bar x\\) and \\(\\bar y\\) are the sample means,\n\\[\ns_{xy}=\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y)\n\\]\nis the sample covariance and\n\\[\ns^2_x = \\frac{1}{n-1} \\sum_{i=1}^n (x_i-\\bar x)^2\n\\]\nis the sample variance of the \\(x\\) values. It can be shown that these estimators are unbiased, that is \\(\\mbox{E}[\\hat\\alpha]=\\alpha\\) and \\(\\mbox{E}[\\hat\\beta]=\\beta\\) – see Section 1.5.\nThe fitted regression lines is then given by \\(\\hat y = \\hat \\alpha +\\hat \\beta x\\), the fitted values by \\(\\hat y_i = \\hat \\alpha +\\hat \\beta x_i\\), and the model residuals by \\(r_i= \\hat \\epsilon_i= y_i-\\hat y_i\\) for all \\(i\\in \\{1,\\dots,n\\}.\\)\nTo complete the model fitting, we also estimate the error variance, \\(\\sigma^2\\), using \\[\n\\hat \\sigma^2 = \\frac{1}{n-2} \\sum _{i=1}^n r_i^2.\n\\tag{1.3}\\]\nNote that, by construction, \\(\\bar r=0\\) and, further, it can be shown that \\(\\hat \\sigma^2\\) is an unbiased estimator of \\(\\sigma^2\\), that is \\(\\mbox{E}[\\hat\\sigma^2]=\\sigma^2\\).\n\n\nCode\nxbar = mean(dose)\nybar = mean(mortality)\ns2x = var(dose)\nsxy = cov(dose, mortality)\n\nbetahat = sxy/s2x\nalphahat = ybar-betahat*xbar\n\ns2hat = sum((mortality-alphahat-betahat*dose)^2)/(length(dose)-2)\n\n\nReturning to the above beetle data example, we have \\(\\hat\\alpha=\\)-8.947843, \\(\\hat\\beta=\\) 5.324937, and \\(\\hat \\sigma^2 =\\) 0.0075151.\nWe will interpret the output later, but in \\(\\b{R}\\), the fitting can be done with a single command with corresponding fitting output from a second command:\n\n\nCode\nlm.fit = lm(mortality ~ dose)\n\nsummary(lm.fit)\n\n\n\nCall:\nlm(formula = mortality ~ dose)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.10816 -0.06063  0.00263  0.05119  0.12818 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -8.9478     0.8717  -10.27 4.99e-05 ***\ndose          5.3249     0.4857   10.96 3.42e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08669 on 6 degrees of freedom\nMultiple R-squared:  0.9524,    Adjusted R-squared:  0.9445 \nF-statistic: 120.2 on 1 and 6 DF,  p-value: 3.422e-05\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou should have met \\(\\b{R}\\) output like this in previous statistics modules, but if you need some revision then see Appendix-C: Background to Analysis of Variance on Minerva under Basic Pre-requisite Material."
  },
  {
    "objectID": "1_intro.html#sec-typevariables",
    "href": "1_intro.html#sec-typevariables",
    "title": "1  Introduction",
    "section": "1.4 Types of variables",
    "text": "1.4 Types of variables\nThe way a variable enters a model will depends on its type. The most common five types of variable are:\n\nQuantitative\n\nContinuous: for example, height; weight; duration. Real valued. Note that although recorded data is rounded it is still usually best regarded as continuous.\nCount (discrete): for example, number of children in a family; accidents at a road junction; number of items sold. Non-negative and integer-valued.\n\nQualitative\n\nOrdered categorical (ordinal): for example, severity of illness (Mild/ Moderate/Severe); degree classification (first/ upper-second/ lower-second/ third).\nUnordered categorical (nominal):\n\nDichotomous (binary): two categories: for example sex (M/ F); agreement (Yes/ No); coin toss (Head/ Tail).\nPolytomous (also known as polychotomous): more than two categories: for example blood group (A/ B/ O); eye colour (Brown/ Blue/ Green).\n\n\n\nNote that although dichotomous is clearly a special case of polytomous, making the distinction is usually worthwhile as it often leads to a simplified modelling and testing approach."
  },
  {
    "objectID": "1_intro.html#focus-on-data-type-quiz",
    "href": "1_intro.html#focus-on-data-type-quiz",
    "title": "1  Introduction",
    "section": "Focus on data type quiz",
    "text": "Focus on data type quiz\n\n\nTest your knowledge recall and comprehension to reinforce ideas ready for later in the module\nFor each of the following situations what is the most appropriate data type: nominal, ordinal, discrete, or continuous?\n\nThe eye colour of 100 patients visiting the Yorkshire Cancer Research Centre, for example, grey, green, brown, blue…. nominalordinaldiscretecontinuous\nThe nationality of students at the University of Leeds, for example, British, Chinese, Greek, Indian…. nominalordinaldiscretecontinuous\nThe five-star ratings submitted by 50 customers on TripAdvisor for the Leeds Queens Hotel, for example 1 star, 2 star,… 5 star. nominalordinaldiscretecontinuous\nThe diastolic blood pressure of 20 male and 20 female patients attending a heart health clinic at the Leeds General Infirmary in a study to investigate differences between men and women, for example, 80 mm Hg, 130 mm Hg,… nominalordinaldiscretecontinuous\nThe daily stock market closing price of British Telecom shares on the London Stock Exchange over a year to study the change over time, for example, 114.75p, 115.10p,… nominalordinaldiscretecontinuous\nThe January monthly rainfall collected since 1961 at a weather monitoring station in the Pennines of Yorkshire, for example, 8 mm , 12 mm,… nominalordinaldiscretecontinuous\nThe level of satisfaction of 100 randomly chosen voters with the policies of a political party, for example, agree, fully agree, neither agree nor disagree, disagree, and fully disagree. nominalordinaldiscretecontinuous\nThe number of new people following the TheRoyalFamily twitter page per day over a year, for example 459, 700,… to study the change due to a royal wedding. nominalordinaldiscretecontinuous\nThe number of road accidents occurring per month, at a busy roundabout over a 10-year period to study the change over time, for example, 0, 1, 2,… nominalordinaldiscretecontinuous\nThe number of Scottish strawberries in 50 randomly selected boxes bought from ASDA supermarket, for example, 50, 58, 68,… nominalordinaldiscretecontinuous\n\n\n\n\nClick here to see explanations\n\n\nEye colour is qualitative and can take any one of an unordered set of categories. Although the eye colours are categories, there is no clear ordering to the colours.\nNationality is qualitative and can take any one of an unordered set of categories. Although the nationality are categories, there is no clear ordering to the countries.\nThe rating is qualitative and can take any one of set of categories but the categories are clearly ordered, 5 star is better than 4 start etc. Although the ratings are represented by integers, there is no reason why the difference between 1 and 2 stars has the same interpretation as between 4 and 5 stars and hence it cannot be discrete.\nAlthough the recorded values might take only integer values, blood pressure is a measurement and could take be any real number.\nShare price is a measurement and could be any real number, even though in practice it will be rounded.\nRainfall is a measurement and although the recorded values might take only integer values, rainfall could be any real number.\nSatisfaction score is qualitative and can take any one of set of categories but the categories are clearly ordered, fully agree is better than agree etc. Although the scores could be represented by numerical values, e.g. 1,2,3,4,5, there is no reason why the difference between 1 and 2 has the same interpretation as between 4 and 5 and hence it is not discrete.\nThe number of people is a quantitative count which is limited to the non-negative integers. The variable is discrete.\nThe number of accidents is a quantitative count, being limited to the non-negative integers and hence is discrete.\nThe number of strawberries is a quantitative a count which is limited to the non-negative integers. The variable is discrete."
  },
  {
    "objectID": "1_intro.html#sec-exercises1",
    "href": "1_intro.html#sec-exercises1",
    "title": "1  Introduction",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\n\n\n\n\n\nImportant\n\n\n\nUnless otherwise stated, data files will be available online at: richardpmann.com/MATH3823/Datasets/filename.ext, where filename.ext is the stated filename with extension. For example, for the first exercise:\nbeetle = read.table(“https://richardpmann.com/MATH3823/Datasets/beetle.txt”, header=T)\n\n\n\n1.1 Consider again the beetle data in Table 1.1. Perform the calculations by hand and then check the answers using \\(\\b{R}\\) – a copy of the data is available in the file beetle.txt. Finally plot the fitted regression line on a scatter plot of the data.\n\n\nClick here to see hints.\n\nSee the code chunk used to produce Figure 1.1.\n\n1.2 Consider the following synthetic data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i=1\\)\n\\(i=2\\)\n\\(i=3\\)\n\\(i=4\\)\n\\(i=5\\)\n\\(i=6\\)\n\\(i=7\\)\n\\(i=8\\)\n\n\n\n\n\\(x_i\\)\n-1\n0\n1\n2\n2.5\n3\n4\n6\n\n\n\\(y_i\\)\n-2.8\n-1.1\n7.2\n8.0\n8.9\n9.2\n14.8\n24.7\n\n\n\nPlot the data to check that a linear model is suitable and then fit a linear regression model. Do you think that the fitted model can be reliably used to predict the values of \\(y\\) when \\(x=5\\) and \\(x=10\\)? Justify your answers.\n\n\nClick here to see hints.\n\nWhich is more reliable prediction for a value within the range of the data (interpolation) or outside the range of the data (extrapolation)?\n\n1.3 Starting from Equation 1.1, derive the estimation equations given in Equation 1.2. Show that \\(\\hat\\alpha\\) and \\(\\hat\\beta\\) are unbiased estimators of \\(\\alpha\\) and \\(\\beta\\). What can be said about \\(\\hat\\sigma^2\\) as an estimator of \\(\\sigma^2\\)?\n\n\nClick here to see hints.\n\nFor unbiasedness of intercept and slope check your MATH1712 lecture notes. For the latter, there is a careful theoretical proof about the variance parameter, but here only an intuitive explanation is expected.\n\n1.4 The Brownlee’s Stack Loss Plant Data2 is already available in \\(\\mathbf{R}\\), with background details on the help page, \\(\\texttt{?stackloss}\\).\nAfter plotting all pairs of variables, which of \\(\\texttt{Air.Flow}\\), \\(\\texttt{Water.Temp}\\) and \\(\\texttt{Acid.Conc}\\) do you think could be used to model \\(\\texttt{stack.loss}\\) using a linear regression? Justify your answer.\nPerform a simple linear regression with using \\(\\texttt{stack.loss}\\) as the response variable and your chosen variable as the explanatory variable. Add the fitted regression line to a scatter plot of the data and comment.\n\n\nClick here to see hints.\n\nYou already met this example in MATH1712 – check your lecture note for guidance.\n\n1.5 In an experiment conducted by de Silva et al. in 20203 data was obtained to investigate falling objects and gravity, as first consider by Galileo and Newton. A copy of the data is available in the file physics_from_data.csv.\nSuppose that we wish to develop a method to predict the maximum Reynolds number from a single explanatory variable. Which of the variables do you think helps explain Reynolds number the best? Why do you think this?\n\n\nClick here to see hints.\n\nRead the data into \\(\\b{R}\\) and perform a simple linear regression of the maximum Reynolds number as the response variable and, in turn, each of the other variables as the explanatory variable. Plot the data, with and add the corresponding fitted linear models."
  },
  {
    "objectID": "1_intro.html#footnotes",
    "href": "1_intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Dobson and Barnett, 3rd edn, p.127↩︎\nBrownlee, K. A. (1960, 2nd ed. 1965) Statistical Theory and Methodology in Science and Engineering. New York: Wiley. pp. 491–500.↩︎\nde Silva BM, Higdon DM, Brunton SL, Kutz JN. Discovery of Physics From Data: Universal Laws and Discrepancies. Front Artif Intell. 2020 Apr 28;3:25. doi: 10.3389/frai.2020.00025. PMID: 33733144; PMCID: PMC7861345.↩︎"
  },
  {
    "objectID": "2_linearmodels.html#sec-essentialsoverview",
    "href": "2_linearmodels.html#sec-essentialsoverview",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nIn many fields of application, we might assume the response variable is normally distributed. For example: heights, weights, log prices, etc.\nThe data1 in Table 2.1 record the birth weights of 12 girls and 12 boys and their gestational ages (time from conception to birth).\n\n\nTable 2.1: Gestational ages (in weeks) and birth weights (in grams) for 24 babies (12 girls and 12 boys).\n\n\n\n\n(a) Girls\n\n\nGestational Age\nBirth weight\n\n\n\n\n40\n3317\n\n\n36\n2729\n\n\n40\n2935\n\n\n37\n2754\n\n\n42\n3210\n\n\n39\n2817\n\n\n40\n3126\n\n\n37\n2539\n\n\n36\n2412\n\n\n38\n2991\n\n\n39\n2875\n\n\n40\n3231\n\n\n\n\n\n\n(b) Boys\n\n\nGestational Age\nBirth weight\n\n\n\n\n40\n2968\n\n\n38\n2795\n\n\n40\n3163\n\n\n35\n2925\n\n\n36\n2625\n\n\n37\n2847\n\n\n41\n3292\n\n\n40\n3473\n\n\n37\n2628\n\n\n38\n3176\n\n\n40\n3421\n\n\n38\n3975\n\n\n\n\n\n\nA key question is, can we predict the birth weight of a baby born at a given gestational age using these data. For this we will need to make assumptions about the relationship between birth weight and gestational age, and any associated natural variation – that is we require a model.\nFirst we should explore the data. Figure 2.1 (a) shows a histogram of the birth weights indicating a spread around modal group 2800-3000 grams; Figure 2.1 (b) indicates slightly higher birth weights for the boys than the girls; and Figure 2.1 (c) shows an increasing relationship between weight and age. Together, these suggest that gestational age and sex are likely to be important for predicting weight.\n\n\nCode\npar(mar=c(4,4,0,1))\n\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\nhist(weight, breaks=6, probability = T, main = \"\", \n     xlab = \"Birth weight (grams)\")\nboxplot(weight~sex, names=c(\"Girl\", \"Boy\"))\n\nplot(age, weight, pch=16, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\n\n\n\n\n\n\n\n\n(a) Weight distribution\n\n\n\n\n\n\n\n(b) Weight sub-divided by Sex\n\n\n\n\n\n\n\n\n\n(c) Relationship beween variables\n\n\n\n\nFigure 2.1: Birthweight and gestational age for 24 babies.\n\n\n\nBefore considering possible models, Figure 2.2 again shows the relationship between weight and age but this time with the points coloured according to the baby’s sex. This, perhaps, shows the boys to have generally higher weights across the age range than girls.\n\n\nCode\npar(mar=c(4,4,0,1))\n\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\nplot(age, weight, col=2-sex, pch=15-11*sex,\n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nlegend(41,2800, c(\"Girl\",\"Boy\"), col = c(2,1), pch=c(15,4))\n\n\n\n\n\nFigure 2.2: Birthweight and gestational age for 12 girls (red squares) and 12 boys (black crosses)."
  },
  {
    "objectID": "2_linearmodels.html#focus-on-modelling-quiz",
    "href": "2_linearmodels.html#focus-on-modelling-quiz",
    "title": "2  Essentials of Normal Linear Models",
    "section": "Focus on modelling quiz",
    "text": "Focus on modelling quiz\nTest your knowledge recall and application to reinforce basic ideas and prepare for similar concepts later in the module.\n\n\nFor each situation, choose one of the following statements which you think is most likely to apply.\n\nWhat is the most useful graphical summary for identifying a potential relationship between two variables? boxplothistogramscatter plotkernel density plotother\nWhat is the most useful numerical summary for identifying a potential linear relationship between two variables? sample meansvariancescorrelationskewother\nWhich of the following is a true statements about the correlation coefficient? (Choose any that apply.)\n\n Can take any real number A value close to -1 means uncorrelated and close to +1 indicates a high correlation Is always between -1 and +1 Is always a positive number Can be positive or negative\n\nWhich of the following is a true statements about regression? (Choose any that apply.)\n\n A linear regression can sometimes be used to approximate a non-linear relationship All regression models describe linear relationships A linear regression model will fit well if the correlation is close to zero A linear model can be fitted to any data set After fitting a regression model we should always consider residuals\n\nWhich of the following is a true statements about statistical modelling? (Choose any that apply.)\n\n Models only involve a statistical distribution Models are part deterministic and part random All models are approximations Only use a model that is 100% correct Only use a linear model"
  },
  {
    "objectID": "2_linearmodels.html#linear-models",
    "href": "2_linearmodels.html#linear-models",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.2 Linear models",
    "text": "2.2 Linear models\nContinuing the birth weight example. Of course, there are very many possible models, but here we will consider the following:\n\n\n\n\n\n\n\n\\(\\texttt{Model 0}:\\)\n\\(\\texttt{Weight}=\\alpha\\)\n\n\n\\(\\texttt{Model 1}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}\\)\n\n\n\\(\\texttt{Model 2}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}+\\gamma.\\texttt{Sex}\\)\n\n\n\\(\\texttt{Model 3}:\\)\n\\(\\texttt{Weight}=\\alpha + \\beta.\\texttt{Age}+\\gamma.\\texttt{Sex} + \\delta.\\texttt{Age}.\\texttt{Sex}\\)\n\n\n\nIn these models, \\(\\texttt{Weight}\\) is called the response variable (sometimes called the dependent variable) and \\(\\texttt{Age}\\) and \\(\\texttt{Sex}\\) are called the covariates or explanatory variables (sometimes called the predictor or independent variables). Here, \\(\\texttt{Age}\\) is a continuous variable whereas \\(\\texttt{Sex}\\) is coded as a dummy variable taking the value 0 for girls and 1 for boys; it is an example of a factor, in this case with just two levels: Girl and Boy.\nNote that \\(\\texttt{Model 0}\\) is a special case of \\(\\texttt{Model 1}\\) (consider the situation when \\(\\beta=0\\)) and that \\(\\texttt{Model 1}\\) is a special case of \\(\\texttt{Model 2}\\) (consider the situation when \\(\\gamma=0\\)) and finally that \\(\\texttt{Model 2}\\) is a special case of \\(\\texttt{Model 3}\\) (consider the situation when \\(\\delta=0\\)) – such models are called nested.\nIn these models, \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) and \\(\\delta\\) are model parameters. Parameter \\(\\alpha\\) is called the intercept term; \\(\\beta\\) is called the main effect of \\(\\texttt{Age}\\); and is interpreted as the effect on birth weight per week of gestational age. Similarly, \\(\\gamma\\) is the main effect of \\(\\texttt{Sex}\\), interpreted as the effect on birth weight of being a boy (because girl is the baseline category).\nParameter \\(\\delta\\) is called the interaction effect between \\(\\texttt{Age}\\) and \\(\\texttt{Sex}\\). Take care when interpreting an interaction effect. Here, it does not mean that age somehow affects sex, or vice-versa. It means that the effect of gestational age on birth weight depends on whether the baby is a boy or a girl.\nThese models can be fitted to the data using (Ordinary) Least Squares to produce the results presented in Figure 2.3.\nWhich model should we use?\n\n\nCode\npar(mar=c(4,4,0,1))\n\nbirthweight = read.table(\"https://rgaykroyd.github.io/MATH3823/Datasets/birthwt-numeric.txt\", header=T)\n\nweight = birthweight$weight\nage = birthweight$age\nsex = birthweight$sex\n\n\nplot(age, weight, pch=16, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nabline(h=mean(weight))\n\nplot(age, weight, pch=16,  \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nM1.fit = lm(weight~age)\nabline(M1.fit$coefficients[1],M1.fit$coefficients[2])\n\n\nplot(age, weight, pch=15-11*sex, col=2-sex, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nlegend(41,2800, c(\"Girl\",\"Boy\"), col = c(2,1),pch=c(15,4))\n\nM2.fit = lm(weight~age+sex)\n\nabline(M2.fit$coefficients[1],M2.fit$coefficients[2], col=2)\nabline(M2.fit$coefficients[1]+M2.fit$coefficients[3],M2.fit$coefficients[2], col=1)\n\n\nplot(age, weight, pch=15+sex, col=2-sex, \n     xlab = \"Gestational age (weeks)\",\n     ylab = \"Birth weight (grams)\")\nlegend(41,2800, c(\"Girl\",\"Boy\"), col = c(2,1), pch=c(15,16))\n\nM3.fit = lm(weight~age+sex+age*sex)\n\nabline(M3.fit$coefficients[1],M3.fit$coefficients[2], col=2)\nabline(M3.fit$coefficients[1]+M3.fit$coefficients[3],M3.fit$coefficients[2]+M3.fit$coefficients[4], col=1)\n\n\n\n\n\n\n\n\n(a) Model 0\n\n\n\n\n\n\n\n(b) Model 1\n\n\n\n\n\n\n\n\n\n(c) Model 2\n\n\n\n\n\n\n\n(d) Model 3\n\n\n\n\nFigure 2.3: Birthweight and gestational age data with superimposed fitted regression lines from various competing models.\n\n\n\nWe know from previous modules that statistical tests can be used to check the importance of regression coefficients and model parameters, but it is also important to use the graphical results, as in Figure 2.3, to guide us.\n\\(\\texttt{Model 0}\\) says that there is no change in birth weight with gestational age which means that we would use the average birth weight as the prediction whatever the gestational age – this makes no sense. As we can easily see from the scatter plot of the data, the fitted line in this case is clearly inappropriate.\n\\(\\texttt{Model 1}\\) does not take into account whether the baby is a girl or a boy, but does model the relationship between birth weight and gestational age. This does seem to provide a good fit and might be adequate for many purposes. Recall from Figure 2.1 (b) and Figure 2.2, however, that for a given gestational age the boys seem to have a higher birth weight than the girls.\n\\(\\texttt{Model 2}\\) does take the sex of the baby into account by allowing separate intercepts in the fitted lines – this means that the lines are parallel. By eye, there is a clear difference between these two lines but it might not be important.\n\\(\\texttt{Model 3}\\) allows for separate slopes as well as intercepts. There is a slight difference in the slopes, with the birth weight of the girls gradually catching-up as the gestational age increases. It is difficult to see, however, if this will be a general pattern or if it is only true for this data set – especially given the relatively small sample size.\nHere, it is not clear by eye which of the fitted models will be the best and hence we should use a statistical test to help. In particular, we can choose between the models using F-tests.\nLet \\(y_i\\) denote the value of the dependent variable \\(\\texttt{Weight}\\) for individual \\(i=1,\\dots,n\\), and let the four models be indexed by \\(k=0,1,2,3\\).\nLet \\(R_k\\) denote the residual sum of squares (RSS) for Model \\(k:\\)\n\\[\nR_k = \\sum_{i=1}^n (y_i - \\hat{\\mu}_{ki})^2,\n\\tag{2.1}\\]\nwhere \\(\\hat{\\mu}_{ki}\\) is the fitted value for individual \\(i\\) under \\(\\texttt{Model}\\) \\(k\\). Let \\(r_k\\) denote the corresponding residual degrees of freedom for \\(\\texttt{Model}\\) \\(k\\) (the number of observations minus the number of model parameters).\nConsider the following hypotheses: \\[\nH_0: \\texttt{Model } 0 \\text{ is true}; \\hspace{5mm} H_1: \\texttt{Model } 1 \\text{ is true}.\n\\] Under the null hypothesis \\(H_0\\), the difference between \\(R_0\\) and \\(R_1\\) will be purely random, so the between-models mean-square \\((R_0 - R_1)/(r_0 - r_1)\\) should be comparable to the residual mean-square \\(R_1/r_1\\). Thus our test statistic for comparing \\(\\texttt{Model } 1\\) to the simpler \\(\\texttt{Model } 0\\) is:\n\\[\nF_{01} = \\frac{(R_0 - R_1)/(r_0 - r_1)}{R_1/r_1}.\n\\tag{2.2}\\]\nIt can be shown that, under the null hypothesis \\(H_0\\), the statistic \\(F_{01}\\) will have an \\(F\\)-distribution on \\(r_0 - r_1\\) and \\(r_1\\) degrees of freedom, which we write: \\(F_{r_0-r_1, r_1}\\). Under the alternative hypothesis \\(H_1\\), the difference \\(R_0-R_1\\) will tend to be larger than expected under \\(H_0\\), and so the observed value \\(F_{01}\\) will probably lie in the upper tail of the \\(F_{r_0-r_1, r_1}\\) distribution.\nReturning to the birth weight data, we obtain the following output from R when we fit \\(\\texttt{Model } 1\\):\n\n\nCode\n# read the data from file into a dataframe called ’birthweight’\nbirthweight = read.table(\"https://richardpmann.com/MATH3823/Datasets/birthwt.txt\", header=T)\n\n# fit Model 1\nfit1 = lm(weight ~ age, data=birthweight)\n\n# print the parameter estimates from Model 1\ncoefficients(fit1)\n\n\n(Intercept)         age \n -1484.9846    115.5283 \n\n\nCode\n# perform an analysis of variance of Model 1\nanova(fit1)\n\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value   Pr(&gt;F)    \nage        1 1013799 1013799   27.33 3.04e-05 ***\nResiduals 22  816074   37094                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThus we have parameter estimates: \\(\\hat\\alpha = -1484.98\\) and \\(\\hat\\beta = 115.5\\). The Analysis of Variance (ANOVA) table gives: \\(R_0-R_1 = 1013799\\) with \\(r_0-r_1 = 1\\) and \\(R_1 = 816074\\) with \\(r_1 = 22\\).\nIf we wanted \\(R_0\\) and \\(r_0\\) then we can either fit \\(\\texttt{Model 0}\\) or get them by subtraction.\nThe \\(F_{01}\\) statistic, Equation 2.2, is then \\[\nF_{01} = \\frac{103799/1}{816074/22} = 27.33,\n\\] which can be read directly from the ANOVA table in the column headed ‘F value’.\nIs \\(F_{01} = 27.33\\) in the upper tail of the \\(F_{1,22}\\) distribution? (See Figure 2.4 and note that 27.33 is very far to the right.) The final column of the ANOVA table tells us that the probability of observing \\(F_{01} &gt; 27.33\\) is only \\(3.04\\times10^5\\) – this is called a p-value. The *** beside this p-value highlights that its value lies between 0 and 0.001. This indicates that we should reject \\(H_0\\) in favour of \\(H_1\\) – there is very strong evidence for the more complicated model. Thus we would conclude that the effect of gestational age is statistically significant in these data.\n\n\nCode\npar(mar=c(4,4,0,1))\n\ncurve(df(x,1,22), 0,30, \n      ylab=expression(\"PDF of \"*\"F\"[0][1]))\n\n\n\n\n\nFigure 2.4: Probability density function of \\(F_{01}\\) distribution.\n\n\n\n\nNext, consider the following hypotheses:\n\\[\nH_0: \\texttt{Model } 1 \\text{ is true}; \\hspace{5mm} H_1: \\texttt{Model } 2 \\text{ is true}.\n\\]\nUnder the null hypothesis \\(H_0\\), the difference between \\(R_1\\) and \\(R_2\\) will be purely random, so the between-models mean-square \\((R_1 - R_2)/(r_1 - r_2)\\) should be comparable to the residual mean-square \\(R_2/r_2\\). Thus our test statistic for comparing \\(\\texttt{Model } 2\\) to the simpler \\(\\texttt{Model } 1\\) is:\n\\[\nF_{12} = \\frac{(R_1 - R_2)/(r_1 - r_2)}{R_2/r_2}.\n\\tag{2.3}\\]\nUnder the null hypothesis \\(H_0\\), the statistic \\(F_{12}\\) will have an \\(F\\)-distribution on \\(r_1 - r_2\\) and \\(r_2\\) degrees of freedom, which we write: \\(F_{r_1-r_2, r_2}\\). Under the alternative hypothesis \\(H_1\\), the difference \\(R_1-R_2\\) will tend to be larger than expected under \\(H_0\\), and so the observed value \\(F_{12}\\) will probably lie in the upper tail of the \\(F_{r_1-r_2, r_2}\\) distribution.\nReturning to the birth weight data, we obtain the following output from R (where \\(\\texttt{sexM}\\) denotes Boy):\n\n\nCode\n# fit Model 2\nfit2 = lm(weight ~ age + sex, data=birthweight)\n# print the parameter estimates from Model 2\ncoefficients(fit2)\n\n\n(Intercept)         age        sexM \n -1773.3218    120.8943    163.0393 \n\n\nCode\n# perform an analysis of variance on the fitted model\nanova(fit2)\n\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nage        1 1013799 1013799 32.3174 1.213e-05 ***\nsex        1  157304  157304  5.0145   0.03609 *  \nResiduals 21  658771   31370                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThus we have parameter estimates: \\(\\hat \\alpha = -1773.3\\), \\(\\hat\\beta = 120.9\\) and \\(\\hat\\gamma = 163.0\\), the latter being the effect of being a boy compared to the baseline category of being a girl.\nThe Analysis of Variance (ANOVA) table gives: \\(R_1-R_2 = 157304\\) with \\(r_1-r_2=1\\), and \\(R_2=658771\\) with \\(r_2=21\\). The \\(F_{12}\\) statistic, Equation 2.3, is then\n\\[\nF_{12} = \\frac{157304/1}{658771/21} = 5.0145,\n\\]\nwhich can be read directly from the ANOVA table in the column headed ‘F value’. Is \\(F_{12} = 5.01\\) in the upper tail of the \\(F_{1,21}\\) distribution?\nThe final column of the ANOVA table tells us that the probability of observing \\(F_{12} &gt; 5.01\\) is only \\(0.03609\\) – this is called a p-value. The * beside this p-value highlights that its value lies between 0.01 and 0.05. This indicates that we should reject \\(H_0\\) in favour of \\(H_1\\) – there is evidence for the more complicated model. Thus we would conclude that the effect of the sex of the baby, after controlling for gestational age, is statistically significant in these data.\nTo complete the analysis, we should now compare \\(\\texttt{Model }2\\) with \\(\\texttt{Model }3\\) – see Exercises."
  },
  {
    "objectID": "2_linearmodels.html#focus-on-regression-quiz",
    "href": "2_linearmodels.html#focus-on-regression-quiz",
    "title": "2  Essentials of Normal Linear Models",
    "section": "Focus on regression quiz",
    "text": "Focus on regression quiz\nTest your knowledge recall and application to reinforce basic ideas and prepare for similar concepts later in the module.\n\n\nFor each situation, choose one of the following statements which you think is most likely to apply.\n\nWhich of the following is a true statement about correlation and linear regression? (Choose any that apply.)\n\n The correlation and the regression slope parameter will have the same sign If the correlation is away from zero then a linear regression should always be used The correlation and the slope parameter will have the opposite sign If the correlation is close to zero then a linear regression will not be useful There is no relationship between correlation and linear regression\n\nWhich of the following is NOT a true statement about model residuals? (Choose any that apply.)\n\n Will always sum to zero Can help to identify unusual data points All should be zero if the model is a good fit Should always be plotted as part of a data analysis Can be used to judge how well the model fits the data\n\nWhich of the following is a true statement about prediction using linear regression? (Choose any that apply.)\n\n Interpolation is performed using the fitted model Interpolation and extrapolation are both types of prediction Interpolation should only be used if we know the true relationship is linear Prediction is always reliable We should be careful about extrapolating\n\nWhich of the following is NOT an important part of regression model fitting? (Choose any that apply.)\n\n A scatter plot of the residuals A scatter plot of the data A histogram of the data The command lm to fit a linear model The command cor to check for a relationship between variables\n\nWhich of the following is NOT important when performing data analysis? (Choose any that apply.)\n\n Are the data representative of what we are aiming to investigate Do we need authorization from the person collecting the data before we can use it The analysis should be done professionally. We must not involve our personal views to influence our analysis and conclusions Is the data reliable and have suitable data checks been performed Background information is not needed before analyzing a new data set – it's only the numbers that matter"
  },
  {
    "objectID": "2_linearmodels.html#sec-typesnormalvariable",
    "href": "2_linearmodels.html#sec-typesnormalvariable",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.3 Types of normal linear model",
    "text": "2.3 Types of normal linear model\nHere we consider how normal linear models can be set up for different types of explanatory variable. The dependent variable \\(y\\) is modelled as a linear combination of \\(p\\) explanatory variables \\(\\mathbf{x} =(x_1, x_2,\\ldots, x_p)\\) plus a random error \\(\\epsilon \\sim N(0, \\sigma^2)\\), where ‘~’ means ‘is distributed as’. Several models are of this kind, depending on the number and type of explanatory variables. Table 2.2 lists some types of normal linear models with their explanatory variable types.\n\n\nTable 2.2: Types of normal linear model and their explanatory variable types where indicator function \\(I(x=j)=1\\) if \\(x=j\\) and \\(0\\) otherwise. \n\n\n\n\n\n\n\n\\(p\\)\nExplanatory variables\nModel\n\n\n1\nQuantitative\nSimple linear regression \\(y=\\alpha+\\beta x+\\epsilon\\)\n\n\n&gt;1\nQuantitative\nMultiple linear regression\\(y=\\alpha+\\sum_{i=1}^p\\beta_i x_i+\\epsilon\\)\n\n\n1\nDichotomous (\\(x=1\\) or \\(2\\))\nTwo-sample t-test \\(y=\\alpha+\\delta ~ I(x=2)+\\epsilon\\)\n\n\n1\nPolytomous, \\(k\\) levels \\((x=1,\\ldots,k)\\)\nOne-way ANOVA\\(y=\\alpha+\\sum_{j=1}^k \\delta_j \\ I(x=j)+\\epsilon\\)\n\n\n&gt;1\nQualitative\n\\(p\\)-way ANOVA\n\n\n\n\nFor the two-sample t-test model2, observations in the two groups have means \\(\\alpha+\\beta_1\\) and \\(\\alpha + \\beta_2\\) . Notice, however, that we have three parameters with only two group sample means and hence parameter estimation is not possible. To avoid this identification problem, we either impose a ‘corner’ constraint: \\(\\beta_1=0\\) and then \\(\\beta_2\\) represents the difference in the Group 2 mean relative to a baseline of Group \\(1\\). Alternatively, we may impose a ‘sum-to-zero’ constraint: \\(\\beta_1+ \\beta_2 =0\\), the values \\(\\beta_1=-\\beta_2\\) then give differences in the groups means relative to the overall mean. Table 2.3 shows the effect of the parameter constraint on the group means.\n\n\nTable 2.3: Parameters in the two-sample t-test model after imposing parameter constraint to avoid the identification problem.\n\n\nConstraint\nGroup 1 mean\nGroup 2 mean\n\n\n\n\n\\(\\beta_1=0\\)\n\\(\\alpha\\)\n\\(\\alpha+\\beta_2\\)\n\n\n\\(\\beta_1+\\beta_2=0\\)\n\\(\\alpha-\\beta_2\\)\n\\(\\alpha+\\beta_2\\)\n\n\n\n\nFor the general one-way ANOVA model with \\(k\\) groups, observations in Group \\(j\\) have mean \\(\\alpha + \\delta_j\\) , for \\(j =1, \\ldots, k\\) – that leads to \\(k + 1\\) parameters describing \\(k\\) group means. Again we can impose the ‘corner’ constraint: \\(\\delta_1 = 0\\) and then \\(\\delta_j\\) represents the difference in means between Group \\(j\\) and the baseline Group \\(1\\). Alternatively, we may impose a ‘sum-to-zero’ constraint:\\(\\sum_{j=1}^k \\delta_j =0\\) and again \\((\\delta_1, \\delta_2,\\dots,\\delta_k)\\) then represents an individual group effect relative to the overall data mean."
  },
  {
    "objectID": "2_linearmodels.html#sec-matrix",
    "href": "2_linearmodels.html#sec-matrix",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.4 Matrix representation of linear models",
    "text": "2.4 Matrix representation of linear models\nAll of the models in Table Table 2.2 can be fitted by least squares (OLS). To describe this, a matrix formulation will be most convenient:\n\\[\n\\mathbf{Y} = X\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\n\\tag{2.4}\\]\nwhere\n\n\\(\\mathbf{Y}\\) is an \\(n\\times 1\\) vector of observed response values with \\(n\\) being the number of observations.\n\\(X\\) is an \\(n\\times p\\) design matrix, to be discussed below.\n\\(\\boldsymbol{\\beta}\\) is a \\(p\\times 1\\) vector of parameters or coefficients to be estimated.\n\\(\\boldsymbol{\\epsilon}\\) is an \\(n\\times 1\\) vector of independent and identically distributed (IID) random variables, which here \\(\\epsilon \\sim N(0, \\sigma^2)\\) and is called the “error” term.\n\nCreating the design matrix is a key part of the modelling as it describes the important structure of investigation or experiment. The design matrix can be constructed by the following process.\n\nBegin with an \\(X\\) containing only one column: a vector of ones for the overall mean or intercept term (the \\(\\alpha\\) in Table 2.2).\nFor each explanatory variable \\(x_j\\), do the following:\n\nIf a variable \\(x_j\\) is quantitative, add a column to \\(X\\) containing the values of \\(x_j.\\)\nIf \\(x_j\\) is qualitative with \\(k\\) levels, add \\(k\\) “dummy” columns to \\(X\\), taking values 0 and 1, where a 1 in the \\(\\ell\\)th dummy column identifies that the corresponding observation is at level \\(\\ell\\) of factor \\(x_j\\) . For example, suppose we have a factor \\(\\mathbf{x}_j = (M, M, F, M, F)\\) representing the sex of \\(n = 5\\) individuals. This information can be coded into two dummy columns of \\(X\\):\n\n\\[\n\\begin{matrix}\n\\begin{matrix}\nF & M\n\\end{matrix}\\\\\n\\begin{bmatrix}\n0 & 1 \\\\\n0 & 1 \\\\\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 0\n\\end{bmatrix}\n\\end{matrix}\n\\]\nWhen qualitative variables are present, \\(X\\) will be singular – that is, there will be linear dependencies between the columns of \\(X\\). For example, the sum of the two columns above is a vector of ones, the same as the intercept column. We resolve this identification problem by deleting some columns of \\(X\\). This is equivalent to applying the corner constraint \\(\\delta_1 = 0\\) in the one-way ANOVA.\nIn the above example, after removing a column, we get:\n\\[\n\\mathbf{X}=\\begin{bmatrix}\n1 &  1 \\\\\n1 &  1 \\\\\n1 &  0 \\\\\n1 &  1 \\\\\n1 &  0\n\\end{bmatrix}.\n\\]\nEach column of \\(X\\) represents either a quantitative variable, or a level of a qualitative variable. We will use \\(i = 1, \\ldots, n\\) to label the observations (rows of \\(X\\)) and \\(j = 1, \\ldots, p\\) to label the columns of \\(X\\).\n\n\n\nExample: Simple linear regression\nConsider the simple linear regression model \\(y=\\alpha+\\beta x+\\epsilon\\) with \\(\\epsilon \\sim N(0, \\sigma^2)\\). Given data on \\(n\\) pairs \\((x_i, y_i), i = 1, \\ldots, n\\), we write this as\n\\[\ny_i = \\alpha+\\beta x_i+\\epsilon_i, \\quad \\text{for } i=1,2,\\dots,n,\n\\tag{2.5}\\]\nwhere the \\(\\epsilon_i\\) are IID \\(N(0,\\sigma^2)\\). In matrix form, this becomes\n\\[\n\\mathbf{Y}=X\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\n\\tag{2.6}\\] with \\[\n\\mathbf{Y}=\\begin{bmatrix}\ny_1\\\\\n\\vdots\\\\\ny_n\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\nX=\\begin{bmatrix}\n1 & x_1\\\\\n\\vdots & \\vdots\\\\\n1 & x_n\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\boldsymbol{\\beta}=\n\\begin{bmatrix}\n\\beta_1\\\\\n\\beta_2\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\alpha\\\\\n\\beta\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\boldsymbol{\\epsilon}=\n\\begin{bmatrix}\n\\epsilon_1\\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}.\n\\] The \\(i\\)th row of Equation 2.6 has the same meaning as Equation 2.5: \\[\ny_i = 1\\times \\beta_1 + x_i\\times \\beta_2 +\\epsilon_i = \\alpha+\\beta x_i +\\epsilon_i,  \\hspace{2mm} \\text{for } i=1,2,\\dots,n.\n\\]\n\nExample: One-way ANOVA\nFor one-way ANOVA with \\(k\\) levels, the model is \\[\ny_i =\\alpha+\\sum_{j=1}^k \\delta_j \\ I(x_i=j)+\\epsilon_i, \\quad \\text{for } i=1, 2, \\dots,n,\n\\] where \\(x_i\\) denotes the group level of individual \\(i\\). So if \\(y_i\\) is from the \\(j\\)th group then \\(y_i \\sim N(\\alpha+\\delta_j, \\sigma^2)\\). Here \\(\\alpha\\) is the intercept and the \\((\\delta_1, \\delta_2, \\dots,\\delta_k)\\) represent the “main effects”.\nWe can store the information about the levels of \\(g\\) in a dummy matrix \\(X^* = (x^*_{ij})\\) where\n\\[\nx^*_{ij} = \\left\\{\n\\begin{array}{cl}\n1, & g_i=j,\\\\\n0, & \\text{otherwise.}\n\\end{array}\n\\right.\n\\]\nThen set \\(X = [1, X^*]\\), where \\(1\\) is an \\(n\\)-vector of \\(1\\)’s. For the male–female example at (1.12), we have \\(n = 5\\) and a sex factor:\n\\[\ng=\\begin{bmatrix}1\\\\ 1 \\\\2\\\\1\\\\2\\end{bmatrix},\n%\n\\hspace{5mm}\n%\nX=\\begin{bmatrix}\n1 & 1 & 0\\\\\n1&1& 0 \\\\ 1& 0 & 1\\\\ 1& 1 & 0 \\\\ 1& 0 & 1\n\\end{bmatrix},\n%\n\\hspace{5mm}\n%\n\\beta=\\begin{bmatrix}\\beta_1\\\\\\beta_2\\\\\\beta_3\\end{bmatrix}\n=\\begin{bmatrix}\\alpha\\\\\\delta_1 \\\\\\delta_2\\end{bmatrix}.\n\\]\nThen the \\(i\\)th row of \\(X\\) becomes \\(\\beta_1 + \\beta_2 = \\alpha + \\delta_1\\) if \\(g_i = 1\\) and \\(\\beta_1 + \\beta_3 = \\alpha + \\delta_2\\) if \\(g_i\\) = 2. That is, the \\(i\\)th row of \\(X\\) is\n\\[\n\\alpha+\\sum_{j=1}^2 \\delta_j I(g_i=j)\n\\] so this model can be written \\(Y=X\\beta+\\epsilon\\). Here, \\(X\\) is singular: its last two columns added together equal its first column. Statistically, the problem is that we are trying to estimate two means (the mean response for Boys and the mean response for Girls) with three parameters (\\(\\alpha\\), \\(\\delta_2\\) and \\(\\delta_2\\)).\nIn practice, we often resolve this aliasing or identification problem by setting one of the parameters to be zero, that is \\(\\delta_1 = 0\\), which corresponds to deleting the second column of \\(X\\))."
  },
  {
    "objectID": "2_linearmodels.html#focus-on-matrix-representations-quiz",
    "href": "2_linearmodels.html#focus-on-matrix-representations-quiz",
    "title": "2  Essentials of Normal Linear Models",
    "section": "Focus on matrix representations quiz",
    "text": "Focus on matrix representations quiz\n\n\nFor each situation, choose one of the following statements which you think is most likely to apply.\n\nWhat is the dimension of the design matrix? n by pn by 12Xp by 1\nWhat is the distribution of the error term? mean zerorandom variableIDDn by 1normal\nWhich quantity represents the model parameters? nXbetaepsilonY\nWhich two terms in the model have the same dimensions? Y and betabeta and epsilonY and XX and betaY and epsilon\nWhich of the following is a potential problem when using qualitative variables? X is not squareX can be negativeX is singularX can be zeroX is full rank"
  },
  {
    "objectID": "2_linearmodels.html#sec-shorthand",
    "href": "2_linearmodels.html#sec-shorthand",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.5 Model shorthand notation",
    "text": "2.5 Model shorthand notation\nIn R, a qualitative (categorical) variable is called a factor, and its categories are called levels. For example, variable \\(\\texttt{Sex}\\) in the birth weight data (above) has levels coded “M” for ‘Boy’ and “F” for ‘Girl’. It may not be obvious to R whether a variable is quantitative or qualitative. For example, a qualitative variable called \\(\\texttt{Grade}\\) might have categories 1, 2 and 3. If was included in a model, R would treat it as quantitative unless we declare it to be a factor, which we can do with the command:\n\\(\\texttt{grade = as.factor(grade)}\\)\nA convenient model-specification notation has been developed from which the design matrix \\(X\\) can be constructed. Below, \\(E, F, \\ldots\\) denote generic quantitative (continuous) or qualitative (categorical) variables. Terms in this notation may take the following forms:\n\n\\(1\\) : a column of 1’s to accommodate an intercept term (the \\(\\alpha\\)’s of Table 2.2 ). This is included in the model by default.\n\\(E\\) : variable \\(E\\) is included in the model. The design matrix includes \\(k_E\\) columns for \\(E\\). If \\(E\\) is quantitative, \\(k_E = 1\\). If E is qualitative, \\(k_E\\) is the number of levels of \\(E\\) minus 1.\n\\(E +F\\) : both \\(E\\) and \\(F\\) are included the model. The design matrix includes \\(k_E +k_F\\) columns accordingly.\n\\(E : F\\) (sometimes \\(E \\cdot F\\)) : the model includes an interaction between \\(E\\) and \\(F\\); each column that would be included for \\(E\\) is multiplied by each column for \\(F\\) in turn. The design matrix includes \\(k_E \\times k_F\\) columns accordingly.\n\\(E * F\\) : shorthand for \\(1 + E + F + E : F\\): useful for crossed models where \\(E\\) and \\(F\\) are different factors. For example, \\(E\\) labels age groups; \\(F\\) labels medical conditions.\n\\(E/F\\) : shorthand for \\(1 + E + E : F\\): useful for nested models where \\(F\\) is a factor whose levels have meaning only within levels of factor \\(E\\). For example, \\(E\\) labels different hospitals; \\(F\\) labels wards within hospitals.\n\\(\\text{poly}(E; \\ell)\\) : shorthand for an orthogonal polynomial, wherein \\(x\\) contains a set of mutually orthogonal columns containing polynomials in \\(E\\) of increasing order, from order \\(1\\) through order \\(\\ell\\).\n\\(-E\\) : shorthand for removing a term from the model; for example \\(E * F -E\\) is short for \\(1 + F + E : F\\).\n\\(I()\\) : shorthand for an arithmetical expression (not to be confused with the indicator function defined above). For example, \\(I(E + F)\\) denotes a new quantitative variable constructed by adding together quantitative variables \\(E\\) and \\(F\\). This would cause an error if either \\(E\\) or \\(F\\) has been declared as a factor. What would happen in this example if we omitted the \\(I(\\cdot)\\) notation?\n\nThe notation uses “~” as shorthand for “is modelled by” or “is regressed on”. For example,\n\nWeight is regressed on age-group and sex with no interaction between them: \\[\n\\texttt{Weight} \\sim \\texttt{Age} + \\texttt{Sex}\n\\] as for the birthweight data in Figure 1.2c.\nWell being is regressed on age-group and income-group, where income is thought to affect wellbeing differentially by age: \\[\n\\texttt{Wellbeing} \\sim \\texttt{Age} * \\texttt{Income}\n\\]\nClass of degree is regressed on school of the university and on degree subject within the school: \\[\n\\texttt{DegreeClass} \\sim \\texttt{School/Subject}\n\\]\nYield of wheat is regressed on seed-variety and annual rainfall: \\[\n\\texttt{Yield} \\sim \\texttt{Variety} + \\texttt{poly}(\\texttt{Rainfall}, 2)\n\\]\nProfit is regressed on amount invested: \\[\n\\texttt{Profit}\\sim \\texttt{Investment}- 1\n\\] (no intercept term, that is a regression through the origin)."
  },
  {
    "objectID": "2_linearmodels.html#focus-on-model-notation-quiz",
    "href": "2_linearmodels.html#focus-on-model-notation-quiz",
    "title": "2  Essentials of Normal Linear Models",
    "section": "Focus on model notation quiz",
    "text": "Focus on model notation quiz\n\n\nFor each situation, choose one of the following statements which you think is most likely to apply.\n\nWhat R command can be used to covert numerical values into a nominal variable? as.factorfactorialas.numericalas.ordinalas.nominal\nWhich of the following defines a model where variable Y is regressed on variables V1 and V2, but without an interaction? Y ~ V1*V2Y ~ V1 + V2 + V1:V2Y ~ V1 + V2Y ~ V1:V2V1 + V2 ~ Y\nWhich of the following defines a model where variable Y is regressed on variables V1 and V2, including a constant and an interaction? Y ~ 1 + V1 + V2Y ~ 1+V1:V2Y ~ 1+V1/V2Y ~ V1*V2Y ~ V1 + V2 + V1:V2 -1\nWhich of the following defines a model regression Y on the product of V1 and V2? Y ~ poly(V1,V2)Y ~ I(V1*V2)Y ~ V1:V2Y ~ I(V1 + V2)Y ~ V1.V2\nWhich of the following defines a model where variable Y is regressed on a second-order polynomial in V1? Y ~ V1*V1Y ~ V1^2Y ~ I(V1)Y ~ poly(V1,2)Y ~ 1+V1"
  },
  {
    "objectID": "2_linearmodels.html#fitting-linear-models-in-r",
    "href": "2_linearmodels.html#fitting-linear-models-in-r",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.6 Fitting linear models in R",
    "text": "2.6 Fitting linear models in R\nA commonly used command for fitting a linear model in R is\\[\\texttt{lm(formula)}.\\]\nLet \\(\\texttt{x,y,z,a,b} \\dots\\) represent R vectors, all of the same length \\(n\\) (perhaps read in from a data file using the \\(\\texttt{read.table}\\) and \\(\\texttt{attach}\\) commands).\nIf \\(\\texttt{a,b}\\) are qualitative variables, then they first need to be declared as factors by \\(\\texttt{a = as.factor(a)}\\), etc.\nThe \\(\\texttt{formula}\\) argument of \\(\\texttt{lm}\\) specifies the required model in compact notation, e.g. \\(\\texttt{y} \\sim \\texttt{x+z}\\) or \\(\\texttt{y} \\sim \\texttt{x + z*a}\\) where \\(\\sim, \\texttt{+,*}\\) have the same meaning as in Section 2.5.\nTo extract information about a fitted linear model, it is best to store the result of \\(\\texttt{lm}\\) as a variable and then to use the following functions:\n\nTo fit a linear model and store the result in \\(\\texttt{my.lm}\\) (for example):\n\\(\\texttt{my.lm = lm(y}\\sim\\texttt{x + a*b})\\)\nTo print various pieces of information including deviance residuals, parameter estimates and standard errors, deviances, and (if specified) correlations of parameter estimates:\n\\(\\texttt{summary(my.lm, correlation=T)}\\)\nTo print the anova table of the fitted model:\n\\(\\texttt{anova(my.lm)}\\)\nTo print the residual degrees of freedom of the fitted model:\n\\(\\texttt{df.residual(my.lm)}\\)\nTo print the vector of fitted values under the fitted model:\n\\(\\texttt{fitted.values(my.lm)}\\)\nTo print the residuals from the fitted model:\n\\(\\texttt{residuals(my.lm)}\\)\n\nTo print the parameter estimates from the fitted model:\n\\(\\texttt{coefficients(my.lm)}\\)\nTo print the design matrix for a specified model formula: \\(\\texttt{model.matrix(y}\\sim\\texttt{a*b)}\\)\n\nThe functions \\(\\texttt{summary}\\), \\(\\texttt{anova}\\), and possibly \\(\\texttt{model.matrix}\\) are the most useful for printing out information about the fitted model. The results of the other functions can be saved as variables for further computation. In particular, you should look for an ansence of structure in a plot the residuals and you may need to make predictions of the response variable for new values of the explanatory variable.\nExample of fitting a linear model in R\nHere is a toy example of R commands for modelling a response in terms of one quantitative explanatory variable.\n\n\nCode\nset.seed(273686) # for reproducibility\n\n# Create some artificial data\nx = seq(1,12, length.out=50)\ny = 2 + 0.2*x + rnorm(length(x),0,0.2)\n\n# Fit the linear model\nmy.lm = lm(y ~ x)\n\n# Show summary information of the fitted model\nsummary(my.lm)\n\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.38790 -0.12026 -0.01578  0.12751  0.44184 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.941552   0.062524   31.05   &lt;2e-16 ***\nx           0.204299   0.008609   23.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1972 on 48 degrees of freedom\nMultiple R-squared:  0.9215,    Adjusted R-squared:  0.9198 \nF-statistic: 563.1 on 1 and 48 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\n# Adjust plot layout\npar(mar=c(4,4,1,1))\n\n# Plot data and add the fitted regression line\nplot(y~x, pch=16)\nabline(my.lm)\n\n# Calculate the std dev of the residuals\nresid.sd = sd(my.lm$residuals)\n\n# Plot the residuals against the fitted values\nplot(my.lm$fitted.values, my.lm$residuals, pch=16,\n     xlab=\"Fitted values\", ylab=\"Residuals\",\n     ylim= 0.6*c(-1, 1))\n\n# Add zero line and lines =/- 2sd\nabline(h=0, lty=2)\nabline(h=2*resid.sd*c(-1, 1), lty=2, col=\"red\")\n\n# Look at distribution of residuals\nhist(my.lm$residuals, probability=T, \n     xlim=0.6*c(-1, 1), main=\"\")\n# Compare residuals with normal distribution\nqqnorm(my.lm$residuals, main=\"\"); qqline(my.lm$residuals)\n\n\n\n\n\n\n\n\n(a) Data and fitted model\n\n\n\n\n\n\n\n(b) Residual plot\n\n\n\n\n\n\n\n\n\n(c) Histogram of residuals\n\n\n\n\n\n\n\n(d) Normal distribution QQ plot\n\n\n\n\nFigure 2.5: Illustration of model fitting on a toy example\n\n\n\nA scatter plot of the data indicates that a linear model would be appropriate and the resulting fitted linear regression describes the data well. A residual plot shows that all residuals are within two standard deviations of zero, with the histogram showing a symmetric distribution and a QQ plot supporting normality of the residuals. All these diagnostic plots support that the model fits well.\nFinally, we may wish to predict values of the response variable at new values of the response variable, for example at \\(x=4\\) and \\(x=6\\) – notice that the values of the explanatory variable must be converted into a R \\(\\texttt{data.frame}\\):\n\n\nCode\n# Use the fitted model for prediction, at say x=4 and x=6\npredict(my.lm, data.frame(x=c(4,6)))\n\n\n       1        2 \n2.758747 3.167345"
  },
  {
    "objectID": "2_linearmodels.html#ethics-in-statistics-and-data-science",
    "href": "2_linearmodels.html#ethics-in-statistics-and-data-science",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.7 Ethics in statistics and data science",
    "text": "2.7 Ethics in statistics and data science\nA brief introduction to ethics and their relevance to statistics and data science.\nIn previous module you have already seen that professional and ethical consideration are important. Whether that be when we choose which graph to present or what action to take when data is missing or how to deal with suspected outliers. It is important for statisticians and data scientists to be aware of the ethical dimensions of their work and to be able to think these through.\nPlease note that the following was produce with the help of Dr Robbie Morgan from the Interdisciplinary Applied Ethics (IDEA) Centre at the University of Leeds.\nRoughly speaking, ethics concerns what we ought to do (should do) and the kind of person that we ought to be. Of course, we’re particularly interested in the kinds of ethical questions and challenges that you will encounter in your work as data scientists, such as:\n\nHow should we collect data in a way that respects participants?\nWhy is privacy important? How should data scientists protect privacy?\nHow can algorithmic bias wrong members of the public? How should data scientists respond to this?\nTo what extent are data scientists responsible for the impact of their work?\nWhen and how should data scientists challenge authority in the workplace?\n\nThe work that you do as data scientists can be hugely beneficial; it develops solutions for pressing real-world problems, enables organisations to carry out important functions more effectively and efficiently, and can improve the well being of the public by enacting innovation and technological advancement. At the same time, work in data science presents risk of significant harm and injustice. Widespread data collection threatens the privacy and safety of data subjects. Facial recognition and other surveillance technologies are the latest site of long-running debates over the values of privacy and freedom versus security. Biased algorithms can inflict injustices and exacerbate entrenched inequality. Automation can cause unemployment and instability, with unpredictable results as it affects ever more areas of our lives. So, it is very important to be clear about the obligations and responsibilities that data scientists have to their employers, colleagues, and, most importantly, to society as a whole.\nPlease keep these issues in mind throughout any data analysis and modelling work, especially if you follow such a career path after graduation."
  },
  {
    "objectID": "2_linearmodels.html#exercises",
    "href": "2_linearmodels.html#exercises",
    "title": "2  Essentials of Normal Linear Models",
    "section": "2.8 Exercises",
    "text": "2.8 Exercises\n\n\n\n\n\n\nImportant\n\n\n\nUnless otherwise stated, data files will be available online at: richardpmann.com/MATH3823/Datasets/filename.ext, where filename.ext is the stated filename with extension.\n\n\n\n2.1. For each situation, consider the data description, correlation value, and data visualization. Then, identify whether the variables are related and if a linear model would be suitable.\n\n\nClick here to see hints.\n\nFor each description, think about what you expect to see and then confirm this, or otherwise, with the scatter plot. Does a linear relationship seem appropriate? Also, does variation in the scatter plot and correlation value suggest a strong relationship.\n\n\nChildhood obesity is a serious medical condition which can lead to long-term health problems. A mixed-sex secondary school for ages 11-18 measures height and weight of all its new students (mostly aged 11 years old) on the first day of term and calculates their body mass index (BMI) and the average daily calorie intake was recorded. The correlation between calorie intake and BMI was 0.6 with the data shown in the scatter plot.\n\n\n\n\n\n\n\nDomestic UK “smart meters” aim to record cumulative electricity and gas usage once per day. They use a wireless internet connection to transmit information to the energy providers, but they do not save previous values. The central system records readings as they are received and calculates the difference between successive readings. The scatter plot shows the daily electricity and gas consumption of a typical house collected using an automatic smart meter. The correlation between electricity and gas consumption is 0.4.\n\n\n\n\n\n\n\nTo investigate the punctuality of trains at Leeds City Station, a platform attendant records the number of minutes late, or early, that each train arrives at a particular platform relative to the timetable. The journey distance of each train arriving was later determined and recorded. The scatter plot shows the Distance Traveled, in miles, and the Delay, in minutes with a correlation of 0.1.\n\n\n\n\n\n\n\nThe 2022-23 UK Housing Survey, included questions regarding household income and the number of rooms. The data are shown in a scatter plot with a correlation of 0.7 between the variables.\n\n\n\n\n\n\n2.2. An extra model which could have been considered for the Birth weight data example would be one that says that \\(\\texttt{Weight}\\) is different for girls and boys, but does not depend on gestational age. Investigate this model.\n\n\nClick here to see hints.\n\nWrite down the equation corresponding to this model. Then, load the birth weight data into RStudio and fit the model. How are the fitted model parameters related to the overall birth weight mean and the mean birth weights of the girls and boys? Is this a good fit to the data? Is Sex statistically significant?\n\n2.3. For each given situation, consider the description and then investigate the suitability of a linear model.\n\n\nClick here to see hints.\n\nFor each given data set, produce an appropriate graph within RStudio, fit a linear regression model and add the fitted model to the graph. Comment on the quality of fit.\n\n\nContinuing the childhood obesity example, use the data in file schoolstudy.csv to model the relationship between BMI and calorie intake in 11-year old children. \n\nTo study the profitability of several iron ore (hematite) extraction quarries, small samples are taken from lorries arriving at an iron purification site. The lorries are open-topped with most travelling less than 20 Km but one quarry is more than 100 Km away. A chemical analysis provides a percentage of pure iron in the sample. Quarries with iron content less than 30% are not considered economically viable. The data file iron.csv contains measurements of percentage pure iron arriving at an iron purification site recorded over a 50 year period. Model the relationship between iron purity and time.\n\nA study aims to investigate osteoporosis in women before and after menopause. The X-rays of a randomly selected sample of patients taking routine mammograms are analysed. The age of the patient and their menopause status are recorded, along with a measure of bone density calculated from the X-ray. Use the data in the file bmd.csv to model the relationship between age and Tscore, noting that a value of below -2.5 indicates osteoperosis, between -2.5 and -1.0 indicates osteopenia whereas above -1.0 is normal.\n\nA primary school head teaching wishes to investigate the relationship between social skills of children and the ages of their brothers and sisters. The hypothesis is that those with older siblings will be better able to deal with social interaction. The file siblings.csv contains data on the age of the eldest sibling of a class of 6-year old school children along with a social skills score for each child assessed during the school lunch break. Model the relationship between sibling age and social skills.\n\n2.4. In an experiment to investigate Ohm’s Law, \\(V=IR\\) where \\(V\\) is Voltage, I is current and \\(R\\) is resistance of the material, the following data3 were recorded:\n\n\nTable 2.4: Experimental verification of Ohm’s Law\n\n\nVoltage (Volts)\n4\n8\n10\n12\n14\n18\n20\n24\n\n\nCurrent (mAmps)\n11\n24\n30\n36\n40\n53\n58.5\n70\n\n\n\n\nDoes this data support Ohm’s Law? What is the resistance of the material used?\n\n\nClick here to see hints.\n\nThere is no data file prepared for this and so create your own variables, then perform a linear regression. Comment on the quality of fit. Note that Ohm’s Law is a linear function but without intercept and that the resistance is a constant multiplying the current.\n\n2.5 In an investigation4 into the effect of eating on pulse rate, 6 men and 6 women were tested before and after a meal, with the following results:\n\n\nTable 2.5: At rest pulse rate before and after a meal for men and women \n\n\nMen\nbefore\n105\n79\n79\n103\n87\n97\n\n\n\nafter\n109\n87\n86\n109\n100\n101\n\n\nWomen\nbefore\n74\n73\n82\n78\n86\n77\n\n\n\nafter\n82\n80\n90\n90\n93\n81\n\n\n\n\nSuggest a suitable model for this situation and write down the corresponding design matrix. Calculate the parameter estimates using the matrix regression estimation equation.\nPerform an appropriate analysis in R to find out if there is evidence to suggest that the change in pulse rate due to a meal is the same for men and women.\n\n\nClick here to see hints.\n\nBeware! This time we have categorical variables: “before”/“after” and “Men/Women”. To create the design matrix think of writing down the terms needed to produce each data value and don’t forget to remove columns to make the solution identifiable. Also, don’t do the matrix multiplication/inversion by hand but use R to solve the matrix calculation. If the change in pulse rate is different for men and women then the interaction should be significant.\n\n2.6 A laboratory experiment5 was performed into the effect of seasonal floods on the growth of barley seedlings in a incubator, as measured by their height in mm. Three types of barley seed (Goldmarker, Midas, Igri) were used with two watering condition (Normal and Waterlogged). Further, each combination was repeated four times on different shelves in the laboratory incubator (Top, Second, Third and Bottom shelf). The data are available in the file barley.csv\nSuggest a suitable model for this situation. Identify the response and explanatory variables and list the levels for any qualitative variables. Write down the design matrix for each model you consider.\nPerform appropriate analyses to test if each of the following are important: (a) watering condition, (b) type of barley seed, and (c) shelf position.\nIn the analysis, do not include any interactions involving shelf position. If you find a significant interaction between watering condition and type of barley seed, carefully interpret the parameter estimates.\n\n\nClick here to see hints.\n\nBeware! This example has categorical explanatory variables and so don’t forget to use as.factor. After that you need to carefully form the model in the lm command and interpret the ANOVA table."
  },
  {
    "objectID": "2_linearmodels.html#footnotes",
    "href": "2_linearmodels.html#footnotes",
    "title": "2  Essentials of Normal Linear Models",
    "section": "",
    "text": "Dobson and Barnett, 3rd edition, Table 2.3.↩︎\nNotice that this is a special case of the one-way ANOVA when there are only two-groups.↩︎\nAykroyd, P.J. (1956). Unpublished.↩︎\nSource unknown.↩︎\nSource unknown.↩︎"
  },
  {
    "objectID": "3_GLM-Theory.html#motivating-examples",
    "href": "3_GLM-Theory.html#motivating-examples",
    "title": "3  GLM Theory",
    "section": "3.1 Motivating examples",
    "text": "3.1 Motivating examples\nWe cannot always assume that the dependent variable \\(Y\\) is normally distributed. For example, for the beetle mortality data in Table 1.1, suppose each beetle subjected to a dose \\(x_i\\) has a probability \\(p_i\\) of being killed. Then, the number of beetles killed \\(Y_i\\) out of a total number \\(m_i\\) at dose-level \\(x_i\\) will have a \\(\\text{Bin}(m_i,p_i)\\) distribution with probability mass function\n\\[\n\\text{Pr}(y_i ;~ p_i,m_i) = \\left(\\begin{array}{c} m_i\\\\ y_i \\end{array} \\right) p_i^{y_i} (1-p_i)^{m_i-y_i}\n\\tag{3.1}\\] where \\(y_i\\) takes values in \\(\\{0,1,\\dots,m_i\\}\\).\nTable 3.1 contains seasonal data on tropical cyclones for 13 seasons. Suppose that, within season \\(i\\), there is a constant probability \\(\\lambda_i dt\\) of a cyclone occurring in any short time-interval \\(dt\\). Then the total number of cyclones \\(Y_i\\) during season \\(i\\) will have a Poisson distribution with mean \\(\\lambda_i\\), that is \\(Y_i\\sim \\text{Po}(\\lambda_i)\\), with probability mass function \\[\n\\text{Pr}(y_i ;~ \\lambda_i) = \\frac{\\lambda_i^{y_i} e^{-\\lambda_i} }{y_i!}\n\\tag{3.2}\\] where \\(y_i\\) takes values in \\(\\{0,1,2,\\dots\\}\\).\n\n\nTable 3.1: Numbers of tropical cyclones in \\(n = 13\\) successive seasons1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeason\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nNo of cyclones\n6\n5\n4\n6\n6\n3\n12\n7\n4\n2\n6\n7\n4\n\n\n\n\nIn these two examples, we have non-normal data and would like to know whether and how the dependent variable \\(Y_i\\) depends on the covariate dose or season.\n\n\nCode\npar(mar=c(4,4,0,1), mgp=c(2,1,0))\n\nbeetle = read.table(\"https://richardpmann.com/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nseason = 1:13\ncyclones = c(6,5,4,6,6,3,12,7,4,2,6,7,4)\n\nplot(season, cyclones, pch=16,\n     ylim=c(0,12),\n     xlab=\"Season\", ylab=\"No. of cyclones\")\n\n\n\n\n\n\n\n\n(a) Binomial model\n\n\n\n\n\n\n\n(b) Poisson model\n\n\n\n\nFigure 3.1: Examples of non-normally distributed data.\n\n\n\nGeneralised linear models provide a modelling framework for data analysis in the non-normal setting. We will revisit the beetle mortality and cyclone data sets after describing the structure of a generalised linear model."
  },
  {
    "objectID": "3_GLM-Theory.html#focus-on-distributions-quiz",
    "href": "3_GLM-Theory.html#focus-on-distributions-quiz",
    "title": "3  GLM Theory",
    "section": "Focus on distributions quiz",
    "text": "Focus on distributions quiz\nTest your knowledge recall and application to reinforce basic ideas and prepare for similar concepts later in the Chapter.\n\n\n\nWhich of the following best describes the sample space of a binomial random variable? Non-negative integersPositive real numbersAll real numbersAn integar values between 0 and mAny real number between 0 and mNone of the above\nWhich of the following best describes the sample space of a Poisson random variable? Non-negative integersPositive real numbersAll real numbersAny real number between 0 and mAn integar values between 0 and mNone of the above\nWhich of the following best describes the sample space of a normal random variable? All real numbersPositive real numbersNon-negative integersAny real number between 0 and mAn integar values between 0 and mNone of the above\nFor the binomial distribution, which of the following best describes the range of values possible for log(p/(1-p))? Positive real numbersAll real numbersNon-negative integersAny real number between 0 and 1An integar values between 0 and 1None of the above\nFor the Poisson distribution, which of the following best describes the range of values possible for log(λ)? Non-negative integersPositive real numbersAn integar values between 0 and 1All real numbersAny real number between 0 and 1None of the above\nFor the normal distribution, which of the following best describes the range of values possible for µ? An integar values between 0 and 1Positive real numbersNon-negative integersAll real numbersAny real number between 0 and 1None of the above"
  },
  {
    "objectID": "3_GLM-Theory.html#sec-glmstructure",
    "href": "3_GLM-Theory.html#sec-glmstructure",
    "title": "3  GLM Theory",
    "section": "3.2 The GLM structure",
    "text": "3.2 The GLM structure\nA generalised linear model relates a continuous or discrete response variable \\(Y\\) to a set of explanatory variables \\(\\mathbf{x}=(x_1, \\ldots, x_p)\\). The model contains three parts:\nRandom part: The probability (mass or density) function of \\(Y\\) is assumed to belong to the two-parameter exponential family of distributions with parameters \\(\\theta\\) and \\(\\phi:\\)\n\\[\nf(y; \\theta, \\phi) = \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}\n                  {\\phi} + c(y, \\phi) \\right\\},\n\\tag{3.3}\\] where \\(\\phi&gt;0\\). Here, \\(\\theta\\) is called the canonical or natural parameter of the distribution and \\(\\phi\\) is called the scale parameter. We show below that the mean \\(\\mbox{E}[Y]\\) depends only on \\(\\theta\\), and \\(\\mbox{Var}[Y]\\) depends on \\(\\phi\\) and possibly also \\(\\theta\\). Various choices for functions \\(b(\\cdot)\\) and \\(c(\\cdot)\\) produce a wide variety of familiar distributions (see below). Sometimes we may set \\(\\phi=1\\); then Equation 3.3 is called the one-parameter exponential family.\nMost of the common statistical distributions are member of the regular exponential family, such as binomial, Poisson, geometric, gamma, normal, but not the Student’s t and uniform with unknown bounds.\nFurther, note that in some references to generalised linear models (such as Dobson and Barnett, 3rd edn.), \\(\\phi\\) does not appear at all in the exponential family formula Equation 3.3, instead it is absorbed into \\(\\theta\\) and \\(b(\\theta)\\).\nIn this module, we will generally assume that each observation \\(Y_i\\), \\(i=1,\\dots,n,\\) is independently drawn from an exponential family where \\(\\theta\\) depends on the covariates. Thus we write \\[\nf(y_i; \\theta_i, \\phi) = \\exp \\left\\{ \\frac{y_i \\theta_i - b(\\theta_i)}   {\\phi} + c(y_i, \\phi) \\right\\}.\n\\] Note the subscripts on both \\(y\\) and \\(\\theta\\), and hence the observations may not be identically distributed.\nSystematic part: This is a linear predictor: \\[\n\\eta = \\sum_{j=1}^p \\beta_j x_j.\n\\tag{3.4}\\] Note that the symbol \\(\\eta\\) is pronounced eta.\nLink function: This is a one-to-one function providing the link between the linear predictor \\(\\eta\\) and the mean \\(\\mu = \\mbox{E}[Y]\\):\n\\[\n\\eta = g(\\mu), \\quad \\mbox{and} \\quad \\mu  = g^{-1}(\\eta) = h(\\eta).\n\\tag{3.5}\\]\nHere, \\(g(\\mu)\\) is called the link function, and \\(h(\\eta)\\) is called the inverse link function.\nWe will now discuss each of these parts in more detail."
  },
  {
    "objectID": "3_GLM-Theory.html#focus-on-glm-structure-quiz",
    "href": "3_GLM-Theory.html#focus-on-glm-structure-quiz",
    "title": "3  GLM Theory",
    "section": "Focus on GLM structure quiz",
    "text": "Focus on GLM structure quiz\nTest your knowledge recall and application to reinforce basic ideas and prepare for similar concepts later in the Chapter\n\n\n\nWhat symbols are usually used for the parameters in the definition of the two-parameter exponential family? θ and φb and cy, θ and φy and xy and φNone of the above\nWhich of the following is NOT a member of the two-parameter exponential family? binomialPoissonBernoullinormalStudent’s tNone of the above\nWhich of the following is NOT part of the usual definition of a generalised linear model? Random partSystematic partDesign matrixLink functionNone of the above\nIn the definition of a GLM, which of the following best describes an assumption about the distribution of the response variable? SystematicMember of the exponential familyLink functionLinear function of the explanatory variablesNone of the above\nWhich part of the GLM definition involves the mean of the response variable and the explanatory variables? Linear predictorExpectation of YRegression parameters, βLink functionNone of the above"
  },
  {
    "objectID": "3_GLM-Theory.html#the-random-part-of-a-glm",
    "href": "3_GLM-Theory.html#the-random-part-of-a-glm",
    "title": "3  GLM Theory",
    "section": "3.3 The random part of a GLM",
    "text": "3.3 The random part of a GLM\nWe begin with some examples of exponential family members.\n\nExample: Poisson distribution\nIf \\(Y\\) has a Poisson distribution with parameter \\(\\lambda\\), that is \\(Y \\sim \\text{Po}(\\lambda)\\), then \\(Y\\) takes values in \\(\\{0,1,2,\\dots\\}\\) and has probability mass function: \\[\nf(y) = \\frac{e^{-\\lambda} \\lambda^y} {y!}  \n= \\exp \\left\\{y \\log \\lambda - \\lambda - \\log y! \\right\\},\n\\tag{3.6}\\] which has the form of Equation 3.3 with components as in Table 3.2.\n\n\nTable 3.2: Exponential model components for the Poisson\n\n\n\\(\\theta\\)\n\\(\\phi\\)\n\\(b(\\theta)\\)\n\\(c(y,\\phi)\\)\n\n\n\n\n\\(\\log\\lambda\\)\n\\(1\\)\n\\(\\lambda=e^\\theta\\)\n\\(-\\log y!\\)\n\n\n\n\nFor example, to model the cyclone data in Table 3.1, we might simply assume that the number of cyclones in each season has a Poisson distribution, assuming a constant rate \\(\\lambda\\) across all seasons \\(i\\). That is \\(Y_i \\sim \\text{Po}(\\lambda).\\) The parameter would be simply estimated by the sample mean, \\(\\hat\\lambda=\\bar y=\\) 5.5384615.\n\n\n\nCode\npar(mar=c(4,4,0,1), mgp=c(2,1,0))\n\nseason = 1:13\ncyclones = c(6,5,4,6,6,3,12,7,4,2,6,7,4)\nn = length(cyclones)\n\nplot(season, cyclones, pch=16,\n     ylim=c(0,12),\n     xlab=\"Season\", ylab=\"No. of cyclones\")\nabline(h=mean(cyclones), lty=2)\n\nhist(cyclones, breaks=1:13-0.5, main=\"\")\nfitted=n*dpois(0:12,mean(cyclones))\nlines(0:12, fitted, type='h', lwd=3)\n\n\n\n\n\n\n\n\n(a) No. of cyclones against season with mean\n\n\n\n\n\n\n\n(b) Fitted Poisson model assuming constant rate\n\n\n\n\nFigure 3.2: Poisson model fitted to cyclone data.\n\n\n\n\nExample: Binomial distribution\nLet \\(Y\\) have a Binomial distribution, that is \\(Y \\sim \\text{Bin}(m, p)\\), with \\(m\\) fixed. Then \\(Y\\) is discrete, taking values in \\(\\{0,1,\\dots,m\\}\\), and has probability mass function: \\[\nf(y) = {m \\choose y} p^y (1 - p)^{m - y} = {m \\choose y} \\left(\\frac{p}{1-p} \\right)^y (1 - p)^m\n\\] which can be re-written as \\[\nf(y)  = \\exp \\left\\{ y\\ \\mbox{logit } p  + m \\log (1-p) + \\log {m \\choose y}\\right\\},\n\\tag{3.7}\\] which has the form of Equation 3.3 with, \\[\n\\theta=\\text{logit} \\ p = \\log \\left( \\frac{p}{1-p}\\right),\n\\] and with components as in Table 3.3.\n\n\nTable 3.3: Exponential model components for the Binomial\n\n\n\\(\\theta\\)\n\\(\\phi\\)\n\\(b(\\theta)\\)\n\\(c(y,\\phi)\\)\n\n\n\n\n\\(\\mbox{logit }p\\)\n\\(1\\)\n\\(m\\log(1+e^\\theta)\\)\n\\(\\log{m\\choose y}\\)\n\n\n\n\nNote that it can be shown that \\(-m\\log(1-p)=m\\log(1+e^\\theta)\\) – see Exercises.\n\n\n\nCode\nbeetle = read.table(\"https://richardpmann.com/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\ny = cbind(beetle$died, beetle$total-beetle$died)\nglm.fit = glm(y ~ dose, family=binomial(link='logit'))\n\noutput.dose = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=output.dose), type=\"response\")\nlines(output.dose, fitted)\n\n\n\n\n\nFigure 3.3: Binomial model fitted to beetle data.\n\n\n\n\n\nExample: Normal distribution\nLet \\(Y\\) have a Normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\), that is \\(Y\\sim N(0, \\sigma^2)\\). Then \\(Y\\) takes values on the whole real line and has probability density function\n\\[\\begin{align*}\nf(y; \\mu, \\sigma^2) &= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ \\frac{-1}{2\\sigma^2} (y - \\mu)^2 \\right\\}, \\notag \\\\\n&= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{-\\frac{y^2}{2 \\sigma^2} + \\frac{y\\mu}{\\sigma^2} - \\frac{\\mu^2}{2 \\sigma^2}\\right\\}\\\\\n&= \\exp \\left\\{ \\frac{y \\mu - \\mu^2/2}{\\sigma^2} + \\left[\\frac{-y^2}{2\\sigma^2} - \\frac{1}{2} \\log (2 \\pi \\sigma^2)\n\\right]\\right\\},\n\\end{align*}\\] which has the form of Equation 3.3 with components as in Table 3.4.\n\n\nTable 3.4: Exponential model components for the Gaussian\n\n\n\n\n\n\n\n\n\\(\\theta\\)\n\\(\\phi\\)\n\\(b(\\theta)\\)\n\\(c(y,\\phi)\\)\n\n\n\n\n\\(\\mu\\)\n\\(\\sigma^2\\)\n\\(\\theta^2/2\\)\n\\(-\\frac{y^2}{2\\phi} - \\frac{1}{2} \\log (2 \\pi \\phi)\\)\n\n\n\n\nFrom the usual regression point of view, we write \\(y = \\alpha + \\beta x + \\epsilon\\), with \\(\\epsilon \\sim N(0, \\sigma^2)\\). From the point of view of a generalised linear model, we write \\(Y \\sim N(\\mu, \\sigma^2)\\) where \\(\\mu(x) = \\alpha + \\beta x\\)."
  },
  {
    "objectID": "3_GLM-Theory.html#sec-exponential-family",
    "href": "3_GLM-Theory.html#sec-exponential-family",
    "title": "3  GLM Theory",
    "section": "3.4 Moments of exponential-family distributions",
    "text": "3.4 Moments of exponential-family distributions\nIt is straightforward to find the mean and variance of \\(Y\\) in terms of \\(b(\\theta)\\) and \\(\\phi\\). Since we want to explore the dependence of \\(\\mbox{E}[Y]\\) on explanatory variables, this property makes the exponential family very convenient.\n\nProposition 3.1 For random variables in the exponential family: \\[\n\\mbox{E}[Y] = b'(\\theta), \\quad \\mbox{and } \\quad \\mbox{Var}[Y] =  b''(\\theta)\\phi.\n\\tag{3.8}\\]\n\nProof We give the proof for a continuous random variables. For the discrete case, replace all integrals by sums – see Exercises.\nStarting with the simple property that all probability density functions integrate to 1, we have \\[\n1 = \\int \\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{\\phi} + c(y,\\phi)\\right\\} dy\n\\] and then differentiating both sides with respect to \\(\\theta\\) gives \\[\n0 = \\int \\left[\\frac{ y - b'(\\theta)}{\\phi} \\right]\\exp \\left\\{ \\frac{y \\theta - b(\\theta)}{\\phi} + c(y,\\phi)\\right\\}\\ dy.\n\\tag{3.9}\\] Next, using the definition of the exponential family to simplify the equation gives \\[\n0 = \\int \\left[\\frac{ y - b'(\\theta)}{\\phi} \\right] f(y; \\theta)\\ dy\n\\] and expanding the brackets leads to \\[\n0 = \\frac{1}{\\phi} \\left(\\int y f(y; \\theta) dy - b'(\\theta) \\int f(y;\\theta)\\ dy \\right).\n\\] The first integral is simply the expectation of \\(Y\\) and the second is the integral of the probability density function of \\(Y\\), and hence \\[\n0 = \\frac{1}{\\phi} \\left(\\mbox{E}[Y] - b'(\\theta)\\right)\n\\] which implies that \\[\n\\mbox{E}[Y] = b'(\\theta),\n\\tag{3.10}\\] which proves the first part of the proposition.\nDifferentiating Equation 3.9 by parts and then using the definition of the exponential family to simplify again yields \\[\n0 = \\int \\left\\{ -\\frac{b''(\\theta)}{\\phi} + \\left[\\frac{ y - b'(\\theta)}{\\phi} \\right]^2 \\right\\} f(y; \\theta)\\ dy\n\\] and using Equation 3.10 gives, \\[\n0  =  -\\frac{b''(\\theta)}{\\phi} +\\int \\left[\\frac{ y - \\mbox{E}[Y]}{\\phi} \\right]^2  f(y; \\theta)\\ dy\n\\] \\[\n0 = -\\frac{b''(\\theta)}{\\phi} + \\frac{\\mbox{Var}[Y]}{\\phi^2}\n\\] which implies that \\[\n\\mbox{Var}[Y] = \\phi \\ b''(\\theta).\n\\] which proves the second part of the proposition.\nTogether, these two results allow us to write down the expectation and variance for any random variable once we have shown that it is a member of the exponential family.\n\nExample: Poisson and normal distribution moments\n\n\nTable 3.5: Summary of moment calculations via exponential family properties \n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\theta\\)\n\\(b(\\theta)\\)\n\\(\\phi\\)\n\\(\\mbox{E}[Y]=b'(\\theta)\\)\n\\(b''(\\theta)\\)\n\\(\\mbox{Var}[Y]=b''(\\theta)\\phi\\)\n\n\n\n\nPoisson, \\(Po(\\lambda)\\)\n\\(\\log \\lambda\\)\n\\(e^\\theta\\)\n\\(1\\)\n\\(e^\\theta=\\lambda\\)\n\\(e^\\theta\\)\n\\(e^\\theta\\times 1=\\lambda\\)\n\n\nNormal, \\(N(\\mu,\\sigma^2)\\)\n\\(\\mu\\)\n\\(\\theta^2/2\\)\n\\(\\sigma^2\\)\n\\(\\theta=\\mu\\)\n\\(1\\)\n\\(1\\times \\sigma^2=\\sigma^2\\)"
  },
  {
    "objectID": "3_GLM-Theory.html#sec-systematic",
    "href": "3_GLM-Theory.html#sec-systematic",
    "title": "3  GLM Theory",
    "section": "3.5 The systematic part of the model",
    "text": "3.5 The systematic part of the model\nThe second part of the generalised linear model, the linear predictor, is given in as \\(\\eta = \\sum_{j=1}^p \\beta_j x_j\\), where \\(x_j\\) is the \\(j\\)th explanatory variable (with \\(x_1=1\\) for the intercept). Now, for each observation \\(y_i,\\ i=1,\\dots,n\\), the explanatory variables may differ. To make explicit this dependence on \\(i\\), we write: \\[\n\\eta_i = \\sum_{j=1}^p \\beta_j x_{ij},\n\\tag{3.11}\\] where \\(x_{ij}\\) is the value of the \\(j\\)th explanatory variable on individual \\(i\\) (with \\(x_{i1}=1\\)). Rewriting this in matrix notation: \\[\n\\eta = X \\beta,\n\\tag{3.12}\\] where now \\(\\boldsymbol{\\eta} = (\\eta_1,\\dots,\\eta_n)\\) is a vector of linear predictor variables, \\(\\boldsymbol{\\beta} = (\\beta_1,\\dots,\\beta_p)\\) is a vector of regression parameters, and \\(X\\) is the \\(n\\times p\\) design matrix.\nRecall from Section 1.4 and Section 2.3 that we are concerned with two kinds of explanatory variable:\n\nQuantitative — for example, \\(x \\in (-\\infty, \\infty)\\) etc.\nQualitative — for example, \\(x \\in \\{A, B, C\\}\\) etc.\n\nAs discussed in Section 2.4, each quantitative variable is represented in \\(X\\) by an \\(n \\times 1\\) column vector. Each qualitative variable, with \\(k+1\\) levels, say, is represented by a dummy \\(n \\times k\\) matrix (one column, usually the first, being dropped to avoid identification problems) of 0’s and 1’s ."
  },
  {
    "objectID": "3_GLM-Theory.html#the-link-function",
    "href": "3_GLM-Theory.html#the-link-function",
    "title": "3  GLM Theory",
    "section": "3.6 The link function",
    "text": "3.6 The link function\nIn Section 3.2 we saw that the random part of an observation, \\(y\\), might be described by a member of the exponential family. We also saw, that the systematic part of \\(y\\) might be described using a linear predictor, \\(\\eta,\\) of the explanatory variables. Further, we introduced the notion of a link function \\(\\eta = g(\\mu)\\) to bring these two parts together, where \\(\\mu\\) is the mean of \\(y\\).\nOccasionally, the choice of link function \\(g(\\mu)\\) is motivated by theory underlying the data at hand. For example, in a dose–response setting, the appropriate model might be motivated by the solution to a set of partial differential equations describing the flow through the body of a dose of a drug.\nWhen there is no compelling underlying theory, however, we typically choose a link function that will transform a restricted range of the dependent variable onto the whole real line. For example, when observations are typically positive, so we have \\(\\mu&gt;0\\), we might choose the logarithmic link: \\[\ng(\\mu) = \\log(\\mu).\n\\tag{3.13}\\]\nWhen observations are binomial counts from \\(B(m,p), \\ 0 &lt; p &lt; 1\\), with mean \\(\\mu = mp\\), we might choose the logit link from \\[\n\\eta = g(\\mu) = \\text{logit}(\\mu/m)= \\text{logit}(p) = \\log\\{p/(1-p)\\}\n\\tag{3.14}\\] or the probit link which is the inverse of the cumulative distribution function of the \\(N(0,1)\\) distribution: \\[\n\\eta = g(\\mu) = \\Phi^{-1}(\\mu/m) = \\Phi^{-1}(p),\n\\tag{3.15}\\] or the complementary log-log (cloglog) link: \\[\n\\eta = g(\\mu) = \\log(-\\log(1-\\mu/m))= \\log(-\\log(1-p)),\n\\tag{3.16}\\] or the cauchit link which is the inverse of the cumulative distribution function of the Cauchy (\\(t_1\\)) distribution: \\[\n\\eta = g(\\mu) = \\tan(\\pi(\\mu/m-\\tfrac{1}{2})) = \\tan(\\pi(p-\\tfrac{1}{2})).\n\\tag{3.17}\\] Figure 3.4 shows these link functions for proportions fitted to the beetle mortality data. This demonstrates that the logit and probit links are very similar, that the complementary log-log link fits these data slightly better in the extremes, but that the cauchit link fits these data quite poorly in the extremes.\n\n\nCode\npar(mar=c(3.5,3.5,0,0), mgp=c(2.5,0.75,0))\nbeetle = read.table(\"https://richardpmann.com/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\nym = cbind(beetle$died,beetle$total-beetle$died)\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1,1.1), ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nglm.fit = glm(ym~dose, family=binomial(link='logit'))\ntmp = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=tmp), type=\"response\")\nlines(tmp, fitted)\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1,1.1), ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nglm.fit = glm(ym~dose, family=binomial(link='probit'))\ntmp = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=tmp), type=\"response\")\nlines(tmp, fitted)\n\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1,1.1), ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nglm.fit = glm(ym~dose, family=binomial(link='cloglog'))\ntmp = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=tmp), type=\"response\")\nlines(tmp, fitted)\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1,1.1), ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\nglm.fit = glm(ym~dose, family=binomial(link='cauchit'))\ntmp = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=tmp), type=\"response\")\nlines(tmp, fitted)\n\n\n\n\n\n\n\n\n(a) Logit link\n\n\n\n\n\n\n\n(b) Probit link\n\n\n\n\n\n\n\n\n\n(c) cloglog link\n\n\n\n\n\n\n\n(d) Cauchit link\n\n\n\n\nFigure 3.4: Dose–response curves fitted to the beetle mortality data from Table 1.1 with different choices of link function.\n\n\n\nA mathematically and computationally convenient choice of link function \\(g(\\mu)\\) can be constructed by setting: \\[\n\\theta =\\eta,\n\\tag{3.18}\\] where \\(\\theta\\) is the canonical parameter of the exponential family as defined in Equation 3.3. Then, Equation 3.8 shows that the mean \\(\\mu\\) is a function of \\(\\theta\\) and therefore, Equation 3.18 indirectly provides a link between \\(\\mu\\) and \\(\\eta\\). That is, Equation 3.18 implicitly defines a link function \\(\\eta=g(\\mu)\\). But what is the form of this \\(g(\\cdot)\\)?\nFrom Equation 3.8, \\[\n\\mu = b'(\\theta).\n\\] So, provided function \\(b'(\\cdot)\\) has an inverse \\((b')^{-1}(\\cdot)\\), we may write \\[\n\\theta = (b')^{-1}(\\mu).\n\\tag{3.19}\\] Now, from Equation 3.5, \\(g(\\mu) = \\eta\\), so using Equation 3.18: \\[\ng(\\mu) = \\theta = (b')^{-1}(\\mu),\n\\tag{3.20}\\] from Equation 3.19. This makes explicit the \\(g(\\mu)\\) that is implicitly asserted by Equation 3.18. The function produced form Equation 3.20 is called the canonical link function.\n\nProposition 3.2 For the canonical link function, \\[\ng'(\\mu) = 1/b''(\\theta).\n\\]\n\nProof: From Proposition 3.1, \\(\\mu = E[Y]=b'(\\theta)\\), so \\[\n\\frac{\\text{d} \\mu }{\\text{d} \\theta} = b''(\\theta).\n\\] From Equation 3.20, for the canonical link function, we have \\(\\theta = g(\\mu)\\), so \\[\n\\frac{\\text{d} \\theta }{ \\text{d} \\mu} = g'(\\mu).\n\\] Now \\(\\text{d} \\theta /\\text{d} \\mu = \\left(\\text{d} \\mu / \\text{d} \\theta\\right)^{-1}\\) and hence \\[\ng'(\\mu) = 1/b''(\\theta).\n\\] Which proves the proposition.\n\nExample: Poisson canonical link function\nFor the Poisson distribution \\(\\text{Po}(\\lambda)\\), we have from Table 3.2 that \\(b(\\theta) = e^\\theta\\). Therefore, \\[\nb'(\\theta) = e^\\theta,\n\\] so the inverse of function \\(b'(\\cdot)\\) exists and is the inverse of the exponential function, which is the logarithmic function. Then, applying Equation 3.20 \\[\ng(\\mu) = \\log(\\mu)\n\\] Thus the canonical link for the Poisson distribution is \\(\\log\\).\n\n\nExample: Normal canonical link function\nFor the Normal distribution \\(N(\\mu, \\sigma^2)\\), we have from Table 3.4 that \\(b(\\theta) = \\theta^2/2\\). Therefore \\[\nb'(\\theta) = \\theta\n\\] so the inverse of function \\(b'(\\cdot)\\) exists and is the inverse of the identity function, which is the identity function. (The identity function is that which maps a value onto itself.) Then, applying Equation 3.20, \\[\ng(\\mu) = \\mu. \\label{eq:canonical.linkfun.normal}\n\\] Thus the canonical link for the Normal distribution is the identity function.\n\nFor many models, \\(\\mu\\) has a restricted range, but we would like \\(\\eta\\) to have unlimited range. It turns out, for several members of the exponential family, that the canonical link function provides \\(\\eta\\) with unlimited range. However, Table 3.6 shows that this is not always so.\n\n\nTable 3.6: Canonical link functions and their ranges (see McCullagh and Nelder, 2nd Edn., p291 with \\(\\dagger\\)binomial distribution with index \\(m\\) and mean \\(\\mu\\) and \\(\\ddagger\\)gamma distribution with mean \\(\\mu\\) (see Exercises for details).\n\n\n\n\n\n\n\n\n\n\n\\(f(y)\\)\nRange of \\(\\mu\\)\n\\(b(\\theta)\\)\n\\(\\mu=b'(\\theta)\\)\nCanonical link, \\(g(\\mu)\\)\nRange of \\(\\eta\\)\n\n\n\n\nNormal\n\\((-\\infty, \\infty)\\)\n\\(\\frac12 \\theta^2\\)\n\\(\\theta\\)\n\\(\\mu\\)\n\\((-\\infty, \\infty)\\)\n\n\nPoisson\n\\((0,\\infty)\\)\n\\(e^\\theta\\)\n\\(e^\\theta\\)\n\\(\\log\\mu\\)\n\\((-\\infty, \\infty)\\)\n\n\nBinomial\\(\\dagger\\)\n\\((0, m)\\)\n\\(m\\log(1+e^\\theta)\\)\n\\(m/(1+e^{-\\theta})\\)\n\\(\\mbox{logit}(\\mu/m)\\)\n\\((-\\infty, \\infty)\\)\n\n\nGamma\\(\\ddagger\\)\n\\((0,\\infty)\\)\n\\(-\\log (-\\theta)\\)\n\\(-\\theta^{-1}\\)\n\\(-\\mu^{-1}\\)\n\\((-\\infty, 0)\\)\n\n\n\n\nWhy is the canonical link function Equation 3.20 convenient? The assertion Equation 3.18 means that, in the exponential-family formula Equation 3.3, we can simply substitute the linear predictor \\[\n\\eta=\\sum_j \\beta_j x_j\n\\] from Equation 3.4 in place of \\(\\theta\\), to give: \\[\n    f(y; \\mathbf{x}, \\boldsymbol{\\beta},\\phi) = \\exp \\left\\{ \\frac{y \\left[\\sum_j \\beta_j x_j\\right] - b\\left(\\left[\\sum_j \\beta_j x_j\\right]\\right)}\n                                            {\\phi} + c(y, \\phi) \\right\\},\n\\tag{3.21}\\] where \\(\\mathbf{x}=\\{x_{j}, j=1,\\dots,p\\}\\) and \\(\\boldsymbol{\\beta}=\\{\\beta_j, j=1,\\dots,p\\}\\).\nFurther, suppose we have \\(n\\) independent observations, \\(\\{y_i,\\ i=1,\\ldots,n\\}\\). As discussed in Section 3.5, the explanatory variables \\((x_{1},\\ldots,x_{p})\\) will depend on \\(i\\), and so \\(\\eta\\) will also depend on \\(i\\). Therefore, we attach subscript \\(i\\) to \\(y\\) and to each \\(x_j\\), giving: \\[\nf(y_i; \\mathbf{x}_i, \\boldsymbol{\\beta}, \\phi) =\n\\exp \\left\\{ \\frac{y_i \\left[\\sum_j \\beta_j x_{ij}\\right] - b\\left(\\left[\\sum_j \\beta_j x_{ij}\\right]\\right)}{\\phi}\n+ c(y_i, \\phi) \\right\\}.\n\\tag{3.22}\\] where \\(\\mathbf{x}_i=\\{x_{ij}, j=1,\\dots,p\\}\\) and \\(\\boldsymbol{\\beta}=\\{\\beta_j, j=1,\\dots,p\\}\\).\nBy independence, the joint distribution of all observations \\(\\mathbf{y} = \\{y_i,\\ i=1,\\dots,n\\}\\), with design matrix \\(X = \\{x_{ij},\\ \\ i=1,\\dots,n; j=1,\\dots,p\\},\\) is: \\[\nf(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) = \\prod_{i=1}^n f(y_i; \\theta_i, \\phi),\n\\] so\n\\[\n\\log f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) =\n\\sum_{i=1}^n \\log f(y_i; \\theta_i, \\phi)\n\\] then substituting in using Equation 3.22 gives \\[\n\\log f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) =\n\\sum_{i=1}^n\n\\left\\{  \\frac{y_i \\left[\\sum_j \\beta_j x_{ij}\\right] - b\\left(\\left[\\sum_j \\beta_j x_{ij}\\right]\\right)}{\\phi} + c(y_i, \\phi)\\right\\}\n\\] and finally simplifying to give \\[\n\\log f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) =\n\\frac{\\sum_j \\beta_j S_j - \\sum_i b\\left(\\left[\\sum_j \\beta_j x_{ij}\\right]\\right)}\n{\\phi} + \\sum_i c(y_i, \\phi)\n\\tag{3.23}\\] where \\[\nS_j = \\sum_{i=1}^n  y_i  x_{ij}.\n\\] Thus, in the log-likelihood Equation 3.23, it is only the first term that involves both the observations \\(\\mathbf{y}=\\{y_i,\\ i=1,\\dots,n\\}\\) and the parameters \\(\\boldsymbol{\\beta}=\\{\\beta_j, j=1,\\dots,p\\}\\), and this term depends on the observations only through the statistics \\(\\mathbf{S}=\\{S_j, j=1,\\dots,p\\}\\) – these are called sufficient statistics, and their appearance in Equation 3.23 confers both theoretical and practical advantages."
  },
  {
    "objectID": "3_GLM-Theory.html#focus-on-link-functions-quiz",
    "href": "3_GLM-Theory.html#focus-on-link-functions-quiz",
    "title": "3  GLM Theory",
    "section": "Focus on link functions quiz",
    "text": "Focus on link functions quiz\nTest your knowledge recall and application to reinforce basic ideas and prepare for similar concepts later in the module.\n\n\n\nIf the reponse variable Y can take only positive values, then which of the following transformations would produce an unrestricted range? 1/Ylog(Y)logit(Y)exp(Y)None of the above\nIf the range of the reponse variable Y is restricted to the interval (0,1), then which of the following transformations would produce an unrestricted range? 1/Ylogit(Y)log(Y)exp(Y)None of the above\nIf response variable Y follows a normal distribution, then which of the following transformations would produce an unrestricted range? log(Y)√Ylogit(Y)exp(Y)None of the above\nIf the range of the reponse variable Y is a percentage, then which of the following transformations would produce an unrestricted range? 1/Yexp(Y)log(Y)logit(Y/100)None of the above\nIf the reponse variable Y can take an real value, then which of the following transformations would produce an unrestricted range? LogisticIdentityLogSquare-rootNone of the above"
  },
  {
    "objectID": "3_GLM-Theory.html#exercises",
    "href": "3_GLM-Theory.html#exercises",
    "title": "3  GLM Theory",
    "section": "3.7 Exercises",
    "text": "3.7 Exercises\n\n3.1 Consider the beetle data again, see Table 1.1, but suppose that we had only been given the \\(y\\) values, that is the number killed, and misinformed that each came from a sample of size \\(62\\). Further, suppose that we did not know that different doses had been used. That is, we where given data: \\(\\mathbf{y}=\\{6, 13, 18, 28, 52, 53, 61, 60\\}\\) and led to believe that the model \\(Y\\sim \\mbox{Bin}(62,p)\\) was appropriate. Use the given data to estimate \\(p\\). Then, calculate the fitted probabilities and superimpose them on a histogram of the data.\n\n\nClick here to see hints.\n\nSee the code chunk used to create Figure 3.2.\n\n3.2 Verify that, in general, if \\(q = 1/(1+e^{-x})\\) then \\(x = \\log(q/(1-q))\\) and then for the binomial distribution, \\(Y\\sim \\mbox{Bin}(m,p)\\), show that \\[\n-m\\log(1-p)=m\\log(1+e^\\theta)\n\\] where \\(\\theta=\\mbox{logit }p = \\log(p/(1-p))\\).\n\n\nClick here to see hints.\n\nEach deviation requires algebraic rearrangement and simplification only.\n\n3.3 Suppose that \\(Y\\) has a gamma distribution with parameters \\(\\alpha\\) and \\(\\beta\\), that is \\(Y\\sim \\mbox{Gamma}(\\alpha, \\lambda)\\), with probability density function \\[\nf(y;\\alpha, \\lambda) = \\frac{\\lambda^\\alpha y^{\\alpha-1} e^{-\\lambda y}}{\\Gamma(\\alpha)} \\quad \\qquad y &gt; 0; \\alpha,\\lambda&gt;0.\n\\] Write this in the form of the exponential family and clearly identify \\(\\theta\\), \\(\\phi\\), \\(b(\\theta)\\) and \\(c(y,\\phi)\\) – as was done for the Poisson and binomial in Table 3.2 and Table 3.3.\n\n\nClick here to see hints.\n\nThis is a difficult one and you might need to try a few rearrangements. Consider treating \\(1/\\alpha\\) as a scale parameter and let \\(\\theta\\) be a suitable function of \\(\\lambda\\) and \\(\\alpha\\).\n\n3.4 Express the geometric distribution, \\(Y\\sim\\mbox{Geom}(p), 0 &lt; p &lt;1\\), with probability mass function \\[\nf(y) = (1 - p)^{y-1} p; \\hspace{1cm} y = 1, 2, 3 \\ldots\n\\] as an exponential family distribution.\n\n\nClick here to see hints.\n\nFollow the same process as usual and you might get an unexpected \\(c(y,\\phi)\\)!\n\n3.5 Prove Proposition 3.1 assuming that \\(Y\\) follows a discrete distribution. Use the proposition, starting from the given \\(b(\\theta)\\), to verify the results for expectation and variance of the Poisson in Table 3.5 and then derive similar results for the binomial, \\(Y\\sim \\mbox{Bin}(m,p)\\).\n\n\nClick here to see hints.\n\nReplace the integration in the continuous case by summation for the discrete – we know that the sum of the probabilities for a discrete random variable equals 1. Remember, however, that \\(\\theta\\) is still a real number and so differentiating with respect to \\(\\theta\\) still makes sense. For the binomial, go through the steps of finding \\(b(\\theta)\\) and then it’s first and second derivative.\n\n3.6 Use the properties of exponential families to find the expectation and variance of each of the geometric and gamma distributions considered above.\n\n\nClick here to see hints.\n\nUse the results from question 3.3 and 3.4, and apply Proposition 3.1.\n\n3.7 Using Equation 3.20, verify the canonical link function, \\(g(\\mu)\\), for the binomial and gamma distributions shown in Table 3.6.\n\n\nClick here to see hints.\n\nUse the appropriate \\(b'(\\theta)\\) and, using \\(\\mu=b'(\\theta)\\) and Equation 3.20, rearrange to find \\(\\theta\\) and hence \\(g(\\mu)\\)."
  },
  {
    "objectID": "3_GLM-Theory.html#footnotes",
    "href": "3_GLM-Theory.html#footnotes",
    "title": "3  GLM Theory",
    "section": "",
    "text": "Dobson and Barnett, 3rd edn, Table 1.2↩︎"
  },
  {
    "objectID": "4_GLM-Fitting.html#sec-mleiid",
    "href": "4_GLM-Fitting.html#sec-mleiid",
    "title": "4  GLM Estimation",
    "section": "4.1 The identically distributed case",
    "text": "4.1 The identically distributed case\n\n4.1.1 Maximum likelihood estimation\nSuppose we have \\(n\\) independent and identically distributed (i.i.d.) observations \\(\\mathbf{y} = \\{y_i,\\ i=1,\\dots,n\\}\\), where each \\(y_i\\) is sampled from the same exponential family density \\[\nf(y_i; \\theta,\\phi) = \\exp \\left\\{ \\frac{y_i\\theta  - b(\\theta)}{\\phi} + c(y_i, \\phi) \\right\\},\n\\tag{4.1}\\] for \\(i=1,\\dots,n.\\) In this case, the canonical parameter \\(\\theta\\) does not depend on \\(i\\) as the observations are identically distributed and hence must have the same parameter.\nBy independence, the joint distribution of all the observations \\(\\mathbf{y}\\) is: \\[\nf(\\mathbf{y}; \\theta,\\phi) = \\prod_{i=1}^n f(y_i; \\theta, \\phi).\n\\] So, taking logs and then substituting for the probability function using the exponential family form, Equation 3.3, gives \\[\n\\log f(\\mathbf{y}; \\theta,\\phi) = \\sum_{i=1}^n \\log f(y_i; \\theta, \\phi)\n= \\sum_{i=1}^n \\left[\\frac{y_i\\theta  - b(\\theta)}{\\phi} + c(y_i, \\phi)\\right].\n\\] Regarding the observations \\(\\mathbf{y}\\) as constants (which they are, once we have data) and the scale parameter \\(\\phi\\) as a fixed nuisance parameter (whose value we may not know), the log-likelihood as a function of the parameter of interest \\(\\theta\\) is: \\[\nl(\\theta; \\mathbf{y},\\phi)  = n\\left( \\frac{\\bar{y}\\, \\theta  -  b(\\theta)}{\\phi}\\right) + \\mbox{constant},\n\\tag{4.2}\\] where \\(\\bar{y}= \\sum y_i/n.\\)\nWe estimate \\(\\theta\\) by maximizing the log likelihood – i.e. given the data \\(\\mathbf{y}\\), we estimate the value of \\(\\theta\\) to be that value for which the likelihood, and hence the log-likelihood, is greatest.\nWe maximize the log-likelihood by differentiating it and setting it to zero: \\[\n\\frac{d l(\\theta; \\mathbf{y},\\phi)}{d \\theta}  \n= n \\left(\\frac{\\bar{y} -  b'(\\theta)}{\\phi}\\right)\n\\] and hence the MLE for \\(\\theta\\), which we denote \\(\\hat\\theta\\), satisfies \\[\nb'(\\hat\\theta) = \\bar{y}\n\\tag{4.3}\\] and hence \\[\n\\hat\\theta =  (b')^{-1}(\\bar{y}).\n\\tag{4.4}\\]\nFurther, we showed in Proposition 3.1 that \\(\\mbox{E}[Y] = \\mu =b'(\\theta)\\) and if we let \\(\\hat\\mu\\) denote the MLE of \\(\\mu\\), then \\(\\hat\\mu = b'(\\hat{\\theta})\\)1, hence we have \\(\\hat\\mu = \\bar{y}\\). So we find that \\(\\hat\\theta\\) is the value of \\(\\theta\\) for which the theoretical mean \\(\\hat\\mu = b'(\\hat\\theta)\\) matches the sample mean \\(\\bar{y}\\).\n\nExample: MLE of the Poisson distribution\nFor the Poisson distribution, \\(\\text{Po}(\\lambda)\\), we have found that \\(b(\\theta) = e^\\theta\\) and therefore \\(b'(\\theta) = e^\\theta\\). Hence, the MLE of natural parameter \\(\\theta\\) is found as the solution of \\(b'(\\hat\\theta) = e^{\\hat\\theta} = \\bar{y}\\), that is \\(\\hat\\theta = \\log(\\bar{y})\\).\n\n\n\n4.1.2 Estimation accuracy\nFor our i.i.d. sample \\(\\mathbf{y} = \\{y_i,\\ i=1,\\dots,n\\}\\), we have \\(b'(\\hat\\theta) = \\hat\\mu = \\bar y\\). Let \\(\\theta_0\\) be the true value of \\(\\theta\\) with corresponding mean \\(\\mu_0\\), i.e. \\[\nb'(\\theta_0) = \\mu_0.  \n\\tag{4.5}\\] How accurate is \\(\\hat\\theta\\)? We know that \\[\n\\mbox{E}[\\bar Y] = \\mbox{E}\\left[\\frac{1}{n}\\sum_{i=1}^n Y_i\\right]\n= \\frac{1}{n} \\sum_{i=1}^n \\mbox{E}[Y_i]\n= \\mu_0  \n= b'(\\theta_0),\n\\tag{4.6}\\] using Equation 4.5. Also, \\[\n\\mbox{Var}[\\bar Y] = \\mbox{Var}\\left[\\frac{1}{n} \\sum_{i=1}^n Y_i\\right]\n= \\frac{1}{n^2}  \\sum_{i=1}^n \\mbox{Var}[Y_i]\n\\] because the observations are independent. Then, using the result Equation 3.8 gives \\[\n\\mbox{Var}[\\bar Y]= \\frac{1}{n} \\ b''(\\theta_0) \\phi.\n\\tag{4.7}\\]\nWe can use Taylor’s theorem to expand \\(b'(\\hat\\theta)\\) about \\(\\theta_0\\): \\[\n\\bar y = b'(\\hat\\theta) \\approx  b'(\\theta_0) + (\\hat\\theta - \\theta_0) b''(\\theta_0),\n\\] which implies that \\[\n(\\hat\\theta - \\theta_0) \\approx  b''(\\theta_0)^{-1}\\{b'(\\hat\\theta) - b'(\\theta_0)\\}  \n= b''(\\theta_0)^{-1}(\\bar Y - \\mu_0),\n\\tag{4.8}\\] using Equation 4.3 and Equation 4.5. We can use Equation 4.8 to get approximations to the mean and variance of \\(\\hat\\theta\\): \\[\n\\mbox{E}[\\hat\\theta - \\theta_0]  \\approx   b''(\\theta_0)^{-1} \\mbox{E}[\\bar Y - \\mu_0]\n= 0,\n\\] using Equation 4.6, so \\[\n\\mbox{E}[\\hat\\theta] \\approx \\theta_0,\n\\tag{4.9}\\] and \\[\n\\mbox{Var}(\\hat\\theta) \\approx \\mbox{E}\\left[(\\hat\\theta - \\theta_0)^2\\right]\n\\] using Equation 4.9, \\[\n\\mbox{Var}(\\hat\\theta) \\approx \\mbox{E}\\left[\\left(b''(\\theta_0)^{-1}(\\bar Y - \\mu_0)\\right)^2\\right]\n\\] using Equation 4.8, \\[\n\\mbox{Var}(\\hat\\theta)\\approx  \\left(b''(\\theta_0)\\right)^{-2} \\mbox{Var}[\\bar Y]  \n\\] using Equation 4.6, \\[\n\\mbox{Var}(\\hat\\theta) = \\frac{\\phi}{n \\ b''(\\theta_0)}\n\\tag{4.10}\\] using Equation 4.7.\nThus we see that the first two derivatives of \\(b(\\theta)\\) play a key role in inference.\n\nExample: Accuracy for the Poisson distribution\nFor the Poisson distribution, \\(\\text{Po}(\\lambda)\\), we have found that \\(\\hat\\theta = \\log(\\bar{Y})\\). Using Equation 4.9 we know that \\(\\hat\\theta\\) is, at least, approximately unbiased. Then, using that \\(\\phi=1\\) (Table 3.2), \\(b'(\\theta)=e^{\\theta}\\) (Table 3.6) hence \\(b''(\\theta)=e^{\\theta}\\), and using Equation 4.10, leads to the result \\[\n\\mbox{Var}(\\hat\\theta) = \\frac{\\phi}{n \\ b''(\\theta_0)}\n=\n\\frac{1}{n e^{\\theta_0}}.\n\\]"
  },
  {
    "objectID": "4_GLM-Fitting.html#focus-on-maximum-likelihood-quiz",
    "href": "4_GLM-Fitting.html#focus-on-maximum-likelihood-quiz",
    "title": "4  GLM Estimation",
    "section": "Focus on maximum likelihood quiz",
    "text": "Focus on maximum likelihood quiz\nTest your knowledge recall and application to reinforce basic ideas and prepare for similar concepts later in the Chapter\n\n\n\nWhich of the following statements best describes the purpose of maximum likelihood estimation? To calculate the mean of a distributionTo find good values of unknown model parametersTo summarise a data setTo estimate probabilities of unknown eventsTo find the true values of model parametersNone of the above\nWhich of the following mathematical ideas is NOT used in the above maximum likelihood estimation? Identically distributedTaking logsBy independenceThe parameter θ depends on iSubstituting for the probability functionNone of the above\nWhich of the following terms is used to refer to the fixed parameter Φ? IndependentCanonicalNuisanceNormally distributedAnnoyanceNone of the above\nWhich of the following is NOT a good property of a parameter estimator? Has expectation equal to the true valueVariance decreases as sample size increasesUnbiasedHas fixed variance for all sample sizesSmall varianceNone of the above\nWhich of the following does NOT influence the variance of the estimator of θ? The sample sizec(y,Φ)b(θ)ΦThe true parameter valueNone of the above"
  },
  {
    "objectID": "4_GLM-Fitting.html#sec-mlegeneral",
    "href": "4_GLM-Fitting.html#sec-mlegeneral",
    "title": "4  GLM Estimation",
    "section": "4.2 The general case",
    "text": "4.2 The general case\n\n4.2.1 MLE Estimation\nSuppose that now the \\(n\\) independent observations \\(\\mathbf{y} = \\{y_i,\\ i=1,\\dots,n\\}\\) are not identically distributed. They are, however, sampled from the same exponential family density but with differing parameters, that is \\[\nf(y_i; \\theta_i,\\phi) = \\exp \\left\\{ \\frac{y_i\\theta_i  - b(\\theta_i)}{\\phi} + c(y_i, \\phi) \\right\\},\n\\] for \\(i=1,\\dots,n.\\) In this case, the canonical parameter does depend on \\(i\\) – but we assume that the scale parameter \\(\\phi\\) does not – and let \\(\\boldsymbol{\\theta} = \\{\\theta_i,\\ i=1,\\dots,n\\}\\).\nIn most applications, we are not interested in estimation of \\(\\boldsymbol{\\theta}\\) but instead we are interested in the linear predictor parameters \\(\\boldsymbol{\\beta} =\\{\\beta_1,\\dots,\\beta_p\\}\\). Note, however, that each \\(\\theta_i\\) will depend on all \\(\\beta_1,\\dots,\\beta_p\\). This is most obvious for the canonical parameter case where a convenient choice of link function is obtained using \\(\\boldsymbol{\\theta} = \\eta = X\\boldsymbol{\\beta}\\), hence \\(\\theta_i= \\mathbf{x}_i^T \\boldsymbol{\\beta} =\\sum_{j=1}^p x_{ij}\\beta_{j}\\) and each \\(\\theta_i\\) clearly depends on all \\(\\beta_1,\\dots,\\beta_p\\).\nThe principle of maximum likelihood will be used to estimate the model parameters \\(\\boldsymbol{\\beta}\\). Using the independence of \\(\\mathbf{y} = \\{y_i,\\ i=1,\\dots,n\\}\\), given the parameters \\(\\boldsymbol{\\beta}\\), the likelihood function is: \\[\nL(\\boldsymbol{\\beta}; \\mathbf{y}, \\phi)\n= f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi)\n= \\prod_{i=1}^n f(y_i; \\mathbf{x}_i, \\boldsymbol{\\beta}, \\phi)\n\\tag{4.11}\\] and the log-likelihood by \\[\nl (\\boldsymbol{\\beta}; \\mathbf{y}, \\phi)\n= \\log L(\\boldsymbol{\\beta}; \\mathbf{y}, \\phi)\n= \\sum_{i=1}^n \\log f(y_i;\n\\mathbf{x}_i,\n\\boldsymbol{\\beta}, \\phi).\n\\tag{4.12}\\] Then we wish to find the value of \\(\\boldsymbol{\\beta}\\) which maximizes the log-likelihood function, \\[\n\\hat{\\boldsymbol{\\beta}} =\n\\max_{\\boldsymbol{\\beta}}\nl (\\boldsymbol{\\beta}; \\mathbf{y}, \\phi).\n\\] For generalised linear models there is usually no closed-form expression for the MLE \\(\\hat{\\boldsymbol{\\beta}}.\\) Instead, an iterative approach based on Newton’s Method is often used.\nFor the normal linear regression model, however, that is where the exponential family is Gaussian and the link function \\(g(\\mu)\\) is the identity function, we have the familiar closed-form expression \\[\n\\hat{\\boldsymbol{\\beta}} = \\left(X^T X\\right)^{-1} X^T \\mathbf{y}.\n\\tag{4.13}\\]\n\n\n4.2.2 The score function and Fisher information\nWe define the score function: \\[\nU(\\boldsymbol{\\beta}) = \\frac{\\partial l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)} {\\partial \\boldsymbol{\\beta}},\n\\tag{4.14}\\] which is a \\(p \\times 1\\) vector. We define the observed Fisher information: \\[\n{\\cal I}(\\boldsymbol{\\beta}) = - \\frac{\\partial U(\\boldsymbol{\\beta})} {\\partial \\boldsymbol{\\beta}^T}\n          = -\\frac{\\partial^2 l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)} {\\partial \\boldsymbol{\\beta} \\, \\partial \\boldsymbol{\\beta}^T},\n\\tag{4.15}\\] which is a \\(p \\times p\\) matrix whose \\((j,k)\\)th element is: \\(-\\frac{\\partial^2 l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)} {\\partial \\beta_j\\partial \\beta_k}\\). We also define the expected Fisher information: \\[\n{\\cal J}(\\boldsymbol{\\beta}) = \\mbox{E}\\left[ -\\frac{\\partial^2 l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)} {\\partial \\boldsymbol{\\beta} \\, \\partial \\boldsymbol{\\beta}^T} \\right],\n\\tag{4.16}\\] which is also a \\(p \\times p\\) matrix.\nNewton’s Method then involves choosing an initial value for the parameter vector, \\(\\hat{\\boldsymbol{\\beta}}_0\\) say, and then repeatedly updating, until convergence, the value using \\(\\hat{\\boldsymbol{\\beta}}_t = \\hat{\\boldsymbol{\\beta}}_{t-1} - {\\cal I}^{-1}(\\hat{\\boldsymbol{\\beta}}_{t-1})\\; U(\\hat{\\boldsymbol{\\beta}}_{t-1})\\). A related method, called Fisher Scoring, replaces \\({\\cal I}\\) in the iterative step by \\({\\cal J}\\) – calculating the expected Fisher information is more work but does lead to better numerical properties.\n\nProposition 4.1 With definitions Equation 4.14, Equation 4.15, Equation 4.16 above,\n\n\\(\\mbox{E}\\left[U(\\boldsymbol{\\beta})\\right] = 0\\),\n\\({\\cal J}(\\boldsymbol{\\beta}) = \\mbox{E}\\left[U(\\boldsymbol{\\beta}) U^T(\\boldsymbol{\\beta})\\right]\\).\n\n\nProof We give the proof for continuous random variables. For the discrete case, replace integration by sums – see Exercises.\nTo start the proof, notice that we can re-write the joint density of the data given the parameters as \\[\nf(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) = L(\\boldsymbol{\\beta}; \\mathbf{y}, \\phi) =\\exp\\left\\{ l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)\\right\\}\n\\] and then \\[\n1 = \\int f(\\mathbf{y}; X \\boldsymbol{\\beta},\\phi) d\\mathbf{y}\n  = \\int \\exp\\{ l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)\\} d\\mathbf{y},\n\\] where \\(d\\mathbf{y}=dy_1\\cdots dy_n\\). Differentiating this with respect to \\(\\boldsymbol{\\beta}=(\\beta_1,\\dots,\\beta_p)^T\\) gives: \\[\\begin{align*}\n0 & = \\int \\frac{\\partial l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)} {\\partial \\boldsymbol{\\beta}}\n      \\exp\\{ l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)\\} d\\mathbf{y}  \\\\\n& = \\int U(\\boldsymbol{\\beta}) f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) d\\mathbf{y}  \\tag{$\\star$} \\\\\n&= \\mbox{E}\\left[U(\\boldsymbol{\\beta})\\right].\n\\end{align*}\\] Proving the first part.\nNext, differentiating \\((\\star)\\) by parts with respect to  \\(\\boldsymbol{\\beta}^T\\), \\[\\begin{align*}\n0 & = \\int \\frac{\\partial U(\\boldsymbol{\\beta})} {\\partial \\boldsymbol{\\beta}^T}\n      f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi)  + \\frac{\\partial l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)}\n      {\\partial \\boldsymbol{\\beta}} \\frac{\\partial l(\\boldsymbol{\\beta}; \\mathbf{y},\\phi)}\n      {\\partial \\boldsymbol{\\beta}^T} f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) d\\mathbf{y} \\\\[2mm]\n& = \\int - {\\cal I}(\\boldsymbol{\\beta}) f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) d\\mathbf{y}\n    + \\int U(\\boldsymbol{\\beta}) U^T(\\boldsymbol{\\beta}) f(\\mathbf{y}; X, \\boldsymbol{\\beta},\\phi) d\\mathbf{y} \\\\[2mm]\n& = - {\\cal J}(\\boldsymbol{\\beta}) + \\mbox{E}\\left[U(\\boldsymbol{\\beta}) U^T(\\boldsymbol{\\beta})\\right].\n\\end{align*}\\] proving the second part.\n\nProposition 4.2 Under some regularity conditions, the MLE \\(\\hat{\\boldsymbol{\\beta}}\\) of \\(\\boldsymbol{\\beta}\\) has the following asymptotic properties:\n\n\\(\\mbox{E}(\\hat{\\boldsymbol{\\beta}}) = \\boldsymbol{\\beta}\\); i.e. \\(\\hat{\\boldsymbol{\\beta}}\\) is unbiased for \\(\\boldsymbol{\\beta}\\).\n\\(\\mbox{Var}(\\hat{\\boldsymbol{\\beta}}) = {\\cal J}^{-1}(\\boldsymbol{\\beta})\\).\n\\(\\hat{\\boldsymbol{\\beta}}\\) follows a \\(p\\)-dimensional Normal distribution.\n\nCombining these three statements we have that asymptotically \\[\n\\hat{\\boldsymbol{\\beta}} \\sim N_p (\\boldsymbol{\\beta}, {\\cal J}^{-1}(\\boldsymbol{\\beta})).\n\\tag{4.17}\\]\n\nProof: Omitted. Similar to the proof using Taylor’s theorem in Section 4.1.\nFrom Equation 4.17, variances \\(\\text{Var}(\\hat{\\beta}_{k})\\), standard errors \\(\\text{se}(\\hat{\\beta}_{k})\\) and correlations between parameter estimates \\(\\text{Corr}(\\hat{\\beta}_{k},\\hat{\\beta}_{h})\\) can be estimated.\n\n\n4.2.3 The saturated case\nAgain we assume the observations \\(y_i,\\ i=1,\\dots,n\\) are independent but now we assume that \\(y_i\\) is sampled from an exponential family probability function with canonical parameter \\(\\theta_i\\), \\[\nf(y_i; \\theta_i,\\phi) = \\exp \\left\\{ \\frac{y_i\\theta_i  - b(\\theta_i)}{\\phi} + c(y_i, \\phi) \\right\\},\n\\] for \\(i=1,\\dots,n.\\) We can form the log-likelihood in the usual way to give \\[\nl (\\boldsymbol{\\theta}; \\mathbf{y}, \\phi) = \\sum_{i=1}^n \\log f(y_i; \\theta_i, \\phi)\n= \\sum_{i=1}^n \\left[\\frac{y_i\\theta_i  - b(\\theta_i)}{\\phi} + c(y_i, \\phi)\\right]\n\\] and so we find the the MLE of \\(\\boldsymbol{\\theta}\\) using \\[\n\\hat \\theta_i = (b')^{-1}(y_i),\n\\quad i=1,\\dots,n.\n\\] Note that each parameter is only a function of the corresponding observation.\nFurther, from Proposition 3.1, \\(\\mbox{E}[Y_i] = \\mu_i =b'(\\theta_i)\\) and if we again let \\(\\hat\\mu_i\\) denote the MLE of \\(\\mu_i\\), then \\(\\hat\\mu_i = b'(\\hat{\\theta_i})\\), hence we have \\(\\hat\\mu_i = y_i\\). Thus we see that, under the saturated model, the mean of the distribution of \\(y_i\\) is estimated to be equal to \\(y_i\\) itself. That is, the data are fitted exactly by the model. Of course this model is quite useless for explanation or prediction, since it misinterprets random variation as systematic variation. Nevertheless, the saturated model is useful as a benchmark for comparing models, as we will see later.\nIt is worth noting that the same situation can occur even when modelling in terms of the regression parameters \\(\\beta_j,\\ j=1,\\dots,p\\). When \\(p\\geq n\\), and if the covariates are linearly independent, each \\(\\theta_i\\) can take on any value independently of the others and so estimating the \\(\\beta_j\\)’s is equivalent to estimating the \\(\\theta_i\\)’s. That is we are also considering the saturated or full model. This highlights the danger of putting too many covariates into the model. There is a big literature on how to deal with more parameters then data using techniques of regularised regression."
  },
  {
    "objectID": "4_GLM-Fitting.html#focus-on-general-maximum-likelihood-quiz",
    "href": "4_GLM-Fitting.html#focus-on-general-maximum-likelihood-quiz",
    "title": "4  GLM Estimation",
    "section": "Focus on general maximum likelihood quiz",
    "text": "Focus on general maximum likelihood quiz\nTest your knowledge recall and application to reinforce basic ideas and prepare for similar concepts later in the Chapter\n\n\n\nWhich of the following statements best describes the purpose of maximum likelihood estimation? To find the true values of model parametersTo find good values of unknown model parametersTo calculate the mean of a distributionTo summarise a data setTo estimate probabilities of unknown eventsNone of the above\nWhich of the following mathematical ideas is NOT used in the general case of maximum likelihood estimation? Usually no closed-form expressionThe parameter θ does not depends on iUsing the independenceNot identically distributedThe log-likelihoodNone of the above\nWhich of the following terms is NOT used in maximum likelihood theory? Score functionThe log-likelihood functionFisher informationThe moment generating functionThe likelihood functionNone of the above\nWhich of the following is a true statement about the saturated case? Has fewer parameters than data observationsThe values of θ will be exactly the same as the values of βIt should never be consideredThe model fits the data exactlyThe parameters θ and unrelated to the values of βNone of the above\nWhich of the following is NOT a true statement about the asymptotic properties of the maximum likelihood estimator of β? Will always be equal to the true value.Follows a normal distributionIs unbiasedThe variance can be found form the expected Fisher informationThe asymptotic properties are when n tend to infinityNone of the above"
  },
  {
    "objectID": "4_GLM-Fitting.html#sec-deviance",
    "href": "4_GLM-Fitting.html#sec-deviance",
    "title": "4  GLM Estimation",
    "section": "4.3 Model deviance",
    "text": "4.3 Model deviance\nThe deviance is a quantity we use to assess the fit of a model to the data. Let \\(M\\) be a model of interest with fitted parameters \\(\\hat{\\boldsymbol\\theta}\\) and corresponding fitted values \\(\\hat{\\boldsymbol\\mu}\\). Also consider the saturated model with fitted parameters \\(\\tilde{\\boldsymbol\\theta}\\) and fitted values \\(\\tilde{\\boldsymbol\\mu}\\).\nThe deviance of model \\(M\\) is defined as twice the difference between the log-likelihood of the saturated model, \\(l(\\tilde{\\boldsymbol\\theta};\\mathbf{y},\\phi)\\), and the log-likelihood of model \\(M\\), \\(l(\\hat{\\boldsymbol\\theta}; \\mathbf{y},\\phi)\\), multiplied by \\(\\phi\\), \\[\\begin{eqnarray}\nD\n& = & 2 \\phi \\left\\{ l(\\tilde{\\boldsymbol\\theta};\\mathbf{y},\\phi) - l(\\hat{\\boldsymbol\\theta}; \\mathbf{y},\\phi) \\right\\} \\label{eq:deviance}\\\\[4mm]\n& = &  \\left\\{ \\begin{array}{ll}\n    \\sum_{i=1}^n  (y_i - \\hat\\mu_i)^2\n   = \\mbox{Residual sum of squares} &  \\mbox{ Normal} \\\\[3mm]\n   2 \\sum_{i=1}^n \\left\\{ y_i \\log \\left( \\frac{y_i}{\\hat\\mu_i} \\right)\n   + (m_i - y_i) \\log \\left( \\frac{m_i - y_i}{m_i - \\hat\\mu_i} \\right) \\right\\}\n   &  \\mbox{ Binomial} \\\\[3mm]\n   2 \\sum_{i=1}^n \\left\\{ y_i \\log \\left( \\frac{y_i}{\\hat\\mu_i} \\right)\n   - y_i + \\hat\\mu_i \\right\\} &  \\mbox{ Poisson} \\\\ \\end{array} \\right. \\notag\n\\end{eqnarray}\\]\nNote that Dobson, and others, call \\(D^*= D/\\phi=2 \\{ l(\\tilde{\\theta};y,\\phi) - l(\\hat{\\theta}; y,\\phi)\\}\\) the scaled deviance.\nWe now consider two situations:\nScale parameter \\(\\phi\\) known. For some data-types (e.g. Poisson, Binomial), we know \\(\\phi=1\\). Consider two nested models \\(M_1\\) and \\(M_2\\) with \\(r_1\\) and \\(r_2\\) parameters respectively where the parameters in \\(M_1\\) are a subset of those in \\(M_2\\) and hence \\(r_1 &lt; r_2\\). Further, let \\(D_1\\) and \\(D_2\\) be the deviances of model \\(M_1\\) and \\(M_2\\) respectively.\nThen, asymptotically,\n\nthe log likelihood-ratio statistic \\(D_1 - D_2 \\sim \\chi^2_{r_2 - r_1}\\) can be used to test the importance of the extra parameters in \\(M_2\\) not included in \\(M_1;\\)\na goodness-of-fit test for \\(M_2\\) can be done based on \\(D_2 \\sim \\chi^2_{n - r_2}\\).\n\nThe quality of the approximations involved depends on there being a large amount of information, for example, large counts for Binomial and Poisson data, or a large sample size for Normal data.\nScale parameter \\(\\phi\\) unknown. For some data-types (e.g. Normal, Gamma), \\(\\phi\\) is not known (typically \\(\\phi = \\sigma^2\\)). We must find a model \\(M_3\\) big enough to be believed, then estimate \\(\\phi\\) by the residual mean square: \\[\n    \\hat{\\phi} = \\frac{D_3}{n - r_3}.\n     \\tag{4.18}\\] Then test \\(M_1\\) against \\(M_2\\) using \\[\n    \\mbox{F}=\\frac{(D_1 - D_2) / (r_2 - r_1)}{\\hat{\\phi}} = \\frac{(D_1 - D_2) / (r_2 - r_1)}{D_3/(n-r_3)}\n     \\tag{4.19}\\] with \\[\n    \\mbox{F} \\sim F_{r_2 - r_1, n - r_3}.\n     \\tag{4.20}\\] So if the observed value of the statistic Equation 4.19 was within the upper (say 5%) tail of the \\(F\\)-distribution Equation 4.20, we would infer that Model \\(M_2\\) is better than Model \\(M_1\\)."
  },
  {
    "objectID": "4_GLM-Fitting.html#model-residuals",
    "href": "4_GLM-Fitting.html#model-residuals",
    "title": "4  GLM Estimation",
    "section": "4.4 Model residuals",
    "text": "4.4 Model residuals\nConsider a generalised linear model with observed values \\(y_i, i=1,\\dots,n\\) and fitted values \\(\\hat\\mu_i\\). Then the raw or response residuals are defined by \\[\ne_i^\\text{raw} = y_i - \\hat\\mu_i.\n\\]\nMore useful are the standardised or Pearson residuals defined by \\[\ne_i^\\text{std} =\ne_i^\\text{P} = \\frac{y_i - \\hat\\mu_i}{\\sqrt{ b''(\\theta_i)}}.\n\\] Recall from Equation 3.8 that \\(\\text{Var}(Y_i) = \\phi \\, b''(\\theta_i)\\).\nDeviance residuals are defined so that the sum of squared deviance residuals equals the total deviance. Thus we set \\[\ne_i^\\text{dev} = \\text{sign}(y_i - \\hat\\mu_i) \\sqrt{d_i},\n\\] where \\(d_i\\) is the contribution of observation \\(i\\) to the deviance, \\(D\\). For example, when \\(y_i\\) has a Poisson distribution with estimated mean \\(\\hat{\\mu}_i\\), we have \\[\ne_i^\\text{dev} = \\text{sign}(y_i - \\hat\\mu_i) \\sqrt{2\\left[y_i \\log\\left(\\frac{y_i}{\\hat{\\mu}_i}\\right) - y_i + \\hat{\\mu}_i\\right]}.\n\\]\nResiduals are useful for assessing the overall fit of a model to the data, and for identifying where the model might need to be improved."
  },
  {
    "objectID": "4_GLM-Fitting.html#fitting-generalised-linear-models-in-r",
    "href": "4_GLM-Fitting.html#fitting-generalised-linear-models-in-r",
    "title": "4  GLM Estimation",
    "section": "4.5 Fitting generalised linear models in R",
    "text": "4.5 Fitting generalised linear models in R\n\n4.5.1 GLM-related R commands\nThe function used to fit a generalised linear model in \\(\\mathbf R\\) is \\[\\texttt{glm(formula, family)}.\\]\nLet \\(\\texttt{x,y,z,a,b,c,}\\) \\(\\dots\\) be a set of vectors all of the same length \\(n\\) (perhaps read in from a data file using the \\(\\texttt{read.table}\\) and \\(\\texttt{attach}\\) commands). If \\(\\texttt{a,b,c}\\) are qualitative variables, then they first need to be declared as factors by \\(\\texttt{a = as.factor(a)}\\), etc.\nThe \\(\\texttt{formula}\\) argument of \\(\\texttt{glm}\\) specifies the required model in compact notation, e.g. \\(\\texttt{y} \\sim \\texttt{x*a}\\) or \\(\\texttt{y} \\sim \\texttt{x + z*a}\\) where \\(\\sim, \\texttt{+,*}\\) have the same meaning as in Section 2.5.\nThe \\(\\texttt{family}\\) argument specifies which exponential family is to be used. We shall use \\(\\texttt{gaussian, poisson}\\) and \\(\\texttt{binomial}\\); \\(\\texttt{gaussian}\\) is the default. Other options are available; see \\(\\texttt{help(family)}\\) for further information.\nAlong with the family, a link function can be specified. The possible choices are:\n\n\\(\\texttt{gaussian} - \\texttt{\"identity\"}\\) (default)\n\\(\\texttt{poisson} - \\texttt{\"log\"}\\) (default), \\(\\texttt{\"sqrt\"}\\), \\(\\texttt{\"identity\"}\\)\n\\(\\texttt{binomial} - \\texttt{\"logit\"}\\) (default), \\(\\texttt{\"probit\"}\\), \\(\\texttt{\"cloglog\"}\\).\n\nR assumes the default options unless we state otherwise. For example,\n\\(\\texttt{glm(y}\\sim\\texttt{a+b)}\\) # Gaussian errors, identity link\n\\(\\texttt{glm(y}\\sim a+b\\texttt{, poisson)}\\) # Poisson errors, log link\n\\(\\texttt{glm(y}\\sim\\texttt{a+b, poisson(\"sqrt\"))}\\) # Poisson errors, sqrt link\nNote that for the binomial case, the response variable should be an \\(n \\times 2\\) matrix \\(\\texttt{ym}\\), say, not a vector, where the first column contains the numbers of successes and the second column the numbers of failures, for example:\n\\(\\texttt{glm(ym}\\sim\\texttt{ a+b, binomial)}\\) # binomial errors, logit link\nTo extract information about a fitted generalised linear model, it is best to store the result of \\(\\texttt{glm}\\) as a variable and then to use the following functions:\n\nTo fit a GLM and store the result in \\(\\texttt{y.glm}\\) (for example):\n\\(\\texttt{y.glm = glm(y}\\sim\\texttt{a*b, poisson(\"sqrt\"))}\\)\nTo print various pieces of information including deviance residuals, parameter estimates and standard errors, deviances, and (if specified) correlations of parameter estimates:\n\\(\\texttt{summary(y.glm, correlation=T)}\\)\nTo print the anova table of the fitted model:\n\\(\\texttt{anova(y.glm)}\\)\nTo print the deviance of the fitted model:\n\\(\\texttt{deviance(y.glm)}\\)\nTo print the residual degrees of freedom of the fitted model:\n\\(\\texttt{df.residual(y.glm)}\\)\nTo print the vector of fitted values under the fitted model:\n\\(\\texttt{fitted.values(y.glm)}\\)\nTo print the residuals from the fitted model:\n\\(\\texttt{residuals(y.glm, type)}\\)\nNote: \\(\\texttt{type}\\) should be \\(\\texttt{\"deviance\"}\\) (default), \\(\\texttt{\"pearson\"}\\), or \\(\\texttt{\"response\"}\\)\nTo print the parameter estimates from the fitted model:\n\\(\\texttt{coefficients(y.glm)}\\)\nTo print the design matrix for a specified model formula: \\(\\texttt{model.matrix(y}\\sim\\texttt{a*b)}\\)\n\nThe functions \\(\\texttt{summary}\\) and \\(\\texttt{anova}\\) are the most useful for printing out information about the fitted model. The results of the other functions can be saved as variables for further computation, if desired.\n\n\n4.5.2 Example of fitting Poisson GLM in R\nHere is a toy example of R commands for modelling a response in terms of two qualitative explanatory variables (that is factors). The model assumes the data are Poisson-distributed and uses the logarithmic link function.\n\n\nCode\ny = c(1, 2, 4, 7, 8, 10, 10, 7, 10, 2, 8, 16)\na = rep(1:4,times=3)\nb = rep(1:3,each=4)\na = as.factor(a)\nb = as.factor(b)\n\ny.glm = glm(y ~ a+b, poisson)\n\nsummary(y.glm, correlation=T)\n\n\n\nCall:\nglm(formula = y ~ a + b, family = poisson)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.1408     0.3351   3.404 0.000663 ***\na2           -0.3054     0.3522  -0.867 0.385934    \na3            0.1466     0.3132   0.468 0.639712    \na4            0.4568     0.2932   1.558 0.119269    \nb2            0.9163     0.3162   2.898 0.003761 ** \nb3            0.9445     0.3150   2.999 0.002712 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 31.725  on 11  degrees of freedom\nResidual deviance: 13.150  on  6  degrees of freedom\nAIC: 68.227\n\nNumber of Fisher Scoring iterations: 5\n\nCorrelation of Coefficients:\n   (Intercept) a2    a3    a4    b2   \na2 -0.45                              \na3 -0.50        0.48                  \na4 -0.54        0.51  0.57            \nb2 -0.67        0.00  0.00  0.00      \nb3 -0.68        0.00  0.00  0.00  0.72\n\n\nCode\nanova(y.glm)\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: y\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev\nNULL                    11     31.725\na     3   6.2793         8     25.445\nb     2  12.2947         6     13.150\n\n\nCode\ndeviance(y.glm)\n\n\n[1] 13.15047\n\n\nCode\ndf.residual(y.glm)\n\n\n[1] 6\n\n\nCode\nfitted.values(y.glm)\n\n\n        1         2         3         4         5         6         7         8 \n 3.129412  2.305882  3.623529  4.941176  7.823529  5.764706  9.058824 12.352941 \n        9        10        11        12 \n 8.047059  5.929412  9.317647 12.705882"
  },
  {
    "objectID": "4_GLM-Fitting.html#focus-on-glm-fitting-in-r-quiz",
    "href": "4_GLM-Fitting.html#focus-on-glm-fitting-in-r-quiz",
    "title": "4  GLM Estimation",
    "section": "Focus on GLM fitting in R quiz",
    "text": "Focus on GLM fitting in R quiz\nTest your knowledge recall and application to reinforce basic ideas and prepare for similar concepts later in the Chapter\n\n\n\nWhich of the following is the main R command used to fit generalised linear models? familyglmlmfitglmformulaNone of the above\nWhich of the following is NOT an option for the model family? gaussianbinomiallogitpoissonGammaNone of the above\nWhich of the following is NOT a link option in a GLM? logidentityprobitconstantlogitNone of the above\nWhich of the following is NOT a link option with the binomial family? cauchitloginverselogitcloglogNone of the above\nWhich of the following is NOT a n R command used to check model fit? anovaresidualssummarydevianceglmNone of the above"
  },
  {
    "objectID": "4_GLM-Fitting.html#exercises",
    "href": "4_GLM-Fitting.html#exercises",
    "title": "4  GLM Estimation",
    "section": "4.6 Exercises",
    "text": "4.6 Exercises\n\n4.1 Use Equation 4.4 to obtain estimation equations for the natural parameter \\(\\theta\\), based on a sample \\(\\mathbf{y}=\\{y_1,\\dots, y_n\\}\\), for each of the following situations:\n\nthe binomial, \\(Y\\sim\\mbox{Bin}(m,p)\\),\nthe geometric, \\(Y\\sim\\mbox{Ge}(p)\\),\nthe exponential, \\(Y\\sim\\mbox{Exp}(\\lambda)\\).\n\nAssuming that \\(n\\) is very large, use Equation 4.9 to comment on possible bias of the estimator for large samples. What is the corresponding variance of the estimator?\n\n\nClick here to see hints.\n\nFor each situation equate the theoretical expectation, in terms of the derivative of b, to the sample mean – that is start from Equation 4.3 and solve. For bias and variance you should consider the asymptotic results.\n\n4.2 For a sample of size \\(n\\) from the normal distribution, \\(Y\\sim N(\\mu, \\sigma^2),\\) how do the results produced using Equation 4.9 and Equation 4.10 compare with the familiar results \\(\\hat\\mu =\\bar Y\\), \\(\\text{E}[\\hat\\mu]=\\mu\\), and \\(\\text{Var}[\\hat\\mu] =\\sigma^2/n?\\)\n\n\nClick here to see hints.\n\nIdentify, \\(\\theta\\), \\(\\phi\\) and \\(b''(\\theta)\\) then substitute into the equations.\n\n4.3 For the normal linear regression model, \\(\\mathbf{Y}=X\\boldsymbol{\\beta}+\\boldsymbol{\\epsilon}\\) where \\(\\boldsymbol{\\epsilon}\\sim N_n(0,\\sigma^2 I_n)\\) use the principle of maximum likelihood to show that the MLE has the closed form given in Equation 4.13.\n\n\nClick here to see hints.\n\nApply the rules for matrix differentiation of a quadratic form. If needed, refer to Appendix C in the Basic Pre-requisite Material folder in Minerva.\n\n4.4 Check that you can derive the formulas given for deviance of normal, binomial, and Poisson models at the start of Section 4.3.\n\n\nClick here to see hints.\n\nForm the likelihoods of the saturated model, recalling that the fitted values are exactly equal to the observed \\(y\\) values, and substitute them into the general deviance equation. Then, simplify.\n\n4.5 How do the deviance tests defined in Section 4.3 related to the tests used to find the best fit model for the birthweight example in Section 2.1?\n\n\nClick here to see hints.\n\nStart by comparing the notation. Then, replace the R and r in the F-statistic with D and adjust the notation for degrees of freedom. You should then get very expressions. The only difference being from which model \\(\\sigma^2\\) is estimated.\n\n4.6 Consider a clinical trial into the effectiveness of Amoxicillin (a widely used antibiotic) against bacterial chest infections in children. A sample of 200 children with chest infections were randomly assigned to one of five groups with Amoxicillin dose levels of 20-100 mg/kg per day. Four weeks later it was recorded whether the child had required a re-treatment with a further dose of antibiotic, or not, with results summarised in Table 4.1\n\n\nTable 4.1: Effectiveness of Amoxicillin against bacterial chest infections in children\n\n\n\n20mg\n40mg\n60mg\n80mg\n100mg\n\n\n\n\nGroup size\n23\n18\n14\n23\n22\n\n\nRe-treatment\n20\n14\n5\n6\n3\n\n\n\n\nFirst fit a normal linear model and then fit a generalised linear model assuming binomial errors and a logistic link. Superimpose both of fitted model onto a scatter plot of the data. Which do you think is the better model? Justify you answer.\n\n\nClick here to see hints.\n\nThe linear model part would be straightforward, look at earlier examples if not. For the glm case, check the R code block for Figure 1.1 (b). Simply compare the fit by eye."
  },
  {
    "objectID": "4_GLM-Fitting.html#footnotes",
    "href": "4_GLM-Fitting.html#footnotes",
    "title": "4  GLM Estimation",
    "section": "",
    "text": "Using the result that the MLE of any function of a parameter is given by the same function applied to the MLE of the parameter.↩︎"
  },
  {
    "objectID": "5_logisticmodel.html#introduction",
    "href": "5_logisticmodel.html#introduction",
    "title": "5  Modelling Proportions",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nIn this chapter we will focus on applications of generalised linear modelling where the response variable follows a binomial distribution. This can arise when the outcome is binary, that is it can take one of only two possible values, or is the sum of a set of such binary outcomes. These two outcomes record whether some event of interest has occurred or not. In the simplest case, the response variable, \\(B\\) say, is defined as \\[\nB =\n\\begin{cases}\n1 & \\text{if the event has occurred} \\\\\n0 & \\text{if the event has not occurred}\n\\end{cases}\n\\tag{5.1}\\] and we set \\(Pr(B=1)=p\\), and hence \\(Pr(B=0)=1-p\\). This is, of course, the definition of a Bernoulli trial leading to a Bernoulli random variables, \\(B\\sim \\text{Bernoulli}(p)\\).\nSuppose now that there are \\(m\\) similar binary outcomes, \\(B_i, i=1,\\dots,m\\) with \\(Pr(B_i=1)=p\\), and that the total number of times that the event occurred is recorded as the response \\(Y\\). Assuming that \\(m\\) is known before the trials start, that \\(p\\) is fixed and that the individual Bernoulli trials are independent then \\(Y\\) follows a binomial distribution, \\(Y\\sim \\text{B}(m, p)\\) with probability mass function \\[\nf(y) = {m \\choose y} p^y(1-p)^{m-y}.\n\\tag{5.2}\\] Note that the binomial random variable can be thought of as the sum of i.i.d Bernoulli random variables, \\(Y= B_1+\\cdots + B_m\\), if that is helpful.\nThe Binomial distribution \\(\\text{B}(m, p)\\) is often described in terms of success and failure and a binomial distribution in terms of the number of successes in \\(m\\) independent trials, where \\(p\\) is the probability of success in each trial. The term success need not correspond to a favourable outcome; it is merely the language traditionally used in connection with this model. For example, success might correspond to death.\nOf course, the special case with \\(m=1\\) reduces to the Bernoulli distribution. Further, if \\(Y_1 \\sim \\text{B}(m_1,p)\\) and independently \\(Y_2 \\sim \\text{B}(m_2,p)\\), then \\(Y_1+Y_2\\) also follows a binomial distribution, \\(\\text{B}(m_1+m_2,p)\\) – note that is only valid when \\(p\\) is common.\nRecall that the binomial can be re-written in the exponential family form of Equation 3.3 with \\[\nf(y)  = \\exp \\left\\{ y\\ \\mbox{logit } p  + m \\log (1-p) + \\log {m \\choose y}\\right\\},\n\\] with natural or canonical parameter \\(\\theta=\\text{logit} \\ p = \\log \\left( p(1-p)\\right),\\) scale parameter \\(\\phi=1\\), \\(b(\\theta)=m\\log(1+e^\\theta)\\) and \\(c(y,\\phi)=\\log{m\\choose y}\\) as in Table 3.3."
  },
  {
    "objectID": "5_logisticmodel.html#sec-linearlogistic",
    "href": "5_logisticmodel.html#sec-linearlogistic",
    "title": "5  Modelling Proportions",
    "section": "5.2 The logistic model",
    "text": "5.2 The logistic model\nThroughout this module we have assumed that a response variable, \\(Y\\), depends on a set of explanatory variables, \\(\\mathbf{x} = \\{x_1,\\dots, x_p\\}\\). In particular, for binomial counts from \\(\\text{B}(m,p)\\), \\(0&lt;p&lt;1\\) with mean \\(\\mu=mp\\) we set the link function, \\(g(\\mu)\\), equal to the linear predictor \\(\\eta = \\sum \\beta_j x_j\\) and hence have \\[\ng(\\mu) = \\sum_{j=1}^p \\beta_j x_j = \\mathbf{x}^T \\boldsymbol{\\beta}\n\\] where \\(\\boldsymbol{\\beta} = \\{\\beta_1,\\dots,\\beta_p\\}\\) are the linear predictor parameters. Recall that the canonical logit link Equation 3.14 for the binomial leads to the systematic part of the model \\[\n\\text{logit}(p) = \\log \\left( \\frac{p}{1-p}\\right) = \\mathbf{x}^T \\boldsymbol{\\beta}\n\\tag{5.3}\\] but that other links function are possible, such as the probit, Equation 3.15, the complementary log-log, Equation 3.16, and the cauchit Equation 3.17.\nThis model can alternatively be written \\[\nY\\ \\sim \\text{B}(m,p), \\quad \\text{where }\np=\\frac{\\exp \\{\\mathbf{x}^T \\boldsymbol{\\beta}\\}}{1+\\exp\\{ \\mathbf{x}^T \\boldsymbol{\\beta}\\}}\n\\tag{5.4}\\] which makes the dependence of \\(Y\\) on \\(\\mathbf{x}\\), and \\(\\boldsymbol{\\beta}\\), more explicit.\nIn general, we might want to consider \\(n\\) independent binomial random variables representing subgroups of the sample with \\(Y_1\\sim \\text{B}(m_1,p_1), \\dots Y_n\\sim \\text{B}(m_n,p_n)\\), see Table 5.1 and hence \\[\n\\mathbf{Y}\\ \\sim \\text{B}(\\mathbf{m},\\mathbf{p}), \\quad \\text{and }\n\\mathbf{p}=\\frac{\\exp \\{X \\boldsymbol{\\beta}\\}}{1+\\exp\\{ X \\boldsymbol{\\beta}\\}},\n\\] with response \\(\\mathbf{Y}=\\{Y_1,\\dots, Y_n\\}\\), \\(\\mathbf{m}=\\{m_1,\\dots, m_n\\}\\), \\(\\mathbf{p}=\\{p_1,\\dots, p_n\\}\\), \\(X\\) being the \\(n \\times p\\) design matrix and \\(\\boldsymbol{\\beta} = \\{\\beta_1,\\dots,\\beta_p\\}\\) the linear predictor parameters.\n\n\nTable 5.1: Linear logistic model\n\n\n\nGroup 1\nGroup 2\n\\(\\dots\\)\nGroup \\(n\\)\n\n\n\n\nSuccesses\n\\(Y_1\\)\n\\(Y_2\\)\n\\(\\dots\\)\n\\(Y_n\\)\n\n\nFailures\n\\(m_1-Y_1\\)\n\\(m_2-Y_2\\)\n\\(\\dots\\)\n\\(m_n-Y_n\\)\n\n\nTotal\n\\(m_1\\)\n\\(m_2\\)\n\\(\\dots\\)\n\\(m_n\\)\n\n\n\n\nThen we can write down the log likelihood of \\(\\boldsymbol{\\beta}\\) using Equation 4.12 and Equation 5.2 as \\[\nl(\\boldsymbol{\\beta}; \\mathbf{y})\n=\n\\sum_{i=1}^n\n\\left\\{\ny_i \\, \\log (p_i) + (m_i-y_i) \\log (1-p_i) + \\log {m_i \\choose y_i}\n\\right\\}\n\\tag{5.5}\\] where \\[\np_i=\\frac{\\exp \\{\\mathbf{x}_i^T \\boldsymbol{\\beta}\\}}{1+\\exp\\{ \\mathbf{x}_i^T \\boldsymbol{\\beta}\\}}.\n\\] We would then use the principle of maximum likelihood to estimate \\(\\boldsymbol{\\beta}\\) \\[\n\\hat{\\boldsymbol{\\beta}} = \\text{arg} \\max_{\\boldsymbol{\\beta}} \\,\nl(\\boldsymbol{\\beta}; \\mathbf{y}).\n\\] Then, the fitted probability are given by \\[\n\\hat{p}_i=\\frac{\\exp \\{\\mathbf{x}_i^T \\hat{\\boldsymbol{\\beta}}\\}}{1+\\exp\\{ \\mathbf{x}_i^T \\hat{\\boldsymbol{\\beta}}\\}}\n\\] and corresponding fitted values \\[\n\\hat y_i = m_i \\, \\hat{p}_i.\n\\]\nThe Pearson residuals for binomial data take the form\n\\[\ne^P_i  =\n\\frac{y_i - m_i\\hat{p}_i}{\\sqrt{m_i \\hat{p}_i (1 - \\hat{p}_i)}} .\n\\] From the general result \\(\\text{Var}(Y_i)=\\phi b''(\\theta)\\) in Proposition 3.1, it can be shown that \\(\\text{Var}(Y_i)=m_i {p}_i (1 - {p}_i)\\). Then, using the estimate of \\(p_i\\) leads to the denominator above.\nFor large \\(m_i\\), the usual Normal approximation to the Binomial means that the Pearson residuals are approximately \\(N(0, 1)\\) distributed.\nIt can be shown that the deviance for the fitted model is \\[\nD =\n2 \\sum_{i=1}^n \\left\\{ y_i \\log \\left( \\frac{y_i} {m_i \\hat{p}_i} \\right) +\n(m_i - y_i) \\log \\left( \\frac{m_i - y_i}{m_i (1 - \\hat{p}_i)} \\right)\n\\right\\},\n\\tag{5.6}\\] which is approximately \\(\\chi^2_{n-r}\\) distributed if the model is correct, where \\(r\\) is the number of degrees of freedom in the model (that is the number of parameters or columns of the design matrix).\nThis formula can be shown to be equivalent to \\[\nD = 2 \\sum_{j=1}^2 \\sum_{i=1}^n O_{ji} \\log\n\\frac{O_{ji}}{E_{ji}}\n\\] where \\(O_{ji}\\) denotes the observed value and \\(E_{ji}\\) denotes the expected value in cell \\((j,i)\\) of the \\(2 \\times n\\) table of successes and failures:\n\nObserved and expected frequencies in the linear logistic model\n\n\n\nGroup 1\nGroup 2\n\\(\\dots\\)\nGroup \\(n\\)\n\n\n\n\nSuccesses, \\(j=1\\)\n\\(O_{11}\\)\n\\(O_{12}\\)\n\\(\\dots\\)\n\\(O_{1n}\\)\n\n\n\n\\(E_{11}\\)\n\\(E_{12}\\)\n\\(\\dots\\)\n\\(E_{1n}\\)\n\n\nFailures, \\(j=2\\)\n\\(O_{21}\\)\n\\(O_{22}\\)\n\\(\\dots\\)\n\\(O_{2n}\\)\n\n\n\n\\(E_{21}\\)\n\\(E_{22}\\)\n\\(\\dots\\)\n\\(E_{2n}\\)\n\n\n\nAnother goodness-of-fit statistic is the Pearson chi-squared statistic: \\[\nX^2 = \\sum_{j=1}^2 \\sum_{i=1}^n \\frac{(O_{ji} - E_{ji})^2}{E_{ji}}.\n\\] This is asymptotically equivalent to the deviance Equation 5.6 (proof is by Taylor series expansion; omitted). Thus, asymptotically, \\(X^2\\) is also approximately \\(\\chi^2_{n-r}\\) distributed. Both approximations can be poor if the expected frequencies are small, but \\(X^2\\) copes slightly better with this problem. See Dobson, p.136 for more details."
  },
  {
    "objectID": "5_logisticmodel.html#focus-on-model-choice-quiz",
    "href": "5_logisticmodel.html#focus-on-model-choice-quiz",
    "title": "5  Modelling Proportions",
    "section": "Focus on model choice quiz",
    "text": "Focus on model choice quiz\nTest knowledge recall and comprehension to reinforce ideas of linear and non-linear models. More questions to be added later.\n\n\n\nWhich of the following is NOT a true statements about modelling?\n\n The context of the problem and description of the data can be ignored – it's only the data that matter Goodness of fit can be assessed using the model residuals. Some situations might suggest more than one model to try. All model assumptions should be checked. It is important to plot the fitted model on the same graph as the data.\n\nWhich of the following is a true statements about linear regression?\n\n It is always possible to use the fitted model for extrapolation. It is always important to check model residuals. When there is a moderate correlation then the relationship between variables is always linear. A linear model will fit best when the correlation is close to zero. A linear model should always be the first model considered.\n\nWhich of the following is a true statements about logistic regression?\n\n Should only be used if a linear model is not a good fit to the data. The explanatory variables follow a binomial distribution A logisitc model should always be used if the correlation is close to zero. It is always possible to use the fitted model for extrapolation. The response variable follows a binomial distribution.\n\n\n\n\n\nClick here to see explanations\n\nMore detail will be added later.\n\nThe starting point should always to learn about the background to a data set as this often suggests models to consider, appropriate distributions and any potential problems. Hence saying that it is only the data that maters is inappropriate.\nOf course there are many important things to check, but looking at residuals is one of the most important.\nLogistic regression assumes that the data follow a binomial distribution."
  },
  {
    "objectID": "5_logisticmodel.html#sec-overdispersion",
    "href": "5_logisticmodel.html#sec-overdispersion",
    "title": "5  Modelling Proportions",
    "section": "5.3 Overdispersion",
    "text": "5.3 Overdispersion\nExamination of residuals and deviances may indicate that a model is not an adequate fit to the data. One possible reason is overdispersion. In particular, it is advisable to compare the residual deviance of the proposed model with the corresponding degrees of freedom. If the ratio, deviance divided by degrees of freedom, is substantially greater than 1, then there is evidence for overdispersion – substantially less than 1 indicates under dispersion but that is very rare. Overdispersion can occur for any error distribution where the variance is linked to the mean — e.g. Binomial, Poisson. In the binomial case, overdispersion is called extra-Binomial variation.\nRecall that if \\(Y_i \\sim \\text{Bin}(m_i, p_i)\\), \\(\\text{Var}(Y_i) = m_i p_i (1 - p_i)\\). Overdispersion occurs if observations which have been modelled by a \\(\\text{Bin}(m_i, \\hat{p}_i)\\) distribution have substantially greater variation than \\(m_i \\hat{p}_i (1 - \\hat{p}_i)\\). This will lead to a value of \\(D\\) substantially greater than the expected value of \\(n - r\\). This can occur if the model is missing appropriate explanatory variables or has the wrong link function, or if the \\(Y_i\\) are not independent.\nOne solution is to include an extra parameter \\(\\tau\\) in the model so that \\(\\text{Var}(Y_i) = \\tau \\times m_i p_i (1 - p_i)\\). For more details, see Section 7.7 of Dobson or Chapter 6 of Collett (1991) Modelling Binary data, Chapman & Hall.\nThe \\(\\texttt{glm}\\) function in R allows for extra-Binomial variation by setting \\(\\texttt{family=quasibinomial()}\\) with the usual link functions available."
  },
  {
    "objectID": "5_logisticmodel.html#odds-ratios-for-2x2-tables",
    "href": "5_logisticmodel.html#odds-ratios-for-2x2-tables",
    "title": "5  Modelling Proportions",
    "section": "5.4 Odds ratios for 2x2 tables",
    "text": "5.4 Odds ratios for 2x2 tables\nIn the special case of the data forming a \\(2 \\times 2\\) table, then a commonly calculated summary statistic is the odds ratio. This is a measure of association between the presence or absence of some property and an outcome. It represents the odds that an outcome will occur given presence, compared to the odds of the outcome occurring in the absence of that property.\nThe following \\(2 \\times 2\\) table summarizes the different possibilities.\n\n\n\n\nSuccess\nFailure\n\n\n\n\nPresent\n\\(y_1\\)\n\\(m_1-y_1\\)\n\n\nAbsent\n\\(y_2\\)\n\\(m_2-y_2\\)\n\n\n\nSimilarly, by dividing by the corresponding row total, the probabilities are summaries as follows.\n\n\n\n\nSuccess\nFailure\n\n\n\n\nPresent\n\\(\\pi_1\\)\n\\(1-\\pi_1\\)\n\n\nAbsent\n\\(\\pi_2\\)\n\\(1-\\pi_2\\)\n\n\n\nThere are conditional probabilities, for example \\(\\text{Pr}(\\text{Success} | \\text{Present})\\) etc., and form a stochastic matrix where sum of each row is now 1.\nFor each row, the odds of success is defined by \\[\nO_i = \\pi_i/(1-\\pi_i), \\quad i=1,2,\n\\] and a comparison between these two probabilities is given by the odds ratio \\[\n\\phi = \\frac{O_1}{O_2} = \\frac{\\pi_1(1-\\pi_2)}{\\pi_2(1-\\pi_1)}.\n\\] If \\(\\phi=1\\) then the property does not affect the odds of success, \\(\\phi&gt;1\\) indicates that the property is associated with higher odds of success, and \\(\\phi&lt;1\\) indicates the property is associated with lower odds of success.\nNote that the log-odds, \\(\\log O_i = \\log \\left(\\pi_i/(1-\\pi_i) \\right)\\), which is the \\(\\text{logit}(\\pi_i),\\) and hence is the natural parameter in the regular exponential family form of the binomial distribution.\nFinally, an approximate 100(1-\\(\\alpha\\))% confidence interval, \\(\\left(\\phi_{\\alpha/2},\\phi_{1-\\alpha/2}\\right)\\), for the odds ratio is calculated using the formulas: \\[\n\\phi_{\\alpha/2} = \\phi \\times \\exp\\left(-z_{1-\\alpha/2}\\sqrt{\\frac{1}{y_1}+\\frac{1}{(m_1-y_1)}+\\frac{1}{y_2}+\\frac{1}{(m_2-y_2)}}\\right)\n\\] and \\[\n\\phi_{1-\\alpha/2} = \\phi \\times \\exp\\left(+z_{1-\\alpha/2}\n\\sqrt{\\frac{1}{y_1}+\\frac{1}{(m_1-y_1)}+\\frac{1}{y_2}+\\frac{1}{(m_2-y_2)}}\\right)\n\\] where \\(z_{1-\\alpha/2}\\) is such that \\(\\text{Pr}(Z \\le z_{1-\\alpha/2})=1-\\alpha/2\\). In particular, for a 95% confidence interval \\(z_{1-\\alpha/2}=z_{0.975}=1.96\\).\nIf the confidence interval for the odds ratio includes the value 1 then the calculated odds ratio would not be considered statistically significant."
  },
  {
    "objectID": "5_logisticmodel.html#application-to-doseresponse-experiments",
    "href": "5_logisticmodel.html#application-to-doseresponse-experiments",
    "title": "5  Modelling Proportions",
    "section": "5.5 Application to dose–response experiments",
    "text": "5.5 Application to dose–response experiments\nA variable dose of some reagent is administered to each study subject, and the occurrence of a specific response is recorded. This is a dose–response experiment, one of the first uses of regression models for Bernoulli (or Binomial) responses.\nFor example, the Table 5.2 (including the information from Table 1.1) gives the number of beetles killed \\(y_i\\) and the number not killed \\((m_i-y_i)\\) out of a total number \\(m_i\\) that were exposed to a dose \\(x_i\\) of gaseous carbon disulphide, for \\(n=8\\) dose levels \\(i=1,\\dots,8\\) (Dobson: pp.109 in 1st edn; pp.119 in 2nd edn; pp.127 in 3rd edn). The proportion killed \\(p_i = y_i/m_i\\) at each dose level \\(i\\) is also shown in Table 5.2 and plotted in Figure 5.1.\n\n\nTable 5.2: Numbers of beetles killed by five hours of exposure to 8 different concentrations of gaseous carbon disulphide\n\n\n\n\n\n\n\n\n\nDose\n\\(x_i\\)\nNo. of beetle\n\\(m_i\\)\nNo. killed\n\\(y_i\\)\nNo. not killed\n\\(m_i-y_i\\)\nProportion\n\\(p_ i=y_i/m_i\\)\n\n\n\n\n1.6907\n59\n6\n53\n0.10\n\n\n1.7242\n60\n13\n47\n0.22\n\n\n1.7552\n62\n18\n44\n0.29\n\n\n1.7842\n56\n28\n28\n0.50\n\n\n1.8113\n63\n52\n11\n0.83\n\n\n1.8369\n59\n53\n6\n0.90\n\n\n1.8610\n62\n61\n1\n0.98\n\n\n1.8839\n60\n60\n0\n1.00\n\n\n\n\n\n\nCode\npar(mar=c(4, 4, 0, 1))\n\nbeetle = read.table(\"https://richardpmann.com/MATH3823/Datasets/beetle.txt\", header=T)\n\ndose = beetle$dose\nmortality = beetle$died/beetle$total\n\nplot(dose, mortality, pch=16,\n     xlim=c(1.65, 1.90), xlab =\"Dose\",\n     ylim=c(-0.1, 1.1),  ylab=\"Mortality\")\nabline(h=c(0,1), lty=2)\n\ny = cbind(beetle$died, beetle$total-beetle$died)\nglm.fit = glm(y ~ dose, family=binomial(link='logit'))\n\noutput.dose = seq(1.6,1.95,0.001)\nfitted = predict(glm.fit, data.frame(dose=output.dose), type=\"response\")\nlines(output.dose, fitted)\n\n\n\n\n\nFigure 5.1: Beetle mortality rates with fitted dose-response curves.\n\n\n\n\nNow Equation 5.4 motivates modelling the beetle data as \\[\nY_i \\sim \\text{B}(m_i,p_i), \\quad \\mbox{for } i=1,\\dots, n=8\n\\] where \\[\n\\eta_i = \\alpha + \\beta x_i\n\\] and so \\[\np_i=\\frac{\\exp\\{\\alpha+\\beta x_i\\}}{(1+\\exp\\{\\alpha+\\beta x_i\\})}.\n\\]\nTo fit this model in R, a matrix with columns containing the numbers killed \\(y_i\\) and the numbers not killed \\(m_i-y_i\\) is first calculated\n\n\nCode\ny = cbind(beetle$died, beetle$total-beetle$died)\n\nhead(y)\n\n\n     [,1] [,2]\n[1,]    6   53\n[2,]   13   47\n[3,]   18   44\n[4,]   28   28\n[5,]   52   11\n[6,]   53    6\n\n\nwhich is then used in the \\(\\texttt{glm}\\) command\n\n\nCode\nglm.fit = glm(y ~ dose, family=binomial)\n\n\nThe model parameter estimates are given by\n\n\nCode\ncoefficients(glm.fit)\n\n\n(Intercept)        dose \n  -60.71745    34.27033 \n\n\nand hence\n\\[\n\\hat p_i = \\frac{\\exp\\{-60.7 + 34.3 x_i\\}}{(1+\\exp\\{ -60.7 + 34.3x_i\\})}.\n\\tag{5.7}\\]\nFurther, the deviance if given by\n\n\nCode\ndeviance(glm.fit)\n\n\n[1] 11.23223\n\n\nor more helpfully a full summary, which contains parameter estimates and deviance values using\n\n\nCode\nsummary(glm.fit)\n\n\n\nCall:\nglm(formula = y ~ dose, family = binomial)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -60.717      5.181  -11.72   &lt;2e-16 ***\ndose          34.270      2.912   11.77   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 284.202  on 7  degrees of freedom\nResidual deviance:  11.232  on 6  degrees of freedom\nAIC: 41.43\n\nNumber of Fisher Scoring iterations: 4\n\n\nor the anova table\n\n\nCode\nanova(glm.fit)\n\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: y\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev\nNULL                     7    284.202\ndose  1   272.97         6     11.232\n\n\nClearly, many of the results are repeated in the various R output.\nWhen considering testing model goodness of fit, it is important to distinguish between deviance values which compare a given model with the saturated model and deviance values which compare two competing models. For example, in the Analysis of Deviance Table, the right hand values compare the Null model with the saturated and the model including dose with the saturated. Whereas the left hand value is relevant to the comparison of Null model with the mode including dose.\nSummary of results:\nFrom the output, we can first test if the Null model, which contains only a constant term, is a good fit to the data using the hypotheses: \\[\nH_0: \\mbox{Null model is true} \\mbox{ against } H_1: \\mbox{Null model is false}.\n\\] From the output, note that the Null deviance is \\(D_0=284.202\\) and that this model has \\(r_0=1\\) parameters. Therefore, \\(D_0\\) follows a \\(\\chi^2\\) distribution with \\(n-r_0=8-1=7\\) degrees of freedom and hence the p-value is:\n\n\nCode\npchisq(284.202, 7, lower.tail = FALSE)\n\n\n[1] 1.425247e-57\n\n\nThis means that we reject \\(H_0\\) as the observed value of \\(D\\) is in the upper tail of the \\(\\chi^2\\) distribution.\nIf we had obtained a non-significant result, then we would stop the analysis and conclude that the Null model is an adequate fit.\nNext, consider testing the model which includes the explanatory variable where the the hypotheses are \\[\nH_0: \\mbox{Null model is true} \\mbox{ against } H_1: \\mbox{Model with dose is true}.\n\\] From the output, the deviance of the proposed model is \\(D_1=11.232\\) with \\(r_1=2\\) parameters. The test statistics is then \\(D_0-D_1=284.202-11.232=272.97\\) which follows a \\(\\chi^2\\) distribution with \\(r_1-r_0=2-1=1\\) degrees of freedom. The p-value is then\n\n\nCode\npchisq(272.97, 1, lower.tail = FALSE)\n\n\n[1] 2.556369e-61\n\n\nand we reject \\(H_0\\) in favour of \\(H_1\\) and conclude that dose is important.\nWe can finish the testing by checking if this model is a good fit to the data. That is, \\[\nH_0: \\mbox{The model with dose is true} \\mbox{ against } H_1: \\mbox{The model with dose is false}\n\\] The deviance of this is \\(D_1=11.232\\) with \\(r_1=2\\) parameters which follows a \\(\\chi^2\\) distribution with \\(n-r_1=8-2=6\\) degrees of freedom and has p=value\n\n\nCode\npchisq(11.232, 6, lower.tail = FALSE)\n\n\n[1] 0.08146544\n\n\nwhich means that we accept \\(H_0\\) at the 5% level and conclude that this model is an adequate fit to the data.\nFinally, we can look at the residuals:\n\n\nCode\npar(mar=c(4, 4, 0, 1))\n\nfit.resids = residuals(glm.fit, type=\"deviance\")\n\nplot(dose, fit.resids, pch=16)\nabline(h=0, lty=2)\n\n\n\n\n\nThese show a very slight u-shaped pattern and hence it would be work exploring fitting using the other link functions. For this data set, the complementary log-log (cloglog) link has a slightly better fit, see Figure 3.4, but a slight pattern remains.\nBefore finishing, suppose that we wish to predict, by hand, the probability for dose level \\(x_0=1.85\\), say, and the expected number of beetles killed when \\(m_0=60\\) beetles, say, are exposed.\nThe probability is predicted as \\[\n\\hat p_i = \\frac{\\exp\\{-60.7 + 34.3 x_0\\}}{(1+\\exp\\{ -60.7 + 34.3x_0\\})} = 0.936\n\\] and the expected number of beetles \\[\nm_0 \\, \\hat p_i =  56.2\n\\] Finally, suppose that we wish to find the minimum dose which kills 95% of the beetles, that is \\(p_0=0.95\\) which requires us to invert the logistic equation.\nIn general, suppose we wish to find \\(x_p\\) which corresponds to proportion \\(p\\) then \\[\nx_p= \\frac{1}{\\beta} \\left\\{ \\log\\left(\\frac{p}{1-p}\\right)-\\alpha \\right\\}\n\\] and hence here \\(\\hat x_p= 1.856\\)."
  },
  {
    "objectID": "5_logisticmodel.html#exercises",
    "href": "5_logisticmodel.html#exercises",
    "title": "5  Modelling Proportions",
    "section": "5.6 Exercises",
    "text": "5.6 Exercises\n\n5.1. Which of the following descriptions suggest that a logistic regression should be considered. Justify your answers.\n\nA pharmaceutical company perform clinical trials where volunteers are given varying doses of an experimental drug to study the chances of potential negative side effects. From historical data 4 previous volunteers out of a total of 120 have had negative side effects.\nA credit card company monitors sales to identify fraudulent transactions. On the previous five days, 26, 20, 18, 19 and 21 regular transactions occurred before the first fraudulent transaction was identified.\nA national internet service provider monitors the frequency of helpline phone calls regarding internet failures. From records, there are an average of 0.27 calls per day.\nThe University Student Education Services monitor the success of registered students every year. In particular, they record whether students pass their examinations, or not, and what application score they were given when they applied for a place at University.\n\n\n\nClick here to see hints.\n\nFor each, consider what is response, is it binomial and what is the explanatory variable. In cases which are not binomial, suggest an alternative distribution.\n\n\n\n5.2. For each of the situation produce appropriate graphs within RStudio. Describe the type of relationship, if any, shown in the plots. Is a linear, a logistic, or some other model appropriate? If linear or logistic, go ahead with model fitting and perform a residual analysis. Do you think that prediction from the model would be reliable? Justify each step of your analysis.\n\nA UK loan company requires a detailed questionnaire to be completed for any loan application including such information as income, regular expenditure, previous loan details and other information from data analytics and consumer credit reporting companies. All the information is combined into a single credit score from 0 to 1000. A large sample of credit applicants were contacted by an independent market research company to discover the outcome of their application. The data file loans.csv gives the number of successful application, number declined and the credit scores.\nA small estate agent company is interested in studying the changes in sales of houses due to targeted advertising campaigns and in particular to determine if additional spending is worthwhile. The company record the weekly sales before each advertising campaign and then again for one week a full month later, along with the cost of each campaign. The datafile advertising.csv contains the advertising budget values in pounds and corresponding changes in sale, also in pounds.\nThe yield of arable crops is sensitive to the nutrient levels in the soil. A key fertilizer used to boost production is the mineral phosphorus which is applied as phosphorus pentoxide, \\(P_2O_5\\). At a wheat large farm, various amounts from 0 kg/ha to 80 kg/ha were applied to investigate the effect on wheat yield, in kg/ha. Use the data in file wheat.csv to investigate this situation.\nCardiac troponin (cTn) is a human protein which is commonly measured in the diagnosis of heart attacks. A small sample of blood is taken from patients suspected of having heart attacks and the level of cardiac troponin is recorded The cardiac troponin levels, in ng/mL, of 100 patients arriving at a hospital Accident & Emergency department with significant chest discomfort along with the later confirmed diagnosis (0=No and 1=Yes).\n\n\n\nClick here to see hints.\n\nFor each, consider what is response, is it binomial and what is the explanatory variable. If the plot reveals a relationship which is either linear or logistic, then follow the appropriate steps previously discussed – check the code blocks in various examples. For the residual analysis, plot a graph of residuals against fitted values and check to a lack of pattern. For the linear cases, also consider the assumption of normality. For example using a histogram.\n\n\n\n5.3 In the beetle example, use the fitted logistic regression model, Equation 5.7, to predict what dose of gaseous carbon disulphide would kill 90% of beetles. Then, fit alternative link functions to the data and plot the results. Do you think that the choice of link function makes a difference to your conclusions? Justify your answer.\n\n\nClick here to see hints.\n\nFor the prediction, rearrange the usual expression for p in terms of x and the model parameters to give an expression for x. To fit alternative link functions, just change the argument in the family option.\n\n\n\n5.4 (Based on Dobson & Barnett, pp 144–145). Suppose there are 2 groups of people: the first group is exposed to some pollutant and the second group is not. In a prospective study, each group is followed for several years and categorised according to the presence or absence of some disease. Let \\(\\pi_i\\) denote the probability that a person in group \\(i\\) contracts the disease, \\(i=1,2.\\) The following \\(2 \\times 2\\) table summarizes the different possibilities.\n\n\n\n\nDiseased\nNot diseased\n\n\n\n\nExposed\n\\(\\pi_1\\)\n\\(1-\\pi_1\\)\n\n\nNot exposed\n\\(\\pi_2\\)\n\\(1-\\pi_2\\)\n\n\n\nNote that the sum of each row is 1. For each \\(i=1,2\\), the odds of contracting the disease is defined by \\[\nO_i = \\pi_i/(1-\\pi_i),\n\\] and a comparison between these two probabilities is given by the odds ratio \\[\n\\phi = \\frac{O_1}{O_2} = \\frac{\\pi_1(1-\\pi_2)}{\\pi_2(1-\\pi_1)}.\n\\]\n\nShow that \\(\\phi=1\\) if and only if there is no difference between the control and exposed groups. What does it mean if \\(\\phi &gt; 1\\)?\nConsider now \\(m\\) tables where each is of this form, with probabilities \\(\\pi_{ij}\\) represented by a logistic model \\[\n\\text{logit}(\\pi_{ij}) = \\alpha_i + \\beta_i x_j, \\ i=1,2,\\\nj=1,\\ldots,m,\n\\] where \\(x_j\\) is some specified quantitative explanatory variable. Interpret the parameters \\(\\alpha_i\\) and \\(\\beta_j\\), and give their effect on the log odds ratio \\(\\log \\phi_j\\), say, for each table. Show that \\(\\log \\phi_j\\) is constant across the \\(m\\) tables if \\(\\beta_1 = \\beta_2\\).\nGive a practical example where such a model might be appropriate.\nHow would you express this model in the R computer language?\n\n\n\nClick here to see hints.\n\nIn (a), one way is easy. For the other way, rearrange the odds ration and consider the ratios of the event occurring and the ratio that the events not occurring. These are only equal if the probabilities are the same. In (b), note that the log odds ratio is exactly the logistic transform. Substitute in and simplify. An example should be easy, and check ealier for the R command.\n\n\n\n5.5. Consider again the beetle example, but suppose that rather than the actual dose concentration had been measured, instead only whether a Low dose or High dose was used recorded. This would lead to the following table:\n\n\n\n\nDied\nNot died\nTotal\n\n\n\n\nLow dose\n65\n172\n237\n\n\nHigh dose\n226\n18\n244\n\n\n\nWhat are the respective probability of death values in the two dose groups? The relative risk is defined as the ratio of these two values. Calculate the risk of death due to High dose exposure relative to Low dose for this example. How could this value be used to measure the association between dose and death?\nWhat are the respective odds of death in each of the two groups? Calculate the odds ratio, defined as the ratio of these two values. How could this value be used to interpret the data?\n\n\nClick here to see hints.\n\nIf there is no difference then the relative risk should be 1 and a value substantially different indicates an association. The odds and the odds ratio have a similar interpretation. All measures are do the same thing, but each is standard in particular application areas."
  },
  {
    "objectID": "6_loglinearmodel.html#overview",
    "href": "6_loglinearmodel.html#overview",
    "title": "6  Loglinear Models",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nIn this chapter, we deal with data sets in which the response variable \\(y\\) is a count and the explanatory variables are all factors – i.e. qualitative variables. Initially we assume \\(y\\) has a Poisson distribution.\nSee Chapter 9 of Dobson and Barnett (2008). See also Agresti (1996) An introduction to categorical data analysis.\nWe assume a generalised linear model with\n\nresponses (counts) having independent Poisson distributions;\na logarithmic link function (hence the name log-linear model).\n\nConsider, for example, the two-way contingency table in Table 6.1}:\n\n\nTable 6.1: A two-way contingency table with \\(k_1\\) rows and \\(k_2\\) columns, where each entry \\(y_{ij}\\) is a count.\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n\\(\\dots\\)\n\\(j\\)\n\\(\\dots\\)\n\\(k_2\\)\nTotal\n\n\n\n\n1\n\\(y_{11}\\)\n\\(y_{12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{1k_2}\\)\n\\(y_{1+}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\\(i\\)\n\\(y_{i1}\\)\n\\(y_{12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{ik_2}\\)\n\\(y_{i+}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\\(k_1\\)\n\\(y_{k_11}\\)\n\\(y_{k_12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{k_2k_2}\\)\n\\(y_{1k_1}\\)\n\n\nTotal\n\\(y_{+1}\\)\n\\(y_{+2}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{+k_2}\\)\n\\(y_{++}\\)\n\n\n\n\nIn row \\(i\\) and column \\(j\\) of Table 6.1} we assume \\[\nY_{ij} \\sim \\text{Po}(\\lambda_{ij})\n\\] where \\[\n\\log \\lambda_{ij} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}.\n\\tag{6.1}\\] Here \\(\\mu\\) is called the main effect; \\(\\alpha_i\\) is a row effect; \\(\\beta_j\\) is a column effect; and \\((\\alpha \\beta)_{ij}\\) denotes an interaction effect parameter. Some or all of these effects must be present in the model, with constraints to ensure that the model is identifiable – this is sometimes refered ot as the identifiability or aliasing problem. Generally, R function automatically sets the first level of each effect to zero to achieve model identification. Thus, for the two-way model, \\[\n\\alpha_1 = 0, \\quad \\beta_1 = 0, \\quad\n(\\alpha \\beta)_{11}=(\\alpha \\beta)_{1j}=(\\alpha \\beta)_{k_1}=0.\n\\]"
  },
  {
    "objectID": "6_loglinearmodel.html#motivating-examples",
    "href": "6_loglinearmodel.html#motivating-examples",
    "title": "6  Loglinear Models",
    "section": "6.2 Motivating examples",
    "text": "6.2 Motivating examples\n\n6.2.1 Malignant melanoma\nThe data in Table 6.2 are from a study of 400 patients with malignant melanoma, a particular form of skin cancer (see Dobson and Barnett, p.172). For each tumour, its type and site were recorded. The data in Table 6.2 comprise the numbers of tumours \\(y\\) in each combination of site and tumour-type. This is a two-way contingency table. We want to know how melanoma frequency depends on site and type.\n\n\nTable 6.2: Melanoma counts by type and site.\n\n\nType\nHead\nTrunk\nExtremities\nTotal\n\n\n\n\nHutchinson’s melanotic freckle\n22\n2\n19\n34\n\n\nSuperficial spreading melanoma\n16\n54\n115\n185\n\n\nNodular\n19\n33\n73\n125\n\n\nIndeterminate\n11\n17\n28\n56\n\n\nTotal\n68\n106\n226\n400\n\n\n\n\n\n\nTable 6.3: Percentages across columns within rows for melanoma data.\n\n\nType\nHead\nTrunk\nExtremities\nTotal\n\n\n\n\nHutchinson’s melanotic freckle\n64.7\n5.9\n29.4\n100\n\n\nSuperficial spreading melanoma\n8.6\n29.2\n62.2\n100\n\n\nNodular\n15.2\n26.4\n58.4\n100\n\n\nIndeterminate\n19.6\n30.4\n50.0\n100\n\n\nTotal\n17.0\n26.5\n56.5\n100\n\n\n\n\n\n\nTable 6.4: Percentages across rows within columns for melanoma data.\n\n\n\n\n\n\n\n\n\nType\nHead\nTrunk\nExtremities\nTotal\n\n\n\n\nHutchinson’s melanotic freckle\n32.4\n1.9\n4.4\n8.5\n\n\nSuperficial spreading melanoma\n23.5\n50.9\n50.9\n46.3\n\n\nNodular\n27.9\n31.1\n32.3\n31.3\n\n\nIndeterminate\n16.2\n16.0\n12.4\n14.0\n\n\nTotal\n100.0\n99.9\n100.0\n100.0\n\n\n\n\nAlthough we have two factors, and , that we may use as predictors, standard ANOVA regression methods are inappropriate here as the dependent variable is not continuous but is instead a count. We will use log-linear regression, a type of generalised linear model, to analyse these data.\nTable 6.3 shows row and Table 6.4 column percentages for these data. For example, 15.2% of nodular melanomas occurred in the head and neck, 26.4% in the trunk, and 58.4% in the extremities. Compare this to the equivalent figures for Hutchinson’s melanotic freckles: 64.7%, 5.9%, and 29.4% - strikingly different. So different types of melanomas are more likely to occur in different locations.\n\n\n6.2.2 Flu vaccine\nThe data in Table 6.5 are from a randomised controlled trial in which 73 patients were randomised into two groups (see Dobson and Barnett, 2008, p.173). The treatment group was given a flu vaccine, while the control group was given a placebo. Levels of an antibody (HIA) were measured after 6 weeks and classified into three groups: Low, Moderate, and High.\n\n\nTable 6.5: Antibody responses to flu vaccine from a randomised controlled trial.\n\n\nGroup\nLow\nModerate\nHigh\n\n\n\n\nPlacebo\n25\n8\n5\n\n\nVaccine\n6\n18\n11\n\n\nTotal\n31\n26\n16\n\n\n\n\nIs the pattern of response the same for each treatment group? The percentages in Table 6.6 suggest not - row percentages indicate lower responses in the placebo group.\n\n\nTable 6.6: Percentages across rows within columns for antibody responses to flu vaccine.\n\n\nGroup\nLow\nModerate\nHigh\nTotal\n\n\n\n\nPlacebo\n65.8\n21.1\n13.2\n100\n\n\nVaccine\n17.1\n51.4\n31.4\n100\n\n\nTotal\n42.4\n35.6\n22.0\n100\n\n\n\n\n\n\nTable 6.7: Percentages across rows within columns for antibody responses to flu vaccine.\n\n\nGroup\nLow\nModerate\nHigh\n\n\n\n\nPlacebo\n80.6\n30.8\n31.2\n\n\nVaccine\n19.4\n69.2\n68.8\n\n\nTotal\n100\n100\n100"
  },
  {
    "objectID": "6_loglinearmodel.html#maximum-likelihood-estimation",
    "href": "6_loglinearmodel.html#maximum-likelihood-estimation",
    "title": "6  Loglinear Models",
    "section": "6.3 Maximum likelihood estimation",
    "text": "6.3 Maximum likelihood estimation\nRecall that for each cell \\(Y_{ij} \\sim \\text{Po}(\\lambda_{ij})\\) so \\(\\mbox{E}[Y_{ij}]= \\lambda_{ij}\\). However, we estimate \\(\\hat{\\lambda}_{ij} = y_{ij}\\) only for the saturated model given by Equation 6.1. In general, for non-saturated models, the estimate \\(\\hat\\lambda_{ij} \\ne y_{ij}\\).\nConsider the independence model for a 2-way table, that is where \\((\\alpha \\beta)_{ij} = 0\\) for all \\(i,j\\). Here, \\[\n\\log \\lambda_{ij} = \\mu +\\alpha_i + \\beta_j \\tag{6.2}\\] and \\(y_{ij}\\) is the observed count for cell \\((i,j)\\), so the likelihood is given by \\[\nL(\\lambda; y)\n=  \\prod_{i,j} \\frac{e^{-\\lambda_{ij}} \\lambda_{ij}^{y_{ij}}}{y_{ij}!}\n\\] and the log-likelihood is \\[\nl(\\lambda; y)\n=  \\sum_{i,j} \\left\\{ y_{ij} \\log \\lambda_{ij} - \\lambda_{ij} -\n    \\log y_{ij}!\\right\\}.\n\\] Next, using Equation 6.2, gives \\[\nl(\\lambda; y) =  \\sum_{i,j} \\left\\{ y_{ij} (\\mu + \\alpha_i + \\beta_j)\n    - \\exp(\\mu + \\alpha_i + \\beta_j)  - \\log y_{ij}! \\right\\}\n\\] which can be re-written as \\[\nl(\\lambda; y)   =  \\mu y_{++} + \\sum_i \\alpha_i y_{i+} + \\sum_j \\beta_j y_{+ j}\n    - e^{\\mu} \\left(\\sum_i e^{\\alpha_i}\\right) \\left(\\sum_j e^{\\beta_j}\n    \\right) - \\sum_{ij} \\log y_{ij}!.\n\\tag{6.3}\\]\nThe maximum likelihood estimates of the model parameters are obtained in the usual way. Differentiating Equation 6.3 with respect to \\(\\mu\\) and setting the result to zero gives, at the MLE, \\[\ny_{++} = e^{\\hat{\\mu}} \\left(\\sum_i e^{\\hat{\\alpha}_i}\\right)\n\\left(\\sum_j e^{\\hat{\\beta}_j}\\right).\n\\tag{6.4}\\]\nDifferentiating Equation 6.3 with respect to \\(\\alpha_i\\) (where \\(i \\ne 1\\) because \\(\\alpha_1=0\\) to avoid an identifiability problem) and setting the result to zero gives, at the MLE, \\[\ny_{i+} = e^{\\widehat{\\mu}}  e^{\\widehat{\\alpha}_i} \\left(\\sum_j e^{\\widehat{\\beta}_j}\\right).  \\notag \\\\\n% = \\sum_j \\widehat{\\lambda_{ij}} = \\widehat{\\lambda}_{i+} ~~~~(i \\not= 1).\n\\tag{6.5}\\] Differentiating Equation 6.3 with respect to \\(\\beta_j\\) (where \\(j \\not= 1\\) because \\(\\beta_1=0\\)) and setting the result to zero gives, at the MLE, \\[\ny_{+j} = e^{\\widehat{\\mu}}  e^{\\widehat{\\beta}_j} \\left(\\sum_i e^{\\widehat{\\alpha}_i}\\right).\n\\tag{6.6}\\] Then \\[\n\\frac{y_{i+}\\ y_{+j}}{y_{++}} = e^{\\widehat{\\mu}+\\widehat{\\alpha}_i+\\widehat{\\beta}_j}  \n= \\widehat{\\lambda}_{ij}.  \n\\tag{6.7}\\] It follows that \\[\n\\widehat{\\lambda}_{i+} = y_{i+} \\quad\n\\widehat{\\lambda}_{+j} = y_{+j} \\quad  \n\\widehat{\\lambda}_{++} = y_{++}.  \n\\]\nThus, the total fitted count in row \\(i\\) is identical to the total observed count in row \\(i\\). Further, the total fitted count in column \\(j\\) is equal to the total observed count in column \\(j\\) and the total fitted count equals the total observed count."
  },
  {
    "objectID": "6_loglinearmodel.html#model-fitting-in-r",
    "href": "6_loglinearmodel.html#model-fitting-in-r",
    "title": "6  Loglinear Models",
    "section": "6.4 Model fitting in R",
    "text": "6.4 Model fitting in R\n\n6.4.1 Malignant melanoma\nConsider an analysis of Melanoma data introduced in Section 6.2.1.\nTo test the independence of \\(\\texttt{Type}\\) and \\(\\texttt{Size}\\), we fit the model \\[\n\\texttt{Count} \\sim \\texttt{Type} + \\texttt{Site}\n\\] assuming Poisson counts and a logarithmic link function, using the commands:\n\n\nCode\nmelanoma = read.table(\"https://richardpmann.com/MATH3823/Datasets/melanoma.txt\", header=TRUE)\n\nmcount = melanoma$count\ntype= melanoma$type\nsite = melanoma$site\n\ntype.F = as.factor(type)\nsite.F = as.factor(site)\n\nglm1 = glm(mcount ~ type.F + site.F, family='poisson')\nsummary(glm1)\n\n\n\nCall:\nglm(formula = mcount ~ type.F + site.F, family = \"poisson\")\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.7544     0.2040   8.600  &lt; 2e-16 ***\ntype.F2       1.6940     0.1866   9.079  &lt; 2e-16 ***\ntype.F3       1.3020     0.1934   6.731 1.68e-11 ***\ntype.F4       0.4990     0.2174   2.295  0.02173 *  \nsite.F2       0.4439     0.1554   2.857  0.00427 ** \nsite.F3       1.2010     0.1383   8.683  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 295.203  on 11  degrees of freedom\nResidual deviance:  51.795  on  6  degrees of freedom\nAIC: 122.91\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe see that the residual deviance for this model is \\(51.795\\) on \\(6\\) degrees of freedom. If the model is true, the residual deviance will have an approximate \\(\\chi^2\\) distribution on \\(6\\) degrees of freedom. If the model is not true, the residual deviance will probably be too large to correspond to this distribution. Thus we calculate the \\(p\\)-value in the upper tail of the \\(\\chi^2_6\\) distribution, which can be computed using the command:\n\n\nCode\npchisq(51.795,6,lower.tail=F)\n\n\n[1] 2.050465e-09\n\n\nThis strongly indicates that the independence model is inadequate. Therefore, unless we can spot alternative simplifications, we will have to use the saturated model.\nWe can look at residuals from the model see where the departures from independence occur. The largest residual is for Hutchinson’s freckle on the head and neck, which occurs more often than would be expected under independence.\nFor the saturated model, \\(\\widehat{\\lambda}_{ij} = y_{ij}\\). In this example, \\(\\sum_{ij} \\widehat{\\lambda}_{ij} = \\sum_{ij} y_{ij} = 400\\), so the probability of a tumour being in category \\((i,j)\\) is \\(y_{ij}/400\\) — just the observed proportions.\nNote that in Table 6.2 we have a total of \\(y_{++} = 400\\) observations. If the data were truly Poisson, this would be a suspiciously round number. In reality, this total was fixed by design, so we should take into account the fact that \\(y_{++} = 400\\) and fit a more suitable model, such as the multinomial model which we will meet in the next chapter.\nOverdispersion can occur for the Poisson model, just as in the binomial case – see Section 5.3. This is called extra-Poisson variation. The \\(\\texttt{glm}\\) function in R can take this into account by including an extra parameter \\(\\tau\\) in the model using \\(\\texttt{family=quasipoisson()}\\).\n\n\n6.4.2 Flu vaccine\nThere is no saved data file for this example – recall it is very small - and so define R variables for the \\(\\texttt{count}\\), \\(\\texttt{response}\\) (perhaps not a good name as it may be confusing but it seems the correct description from the context) and \\(\\texttt{drug}\\) used. Don’t forget to declare the explanatory variables as factors.\n\n\nCode\n# Define the data\ncount = c(25, 8, 5, 6,18,11)\nresponse = c(\"L\",\"M\",\"H\", \"L\",\"M\",\"H\")\ndrug = c(\"P\",\"P\",\"P\",\"V\",\"V\",\"V\")\n\nresponse.F = as.factor(response)\ndrug.F = as.factor(drug)\n\n\nNow fit the first model, here consider the saturated model with main effects and interaction.\n\n\nCode\n##########################################\n# Fit a log-linear model - saturated model\nglm.fit0 = glm(count ~ response.F * drug.F, family=poisson)\nsummary(glm.fit0)\n\n\n\nCall:\nglm(formula = count ~ response.F * drug.F, family = poisson)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          1.60944    0.44721   3.599  0.00032 ***\nresponse.FL          1.60944    0.48990   3.285  0.00102 ** \nresponse.FM          0.47000    0.57009   0.824  0.40969    \ndrug.FV              0.78846    0.53936   1.462  0.14379    \nresponse.FL:drug.FV -2.21557    0.70539  -3.141  0.00168 ** \nresponse.FM:drug.FV  0.02247    0.68663   0.033  0.97389    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2.3807e+01  on 5  degrees of freedom\nResidual deviance: 4.6629e-15  on 0  degrees of freedom\nAIC: 37.128\n\nNumber of Fisher Scoring iterations: 3\n\n\nEven though the model is a perfect fit – the fitted values are exactly the data values (check next) we see that \\(\\texttt{response.F=\"M\"}\\) is not significantly different to the baseline \\(\\texttt{response.F=\"H\"}\\) and also that \\(\\texttt{drug.F=\"V\"}\\) is not significantly different to \\(\\texttt{drug.F=\"P\"}\\).\nLet’s check the fitted values:\n\n\nCode\npredict(glm.fit0, type=\"response\")\n\n\n 1  2  3  4  5  6 \n25  8  5  6 18 11 \n\n\nwhere we see that, indeed, they are the same as the data.\nThe Null deviance is \\(23.807\\) and follows a \\(\\chi^2\\) distribution on \\(5\\) degrees of freedom (under the Null hypothesis). This has a p-value of  \\(p&lt;0.001\\) which is highly significant and hence the null model is not adequate.\nWe cannot test the saturated model as it fits perfectly.\nNext let’s try a reduced model with the interaction removed:\n\n\nCode\n##########################################\n# Fit model without interaction\nglm.fit1 = glm(count ~ response.F + drug.F, family=poisson)\nsummary(glm.fit1)\n\n\n\nCall:\nglm(formula = count ~ response.F + drug.F, family = poisson)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.11972    0.27408   7.734 1.04e-14 ***\nresponse.FL  0.66140    0.30783   2.149   0.0317 *  \nresponse.FM  0.48551    0.31774   1.528   0.1265    \ndrug.FV     -0.08224    0.23428  -0.351   0.7256    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 23.807  on 5  degrees of freedom\nResidual deviance: 18.643  on 2  degrees of freedom\nAIC: 51.771\n\nNumber of Fisher Scoring iterations: 5\n\n\nThe \\(\\texttt{response.F=\"M\"}\\) is again not significantly different to the baseline \\(\\texttt{response.F=\"H\"}\\) and \\(\\texttt{drug.F=\"V\"}\\) is not significantly different to \\(\\texttt{drug.F=\"P\"}\\).\nThe model deviance is \\(18.643\\) following a \\(\\chi^2_2\\) distribution with corresponding p-value of \\(p &lt;0.001\\).  This is highly significant suggesting that this and any model without an interaction term is unlikely to be adequate.\nLet use return to a model with interaction, and let’s try merging response \\(\\texttt{response.F=\"M\"}\\) and \\(\\texttt{\"H\"}\\) – calling them \\(\\texttt{\"HM\"}\\) (I chose this name so that alphabetically it would be first just as \\(\\texttt{\"H\"}\\) was first):\n\n\nCode\n# Medium response not significant\n# Merge response M with response H -- to response HM\nresponse2 = response\nresponse2[response==\"H\"] = \"HM\"\nresponse2[response==\"M\"] = \"HM\"\nresponse2.F = as.factor(response2)\n\nglm.fit2 = glm(count ~ response2.F*drug.F, family=poisson)\nsummary(glm.fit2)\n\n\n\nCall:\nglm(formula = count ~ response2.F * drug.F, family = poisson)\n\nCoefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            1.8718     0.2774   6.749 1.49e-11 ***\nresponse2.FL           1.3471     0.3419   3.940 8.17e-05 ***\ndrug.FV                0.8023     0.3338   2.404   0.0162 *  \nresponse2.FL:drug.FV  -2.2295     0.5640  -3.953 7.71e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 23.807  on 5  degrees of freedom\nResidual deviance:  2.405  on 2  degrees of freedom\nAIC: 35.533\n\nNumber of Fisher Scoring iterations: 4\n\n\nInterestingly all terms in this model are significant but let’s check the overall goodness of fit.\nThe model deviance is \\(2.405\\) following a \\(\\chi^2_2\\) distribution with corresponding p-value of \\(p=0.3\\).  This is non-significant suggesting that this model is a good fit to the data.\nThe fitted values are:\n\n\nCode\npredict(glm.fit2, type=\"response\")\n\n\n   1    2    3    4    5    6 \n25.0  6.5  6.5  6.0 14.5 14.5 \n\n\nand the residuals (which I have rounded for display purposes):\n\n\nCode\nround(residuals(glm.fit2, type=\"response\"),2)\n\n\n   1    2    3    4    5    6 \n 0.0  1.5 -1.5  0.0  3.5 -3.5 \n\n\nThe fitted values are a good fit to the data and hence, of course, the residuals are not too large.\nThe overall conclusion is that the model with response and drug, and their interaction, but with the merging of response levels H and M is an appropriate model. This indicates that the vaccine does have an effect on the antibody response but that it does not influence whether that is a medium or high response."
  },
  {
    "objectID": "6_loglinearmodel.html#multi-way-contingency-tables",
    "href": "6_loglinearmodel.html#multi-way-contingency-tables",
    "title": "6  Loglinear Models",
    "section": "6.5 Multi-way contingency tables",
    "text": "6.5 Multi-way contingency tables\nWe can generalize the model notation introduced in Equation 6.1 to accommodate multi-way contingency tables; that is tables of counts indexed by multiple factors. For example, for a 3-way table of cell counts \\(y_{ijk}\\), the saturated model would be written: \\[\nY_{ijk} \\sim \\text{Po}(\\lambda_{ijk})\n\\] where \\[\n\\log \\lambda_{ijk} =\n\\mu + \\alpha_i + \\beta_j +\\gamma_k\n+ (\\alpha \\beta)_{ij}\n+ (\\alpha\\gamma)_{ik}\n+ (\\beta \\gamma)_{jk}\n+ (\\alpha \\beta \\gamma)_{ijk}.\n\\] Here, \\(\\alpha_i\\), \\(\\beta_j\\) and \\(\\gamma_j\\) are main effects; \\((\\alpha \\beta)_{ij}\\), \\((\\alpha\\gamma)_{ik}\\), and \\((\\beta \\gamma)_{jk}\\) are two-way interaction effects; and \\((\\alpha \\beta \\gamma)_{ijk}\\) is a three-way interaction.\nThis approach is easily extended to tables of any number of factors. Note, however, that not all terms need be present in a model, thought 3-way or higher-order interactions might sometimes be required.\nA hierarchical model is one in which each term in the model is accompanied by all lower-order terms. For example, including the term \\((\\alpha\\beta\\gamma)_{ijk}\\) would require inclusion of each of the terms \\(\\alpha_i, \\beta_j, \\gamma_k, (\\alpha\\beta)_{ij}, (\\alpha\\gamma)_{ik}, (\\beta\\gamma)_{jk}\\). We will be concerned only with hierarchical contingency table models. Non-hierarchical models are sometimes appropriate, depending on the nature of the factors involved, but hierarchical models are more generally applicable and easier to interpret."
  },
  {
    "objectID": "6_loglinearmodel.html#exercises",
    "href": "6_loglinearmodel.html#exercises",
    "title": "6  Loglinear Models",
    "section": "6.6 Exercises",
    "text": "6.6 Exercises\n\n6.1 Overdispersion is an occasional problem when fitting generalised linear models with a known scale parameter in situations where there is unexplained variation. In this exercise we illustrate the problem in Poisson regression. The starting point is the observation that if \\(Y \\sim P(\\lambda)\\), then \\(\\mbox{E}[Y] = \\lambda\\) and \\(\\mbox{Var}[Y] = \\lambda\\).\n\nConsider joint random variables \\((X,Y)\\) where \\(X\\) takes two possible values with equal probabilities, \\(Pr(X=1) = Pr(X=2) = 1/2\\).\nSuppose the conditional distribution of \\(Y\\) given \\(X\\) is Poisson, \\[\\begin{equation*} \\label{eq:pmix}\nY|(X=1) \\sim P(\\lambda_1), \\quad Y|(X=2) \\sim P(\\lambda_2), \\quad \\quad \\quad (*)\n\\end{equation*}\\] where \\(\\lambda_1 &lt; \\lambda_2\\). Let \\(\\lambda = (\\lambda_1 + \\lambda_2)/2\\) denote the average value. Thus the marginal distribution of \\(Y\\) is a mixture of two Poisson distributions. Show that \\[\\begin{equation*}\n\\mbox{E}[Y] = \\lambda, \\quad \\mbox{Var}[Y] = \\lambda + (\\lambda_1 - \\lambda_2)^2/4,\n\\end{equation*}\\] that is, although the mean of \\(Y\\) is the same under the mixture (or conditional Poisson) model as under the Poisson model, the variance is larger.\n\n\nClick here to see hints.\n\nStart with the definition of expectation as a weighted average of the conditional expectations. Then, for the variance, apply a similar result to find the expectation of the square.\n\nThis phenomenon might be observed in data as follows. Let \\(n=60\\) and let an explanatory variable \\(x_i\\) take the value \\(x_i=1\\) for \\(i=1, \\ldots, 30\\) and \\(x_i=2\\) for \\(i=31, \\ldots, 60\\). Suppose that the observations \\(y_i | x_i\\) come from the above conditional Poisson model \\((*)\\).\nConsider fitting the following two models in R with Poisson errors and a log link function: \\[\\begin{equation*}\n\\text{(i)} ~~ y \\sim 1, \\quad \\quad \\text{(ii)} ~~ y \\sim x.\n\\end{equation*}\\]\nSince model (ii) is the correct model, it should yield a good fit to the data. But if the experimenter does not know about the variable \\(x\\), it will only be feasible to fit model (i). Let \\(\\overline{Y}\\) and \\(S^2\\) denote the sample mean and variance of the \\(Y_i, \\ i=1,\\ldots,60\\). Show that \\[\\begin{equation*}\n\\mbox{E}[\\overline{Y}] = \\lambda, \\quad\n\\mbox{E}[S^2] = \\lambda +\n\\frac{60}{59}(\\lambda_1 - \\lambda_2)^2/4.\n\\end{equation*}\\]\nHence show that the Pearson \\(\\chi^2\\) goodness of fit statistic for model (i) will indicate a poorly fitting model if \\(\\lambda_1\\) and \\(\\lambda_2\\) are far apart.\n\n\nClick here to see hints.\n\nFor the expectation of the sample variance, first find the expectation of the square of the sample mean. Next, note that the Pearson goodness of fit statistics depends on the ratio of the sample variance divided by the sample mean. Under the conditional Poisson model the variance would be larger but the mean unchanged, hence the statistics gets bigger and so will be further into the upper tail.\n\nThis example is very simple, but overdispersion can occur much more widely. Why is overdisperson not a problem for generalised linear models in which the response distribution includes a scale parameter?\n\n\nClick here to see hints.\n\nIn the Poisson, and the Binomial, the problem is caused by the mean and variance being defined by the same model parameter.\n\n\n6.2 (From Dobson & Barnett, p 163) This question should be done in a computer package such as R. You should think carefully about which variables, if any, to condition on in your analysis: HOME = 1,2,3, CONTACT = 1,2, or SATISFACTION = 1,2,3.\nThe data relate to an investigation into satisfaction with housing conditions in Copenhagen. Residents of selected areas living in rented houses built between 1960 and 1968 were questioned about their satisfaction and their degree of contact with other residents. The data were tabulated by type of housing. Investigate the associations between satisfaction, contact with other residents and type of housing.\nLow Contact:\n\n\n\nSatisfaction:\nLow\nMedium\nHigh\n\n\n\n\nTower blocks\n65\n54\n100\n\n\nApartments\n130\n76\n111\n\n\nHouses\n67\n48\n62\n\n\n\nHigh Contact:\n\n\n\nSatisfaction:\nLow\nMedium\nHigh\n\n\n\n\nTower blocks\n34\n47\n100\n\n\nApartments\n141\n116\n191\n\n\nHouses\n130\n105\n104\n\n\n\n\nProduce appropriate tables of percentages to gain initial insights into the data; for example, percentages in each contact level by type of housing and level of satisfaction, or percentages in each level of satisfaction by contact and type of housing.\n\n\nClick here to see hints.\n\nConsider many 2x2 tables for separate levels of the third variable, Each time re-scaling to create rows or columns, as appropriate, summing to 100%.\n\nUsing e.g. R, fit various log-linear models to investigate interactions between the variables.\n\n\nClick here to see hints.\n\nDefine your own variables and make sure the explanatory variables are converted to factor. Including the three-way interaction would lead to the saturated model – why? – and hence only consider pair-wise two-way interactions (as well as the variables separately).\n\nFor some model that fits (at least moderately) well, calculate the Pearson residuals and use them to find where the largest discrepancies are between the observed and expected values.\n\n\nClick here to see hints.\n\nUse the residuals command with type=“pearson”"
  },
  {
    "objectID": "7_extendedloglinearmodel.html#overview",
    "href": "7_extendedloglinearmodel.html#overview",
    "title": "7  Extensions to Loglinear models",
    "section": "7.1 Overview",
    "text": "7.1 Overview\nIn this chapter we consider two extensions to the log-linear models studied in the previous chapter. Recall that the log-linear model assumes that each observation follows a Poisson distribution, but in many cases these will not be all independent. Instead, row totals or the grand total will be fixed. This structure means that the observed counts will no longer follow the Poisson distribution and hence modification to the theory and calculations are needed."
  },
  {
    "objectID": "7_extendedloglinearmodel.html#contingency-tables-with-fixed-marginals",
    "href": "7_extendedloglinearmodel.html#contingency-tables-with-fixed-marginals",
    "title": "7  Extensions to Loglinear models",
    "section": "7.2 Contingency tables with fixed marginals",
    "text": "7.2 Contingency tables with fixed marginals\n\n7.2.1 Introduction\nIn the melanoma example, see Table 6.2, the overall total number of observations was exactly \\(400\\). If the data were truly Poisson, this would be a suspiciously round number. In reality, this total was fixed by design, so we should take into account the fact that \\(y_{++} = 400\\) and fit a more appropriate model. Also, recall the flu vaccine example of Table 6.5. Suppose that some of the marginal totals are fixed by design. For example, that the number of patients in each arm of the trial (38 in the Placebo group, 35 in the Vaccine group) was fixed before data collection started. In this case, we should also take into account the fact that \\(y_{1+}=38\\) and \\(y_{2+}=35\\) and fit a more suitable model.\nBefore we can re-consider these data sets, we first need to establish some theoretical results to deal with such conditional distribution cases.\n\n\n7.2.2 Modelling\nConsider, again, a general two-way contingency table such as that in Table 7.1. The fixed marginals are the column sums: \\(y_{+j},\\) the row sums: \\(y_{i+},\\) and the overall sum: \\(y_{++}.\\)\n\n\nTable 7.1: A two-way contingency table with \\(k_1\\) rows and \\(k_2\\) columns, where each entry \\(y_{ij}\\) is a count.\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n\\(\\dots\\)\n\\(j\\)\n\\(\\dots\\)\n\\(k_2\\)\nTotal\n\n\n\n\n1\n\\(y_{11}\\)\n\\(y_{12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{1k_2}\\)\n\\(y_{1+}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\\(i\\)\n\\(y_{i1}\\)\n\\(y_{12}\\)\n\\(\\dots\\)\n\\(y_{1j}\\)\n\\(\\dots\\)\n\\(y_{ik_2}\\)\n\\(y_{i+}\\)\n\n\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\\(\\dots\\)\n\n\n\\(k_1\\)\n\\(y_{k_11}\\)\n\\(y_{k_12}\\)\n\\(\\dots\\)\n\\(y_{k_1j}\\)\n\\(\\dots\\)\n\\(y_{k_1k_2}\\)\n\\(y_{k_1+}\\)\n\n\nTotal\n\\(y_{+1}\\)\n\\(y_{+2}\\)\n\\(\\dots\\)\n\\(y_{+j}\\)\n\\(\\dots\\)\n\\(y_{+k_2}\\)\n\\(y_{++}\\)\n\n\n\n\nSuppose that we have fixed row totals \\(y_{i+},\\) or a fixed overall total \\(y_{++}\\), how would such constraints affect the distribution of the random variables \\(Y_{ij}\\)?\nTo answer this question, we will need the following results.\n\nProposition 7.1 Let \\(Y_{ij}\\), for \\(i=1,\\dots,k_1\\) and \\(j=1,\\dots, k_2\\), be the responses in a two-way contingency table, with \\(k_1\\) rows and \\(k_2\\) columns. Further, assume that the counts follow independent Poisson distributions with corresponding means \\(\\lambda_{ij}\\), that is \\(Y_{ij}\\sim \\mbox{Po}(\\lambda_{ij})\\). Then, the distributions of the row sums are, \\[\nY_{i+} \\sim \\mbox{Po}(\\lambda_{i+}),\n\\] where \\(\\displaystyle Y_{i+} = \\sum_{j=1}^{k_2} Y_{ij}\\) and \\(\\displaystyle \\lambda_{i+}=\\sum_{j=1}^{k_2} \\lambda_{ij}\\).\nNotes:\n\nHere we are not yet assuming that the marginal totals are fixed.\nSimilar results hold when considering column sums and the overall sum.\n\n\nProof: See MATH2715, Chapter 7 on MGF Properties of Linear Functions.\n\nProposition 7.2 Let \\(Y_{ij}\\sim \\mbox{Po}(\\lambda_{ij})\\) for \\(i=1,\\dots,k_1\\) and \\(j=1,\\dots, k_2\\) then the conditional distribution of each count in a row given that the row sum is fixed is given by \\[\n\\left( Y_{ij} \\, |\\,   Y_{i+} = m \\right)\n\\sim\n\\mbox{Bin}\\left(m,\n\\frac{\\lambda_{ij}}{\\lambda_{i+}}\n\\right).\n\\] That is, conditioning on the sum of independent Poisson random variables induces a Binomial distribution.\n\nProof: First define \\(Y_{i-j}= Y_{i+}-Y_{ij}\\) with correspondingly \\(y_{i-j}= m-y_{ij}\\) and also \\(\\lambda_{i-j}= \\lambda_{i+}-\\lambda_{ij}\\).\nConsider the conditional probability mass function \\[\\begin{align*}\np(Y_{ij} = y_{ij} | Y_{i+} = m)  \n& = \\frac{p(Y_{ij} = y_{ij}, Y_{i+} = m)} {p(Y_{i+} = m)}\n\\quad \\text{by conditional probability definition}\\\\\n& = \\frac{p(Y_{ij} = y_{ij}, Y_{i-j} = m-y_{ij})} {p(Y_{i+} = m)} \\quad \\text{rearranging} \\\\\n& = \\frac{p(Y_{ij} = y_{ij})p(Y_{i-j} = m-y_{ij})} {p(Y_{i+} = m)} \\quad \\text{by independence}.\n\\end{align*}\\] Then, each of these follows a Poisson distribution: \\(Y_{ij} \\sim \\text{Po}(\\lambda_{ij})\\), \\(Y_{i-j}\\sim \\text{Po}(\\lambda_{i-j})\\), and \\(Y_{i+} \\sim \\text{Po}(\\lambda_{i+})\\). Therefore, \\[\\begin{align*}\np(Y_{ij} = y_{ij} | Y_{i+} = m)  \n& =\n\\frac{\\lambda_{ij}^{y_{ij}}e^{-\\lambda_{ij}}}{y_{ij}!}\n\\frac{\\lambda_{i-j}^{m-y_{ij}}e^{-\\lambda_{i-j}}}{m-y_{ij}!}\n\\Bigg/\n\\frac{\\lambda_{i+}^{m} e^{-\\lambda_{i+}}}{m!} \\\\\n& =\n{m \\choose y_{ij}}\n\\left(\\frac{\\lambda_{ij}}{\\lambda_{i+}} \\right) ^{y_{ij}}\n\\left(\\frac{\\lambda_{i-j}}{\\lambda_{i+}} \\right) ^{m-y_{ij}}\\\\\n& =\n{m \\choose y_{ij}}\n\\pi ^{y_{ij}}\n\\left(1-\\pi\\right) ^{m-y_{ij}}\n\\end{align*}\\]\nwhich is the probability mass function of a \\(\\text{Bin}(m, \\pi)\\) random variable, where \\(\\pi = \\lambda_{ij} / \\lambda_{i+}\\).\n\nProposition 7.3 Let \\(Y_{ij}\\sim \\mbox{Po}(\\lambda_{ij})\\) for \\(i=1,\\dots,k_1\\) and \\(j=1,\\dots, k_2\\) then the joint conditional distribution of the counts in a row given that the row sum is fixed is given by \\[\np(Y_{i1},\\dots, Y_{i k_2} \\, | \\, Y_{i+}=m)\n=\n\\frac{m!}{y_{i1}!\\cdots y_{i k_2}!}\n\\pi_{i1}^{y_{i1}} \\cdots\\pi_{ik_2}^{y_{ik_2}}\n\\] with \\[\n\\pi_{ij}=\\lambda_{ij}/\\lambda_{i+}\n\\quad \\mbox{ and } \\quad\n\\sum_{j=1}^{k_2} \\pi_{ij} =1.\n\\] This distribution is called the Multinomial distribution, with index \\(m\\) and parameters \\(\\pi_{ij}, j=1,\\dots k_2\\), which can be denoted \\[\n(Y_{i1},\\dots, Y_{i k_2} \\, | \\, Y_{i+}=m)\n\\sim\n\\text{Mult}(m, \\pi_{i1}, \\dots \\pi_{ik_2}\n).\n\\]\n\nProof: Omitted.\n\nProposition 7.4 Let \\(Y_{ij}\\sim \\mbox{Po}(\\lambda_{ij})\\) for \\(i=1,\\dots,k_1\\) and \\(j=1,\\dots, k_2\\) then the conditional distribution of the counts given that the overall sum is fixed is \\[\n(Y_{11},\\dots, Y_{k_1k_2} \\, | \\, Y_{++}=m)\n\\sim\n\\text{Mult}(m, \\pi_{11},\\dots, \\pi_{k_1k_2}).\n\\] with probability mass function given by \\[\np(Y_{11},\\dots, Y_{k_1k_2} \\, | \\, Y_{++}=m)\n=\n\\frac{m!}{y_{11}!\\cdots y_{k_1k_2}!}\n\\pi_{11}^{y_{11}} \\cdots\\pi_{k_1k_2}^{y_{k_1k_2}}\n\\] where \\(\\pi_{ij} = \\lambda_{ij}/\\lambda_{++}\\).\n\nProof: Omitted."
  },
  {
    "objectID": "7_extendedloglinearmodel.html#product-multinomial-models",
    "href": "7_extendedloglinearmodel.html#product-multinomial-models",
    "title": "7  Extensions to Loglinear models",
    "section": "7.3 Product-multinomial models",
    "text": "7.3 Product-multinomial models\nThe Poisson contingency table model assumes that each cell count (and therefore row and column total) is random – specifically, Poisson.\nHowever, in many examples, some of the marginal totals are fixed by design.\nFor example, as discussed in the Flu-vaccine example of Section 6.2.2, the numbers in the placebo (38) and vaccine (35) groups are not random, but fixed by the experimenter. Further, we are not interested in modelling these row totals but instead the response to treatment (placebo or vaccine). Thus, in the Flu vaccine example, we should condition the Poisson model on the row totals.\nFrom Equation 6.1 the saturated model for a two-factor table is \\[\n\\lambda_{ij} =\n\\exp\\left\\{\\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}\\right\\}.\n\\] Then, using the result in Proposition 7.3, we see that \\[\n\\pi_{ij} = \\frac{\\lambda_{ij}}{\\lambda_{i+}}\n=\n\\frac{\\exp\\left\\{\\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}\\right\\}}{\\sum_j \\exp\\{\\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}\\}}\n\\] which simplifies to give \\[\n\\pi_{ij}\n=\n\\frac{\\exp\\left\\{\\beta_j + (\\alpha \\beta)_{ij}\\right\\}}{\\sum_j \\exp\\left\\{\\beta_j + (\\alpha \\beta)_{ij}\\right\\} }.\n\\tag{7.1}\\] Thus, for each row, that is each \\(i\\), this multinomial distribution depends on \\(\\beta_j\\) and \\((\\alpha\\beta)_{ij}\\) for \\(j=1,\\dots, k_2\\), but not on \\(\\mu\\) and \\(\\alpha_i\\). This means that the data in each row of the table are independent and hence, overall, the model likelihood is given by \\[\nL(\\boldsymbol{\\beta}; \\mathbf{y}) =\n\\prod_{i=1}^{k_1}\np(Y_{i1},\\dots, Y_{i k_2} \\, | \\, Y_{i+}=y_{i+}),\n\\] with \\(\\boldsymbol{\\beta}=\\{\\mu, \\alpha_1,\\dots, \\alpha_{k_1}, \\beta_1,\\dots, \\beta_{k_2}, (\\alpha\\beta)_{11},\\dots, (\\alpha\\beta)_{k_1k_2}\\}\\) and where \\[\np(Y_{i1},\\dots, Y_{i k_2} \\, | \\, Y_{i+}=y_{i+})\n=\n\\frac{y_{i+}!}{y_{i1}!\\cdots y_{i k_2}!}\n\\pi_{i1}^{y_{i1}} \\cdots\\pi_{ik_2}^{y_{ik_2}} .\n\\] Further, using Equation 7.1 the log-likelihood is given by \\[\nl(\\boldsymbol{\\beta}; \\mathbf{y}) =\n\\text{const} +\n\\sum_{i=1}^{k_1} \\sum_{j=1}^{k_2}\ny_{ij}\n\\left\\{\n\\beta_j + (\\alpha \\beta)_{ij}\n-  \\log \\sum_j \\exp\\left\\{\\beta_j + (\\alpha \\beta)_{ij}\\right\\}\\right\\}.\n\\] Recall that the saturated model will have the fitted values equal to the data values in all cases. Therefore, we concentrate on the product multinomial models with interactions set to zero, that is \\((\\alpha\\beta)_{ij}=0\\), then \\[\nl(\\boldsymbol{\\beta}; \\mathbf{y}) =\n\\text{const} +\n\\sum_{i=1}^{k_1} \\sum_{j=1}^{k_2}\ny_{ij} \\left\\{ \\beta_j\n-  \\log \\sum_j \\exp\\left\\{\\beta_j \\right\\}\\right\\}\n\\] which simplifies to \\[\nl(\\boldsymbol{\\beta}; \\mathbf{y}) =\n\\text{const} +\n\\sum_{j=1}^{k_2}\ny_{+j} \\left\\{ \\beta_j\n-  y_{++}\\log \\sum_j \\exp\\left\\{\\beta_j \\right\\}\\right\\}.\n\\] Differentiating this with respect to single parameter \\(\\beta_k\\) and setting to zero gives \\[\n\\frac{y_{+k}}{y_{++}} =\n{ e^{\\hat\\beta_k}}\\Big/ {\\sum_j e^{\\hat\\beta_j}}\n\\tag{7.2}\\] which is identical to the parameter estimate for the previous independence Poisson model. Thus, in this case, if we want to fit the product multinomial model (which is hard) we can instead fit the Poisson model (which is easy). Note, however, that these estimates only coincide when \\(\\alpha_1, \\dots, \\alpha_{k_1}\\) are included in the Poisson independence model, even though we see that these are eliminated from the multinomial model. In fact, these parameter estimates reflect the fixed marginal totals in the multinomial situation.\nAlthough this derivation has only considered the cases of the independence Poisson model: \\(\\log \\lambda_{ij} = \\mu+\\alpha_i+\\beta_j\\) and the product multinomial model with \\((\\alpha\\beta)_{ij}=0\\), the following result gives general guidance.\n\nTheorem 7.1 Tables with fixed margin sums can be analyzed using a multinomial or product-multinomial model as though they were independent Poisson models, with log-linear predictor, provided terms corresponding to the fixed margins are included in the model. Then the MLEs for the two models yield the same values for the parameters of interest and have the same deviances.\n\nFor a 2-way table:\n\n\n\nMultinomial conditioning on\nPoisson model must include\n\n\n\n\n\\(y_{++}\\)\n\\(\\mu\\)\n\n\n\\(y_{i+}\\)\n\\(\\mu, \\alpha_i\\)\n\n\n\nFor a 3-way table:\n\n\n\n\n\n\n\nMultinomial conditioning on\nPoisson model must include\n\n\n\n\n\\(y_{+++}\\)\n\\(\\mu\\)\n\n\n\\(y_{i++}\\)\n\\(\\mu, \\alpha_i\\)\n\n\n\\(y_{ij+}\\)\n\\(\\mu, \\alpha_i, \\beta_j, (\\alpha\\beta)_{ij}\\)\n\n\n\\(y_{ij+}\\), \\(y_{+jk}\\)\n\\(\\mu, \\alpha_i, \\beta_j, \\gamma_k, (\\alpha\\beta)_{ij},(\\beta\\gamma)_{jk}\\)\n\n\n\nProof: Omitted. For details see Birch, 1963, JRSS(B), 220–233. The proof is not especially difficult but is a bit messy. It depends on the property of the Poisson model with log link function that observed totals equals fitted totals for the effects present in the Poisson model."
  },
  {
    "objectID": "7_extendedloglinearmodel.html#model-fitting-in-r",
    "href": "7_extendedloglinearmodel.html#model-fitting-in-r",
    "title": "7  Extensions to Loglinear models",
    "section": "7.4 Model fitting in R",
    "text": "7.4 Model fitting in R\nConsider again he flu vaccine data described in Section 6.2.2 with data repeated below. Recall that 38 people were recruited to the Placebo group and 35 to the Vaccine group and that we assume these were fixed before data collection started.\n\nAntibody responses to flu vaccine from a randomised controlled trial.\n\n\nGroup\nLow\nModerate\nHigh\nTotal\n\n\n\n\nPlacebo\n25\n8\n5\n38\n\n\nVaccine\n6\n18\n11\n35\n\n\nTotal\n31\n26\n16\n73\n\n\n\nThe fixed row sums mean that a product-multinomial model is appropriate but this can be fitted using the usual same command as the Independence Poisson model as long as we include the term for the row effect, that is for Group.\n\n\nCode\ncount    = c(25, 8, 5, 6, 18, 11)\nantibody = c( 1, 2, 3, 1,  2, 3)\ngroup    = c( 1, 1, 1, 2,  2, 2)\n\nantibodyF = as.factor(antibody)\ngroupF = as.factor(group)\n\nglm.fit = glm(count ~ antibodyF + groupF, family=\"poisson\")\n\nsummary(glm.fit)\n\n\n\nCall:\nglm(formula = count ~ antibodyF + groupF, family = \"poisson\")\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.78111    0.21184  13.129   &lt;2e-16 ***\nantibodyF2  -0.17589    0.26593  -0.661   0.5083    \nantibodyF3  -0.66140    0.30783  -2.149   0.0317 *  \ngroupF2     -0.08224    0.23428  -0.351   0.7256    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 23.807  on 5  degrees of freedom\nResidual deviance: 18.643  on 2  degrees of freedom\nAIC: 51.771\n\nNumber of Fisher Scoring iterations: 5\n\n\nHence, our proposed model has a residual deviance of \\(18.643\\) on \\(2\\) degrees of freedom. The chi-square test gives a \\(p\\)-value of \\[p = \\mbox{Pr}(\\chi^2_2 &gt; 18.643) = 8.95\\times 10^{-5},\\] so we reject the independence model in favour of the saturated model.\nFor the independence model \\[\n\\hat\\pi_{ij} =\n{ e^{\\hat\\beta_j}}\\Big/ {\\sum_j e^{\\hat\\beta_j}}\n\\] whose right-hand side does not depend on \\(i\\) and hence is the same for all rows. The corresponding fitted values are then given by \\(\\hat y_{ij} = y_{i+} \\hat \\pi_{ij}\\)."
  },
  {
    "objectID": "7_extendedloglinearmodel.html#exercises",
    "href": "7_extendedloglinearmodel.html#exercises",
    "title": "7  Extensions to Loglinear models",
    "section": "7.5 Exercises",
    "text": "7.5 Exercises\n\n7.1 Consider a \\(2 \\times m\\) contingency table with entries \\(y_{ij}, \\ i=1,2,\\ j=1,\\ldots,m\\). The rows label STATUS represents alive (STATUS = 1) or dead (STATUS=2) and the columns label AGE represents \\(m\\) age groups \\((\\mbox{AGE} = 1,\\ldots,m\\)) and not the exact age.\nSuppose that a Poisson model \\(P(\\lambda_{ij})\\) with \\[\n\\log(\\lambda_{ij}) = \\delta + \\alpha_i+ \\beta_j + \\gamma_i \\, AGE_j\n\\] is considered appropriate. Note that \\(AGE_j=j\\) is treated as a quantitative variable in the last term. What additional constraints on the parameters should be included to make estimation unique – that is to remove the aliasing problem.\nSuppose that the column totals, \\(Y_{+j}\\), are fixed. Explain why a product binomial model \\(B(y_{+j},\\pi_j), \\ j=1,\\ldots,m\\), is suitable in this case and find the form of \\(\\pi_j.\\) Which parameters can be estimated under this model?\n7.2 In the product-multinomial model for a table with fixed row totals, model parameter estimates are given as the solution of Equation 7.2 and it was claimed that this had the same form as for the previous independence Poisson model (without fixed row totals). Use Equation 6.4 and Equation 6.6 to show that indeed the estimation equations are identical and hence that the parameter estimates will be the same."
  }
]