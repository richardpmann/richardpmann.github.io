[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH5824 Generalised Linear and Additive Models",
    "section": "",
    "text": "\\[\n\\def\\b#1{\\mathbf{#1}}\n\\]\n\n\nWeekly schedule\nItems will be added here week-by-week and so keep checking when you need up-to-date information on what you should be doing. Note that items specific to MATH5824M will be marked accordingly, otherwise items refer to the material common with MATH3823.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProvisional Weekly Lecture Schedule\n\n\n\n\n\n\nWeek 1\nChapter 1\nAll\n\n\nWeek 2\nChapter 2\nAll\n\n\nWeek 3\nChapter 3\nSections 3.1-3.3\n\n\nWeek 4\n\nSections 3.4-3.5\n\n\nWeek 5\nChapter 4\nSections 4.1-4.3\n\n\nWeek 6\n\nSections 4.4-4.5\n\n\nWeek 7\nChapter 5\nSections 5.1-5.3\n\n\nWeek 8\n\nSections 5.4-5.6\n\n\nEaster\n\n\n\n\nWeek 9\nChapter 6\nAll\n\n\nWeek 10\nExercises\nAll Exercises\n\n\nWeek 11\nRevision"
  },
  {
    "objectID": "0_preface.html",
    "href": "0_preface.html",
    "title": "Overview",
    "section": "",
    "text": "Official Module Description"
  },
  {
    "objectID": "0_preface.html#preface",
    "href": "0_preface.html#preface",
    "title": "Overview",
    "section": "Preface",
    "text": "Preface\nThese lecture notes are produced for the University of Leeds module “MATH5824 - Generalised Linear and Additive Models” for the academic year 2023-24. They are based on the lecture notes used previously for this module and I am grateful to the previous module leader Robert Aykroyd for sharing his notes and the considerable effort he made in developing these. A PDF version will be made available on the University of Leeds Minerva system. This will be my first year teaching this module, so I encourage you to offer feedback that can be used to improve the content throughout the semester.\nRP Mann, Leeds, January 3, 2025"
  },
  {
    "objectID": "0_preface.html#changes-since-last-year",
    "href": "0_preface.html#changes-since-last-year",
    "title": "Overview",
    "section": "Changes since last year",
    "text": "Changes since last year\nFeedback from the students last year was very positive, but there were consistent comments regarding two issues: (1) a shortage of practice exercises and the opportunity to discuss these in class, and (2) limited RStudio support in preparation for the assessment. For the first of these, additional exercises have been prepared and are included in the learning material. Also, I am trying some short quizzes so that you can check your basic knowledge. Further, I intend to set-aside some lecture time for us to discuss selected exercises. For the second, an additional computer session has been added, in Week 5 (26 February - 1 March), this is 3 weeks before the assessed practice in Week 8 (18 - 22 March). Further, a few new instructional videos will be available addressing some RStudio topics. Together, these represents a considerable about of extra work for me, but I hope that they are helpful and so please give your feedback whenever there is an opportunity."
  },
  {
    "objectID": "0_preface.html#generative-ai-usage-within-this-module",
    "href": "0_preface.html#generative-ai-usage-within-this-module",
    "title": "Overview",
    "section": "Generative AI usage within this module",
    "text": "Generative AI usage within this module\nThe assessments for this module fall in the red category for using Generative AI which means you must not use Generative AI tools. The purpose and format of the assessments makes it inappropriate or impractical for AI tools to be used.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStatistical ethics and sensitive data\nPlease note that from time to time we will be using data sets from situations which some might perceive as sensitive. All such data sets will, however, be derived from real-world studies which appear in textbooks or in scientific journals. The daily work of many statisticians involves applying their professional skills in a wide variety of situations and as such it is important to include a range of commonly encountered examples in this module. Whenever possible, sensitive topics will be signposted in advance. If you feel that any examples may be personally upsetting then, if possible, please contact the module lecturer in advance. If you are significantly effected by any of these situations, then you can seek support from the Student Counselling and Wellbeing service."
  },
  {
    "objectID": "0_preface.html#module-summary",
    "href": "0_preface.html#module-summary",
    "title": "Overview",
    "section": "Module summary",
    "text": "Module summary\nLinear regression is a tremendously useful statistical technique but is limited to normally distributed responses. Generalised linear models extend linear regression in many ways - allowing us to analyse more complex data sets. In this module we will see how to combine continuous and categorical predictors, analyse binomial response data and model count data.A further extension is the generalised additive model. Here, we no longer insist on the predictor variables affecting the response via a linear function of the predictors, but allow the response to depend on a more general smooth function of the predictor."
  },
  {
    "objectID": "0_preface.html#objectives",
    "href": "0_preface.html#objectives",
    "title": "Overview",
    "section": "Objectives",
    "text": "Objectives\nOn completion of this module, students should be able to:\n\ncarry out regression analysis with generalised linear models including the use of link functions, deviance and overdispersion;\nfit and interpret the special cases of log linear models and logistic regression;\ncompare a number of methods for scatterpot smoothing suitable for use in a generalised additive model;\nuse a backfitting algorithm to estimate the parameters of a generalised additive model;\ninterpret a fitted generalised additive model;\nuse a statistical package with real data to fit these models to data and to write a report giving and interpreting the results."
  },
  {
    "objectID": "0_preface.html#syllabus",
    "href": "0_preface.html#syllabus",
    "title": "Overview",
    "section": "Syllabus",
    "text": "Syllabus\nGeneralised linear model; probit model; logistic regression; log linear models; scatterplot smoothers; generalised additive model."
  },
  {
    "objectID": "0_preface.html#university-module-catalogue",
    "href": "0_preface.html#university-module-catalogue",
    "title": "Overview",
    "section": "University Module Catalogue",
    "text": "University Module Catalogue\nFor any further details, please see MATH5824 Module Catalogue page"
  },
  {
    "objectID": "1_introduction.html#introduction",
    "href": "1_introduction.html#introduction",
    "title": "1  Non-parametric Modelling",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nIn the Level 3 component of this module, we extend the simple linear regression model to the generalised linear model which can cope with non-normally distributed response variables, in particular data following binomial and Poisson distributions. However, we still just use linear functions of the predictor variables. A further extension of the linear model is the generalised additive model. Here, we no longer insist on the predictor variables affecting the response via a linear function of the predictors, but allow the response to depend on a more general smooth function of the predictor. In the Level 5 component of this module, we study splines and their use in interpolating and smoothing the effects of explanatory variables in the generalised linear models of the Level 3 component of this module (see separate Lecture Notes accompanying MATH3823). Towards the end of the material, we will learn that the fitting of generalised additive models is a straightforward extension of what is learnt in MATH3823.\nOutline of the additional material in MATH5824 compared to MATH3823:\n\nInterpolating and smoothing splines.\nCross-validation and fitting splines to data.\nThe generalised additive model."
  },
  {
    "objectID": "1_introduction.html#motivation",
    "href": "1_introduction.html#motivation",
    "title": "1  Non-parametric Modelling",
    "section": "1.2 Motivation",
    "text": "1.2 Motivation\nTable 1.1 reports on the depth of a coal seam determined by drilling bore holes at regular intervals along a line. The depth \\(y\\) at location \\(x=6\\) is missing: could we estimate it?\n\n\nTable 1.1: Coal-seam depths (in metres) below the land surface at intervals of 1 km along a linear transect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation, \\(x\\)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nDepth, \\(y\\)\n-90\n-95\n-140\n-120\n-100\n-75\nNA\n-130\n-110\n-105\n-50\n\n\n\n\nFigure 1.1 plots these data, superimposed with predictions from several polynomial regression models.\n\n\n\n\n\n\n\n(a) Constant model\n\n\n\n\n\n\n\n(b) Linear model\n\n\n\n\n\n\n\n\n\n(c) Quadratic model\n\n\n\n\n\n\n\n(d) Cubic model\n\n\n\n\nFigure 1.1: The coal-seam data superimposed with predictions from polynomial regression models.\n\n\nEach of these models would predict a different value for the missing observation \\(y_6.\\) We do not know the accuracy of the depth measurements, so in principle any of these curves could be correct. Clearly, the residual variance is largest for the constant-depth model in Figure 1.1 (a), and smallest for the cubic polynomial in Figure 1.1 (c) – the residual sums of squares are: 6252.5, 5773.05, 4489.84, 4242.89. However, none of these models produces a convincingly good fit. Moreover, these models are not particularly believable, since we know that geological pressures exerted over very long periods of time cause the landscape and its underlying layers of rock to undulate and fracture. This suggests we need a different strategy.\nNext, consider the simulated example in Figure 1.2. At first look we might be happy with the fitted curves in Figure 1.2 (a) or Figure 1.2 (b). The data, however, are created with a change-point at \\(x=0.67\\) where the relationship changes from linear with slope \\(0.6\\) to a constant value of \\(0.75\\). This description is completely lost with these two models.\n\n\n\n\n\n\n\n(a) Linear model\n\n\n\n\n\n\n\n(b) Quadratic model\n\n\n\n\n\n\n\n\n\n(c) Piecewise linear model\n\n\n\n\n\n\n\n(d) Cubic smoothing spline\n\n\n\n\nFigure 1.2: Simulated data superimposed with predictions from various models.\n\n\nFigure 1.2 (c) shows the result of fitting one linear function to the data below \\(0.67\\) and a second linear function above. Clearly, this fits well but it has assumed that the change-point location is known – which is unrealistic. Finally, Figure 1.2 (d) shows a fitted cubic smoothing spline to the data – we will studies these models later. This shows an excellent fit and leads to appropriate conclusions. That is, the relationship is approximately linear for small values, then there is a rapid increase, and finally a near constant value for high values. Of course, this is not exactly as the true relationship with a discontinuity at \\(x=0.67\\) but it would definitely suggest something extreme occurs between about \\(0.6\\) to \\(0.7\\). Full details will follow later, but the cubic spline fits local cubic polynomials which are constrained to create a continuous curve.\nNow returning to the coal seam data. Figure 1.3 shows the data again, superimposed with predictions from methods which are not constrained to produce such smooth curves.\n\n\n\n\n\n\n\n(a) Constant interpolating spline\n\n\n\n\n\n\n\n(b) Linear interpolating spline\n\n\n\n\n\n\n\n\n\n(c) Cubic interpolating spline\n\n\n\n\n\n\n\n(d) Cubic smoothing spline\n\n\n\n\nFigure 1.3: The coal-seam data superimposed with predictions from various spline models.\n\n\nThe simplest method, constant-spline interpolation, assumes that the dependent variable remains constant between successive observations, with the result shown in Figure 1.3 (a). However, the discontinuities in this model make it quite unreliable. A better method, whose results are shown in Figure 1.3 (b), is linear-spline interpolation, which fits a straight line between successive observations. Even so, this method produces discontinuities in the gradient at each data point. A better method still, shown in Figure 1.3 (c), is cubic spline interpolation, which fits a cubic polynomial between successive data points such that both the gradient and the curvature at each data point is continuous.\nA feature of all these interpolation methods is that they fit the data exactly. Is this a good thing? The final method assumes that there may be some measurement error in the observations, which justifies fitting a smoother cubic spline than the cubic interpolating spline, but as we see in Figure 1.3 (d) which does not reproduce the data points exactly. Is this a bad thing? We will see during this module how to construct and evaluate these curves. Here, the results are presented only for motivation."
  },
  {
    "objectID": "1_introduction.html#focus-on-polynomials-quiz",
    "href": "1_introduction.html#focus-on-polynomials-quiz",
    "title": "1  Non-parametric Modelling",
    "section": "Focus on polynomials quiz",
    "text": "Focus on polynomials quiz\nTest your knowledge recall and comprehension to reinforce basic ideas about continuity and differentiability.\n\n\n\nWhich of the predicted curves in Figure 1.3 are continous? None of the modelsModel (a) onlyModels (c) and (d) onlyAll models\nWhich of the predicted curves in Figure 1.3 have a continous first derivative? None of the modelsModel (b) onlyModels (c) and (d) onlyAll models\nWhich of the predicted curves in Figure 1.3 have a continous second derivative? None of the modelsModel (b) onlyModels (c) and (d) onlyAll models\nWhich of the predicted curves in Figure 1.3 have a continous third derivative? None of the modelsModel (b) onlyModels (c) and (d) onlyAll models\nWhich of the predicted curves in Figure 1.3 has the highest residual sum of squares? None of the modelsModel (a)Model (b)Model (c)Model (d)All model"
  },
  {
    "objectID": "1_introduction.html#sec-genmodelling",
    "href": "1_introduction.html#sec-genmodelling",
    "title": "1  Non-parametric Modelling",
    "section": "1.3 General modelling approaches",
    "text": "1.3 General modelling approaches\nWe wish to model the dependence of a response variable \\(y\\) on an explanatory variable \\(x\\), where \\(y\\) and \\(x\\) are both continuous. We observe \\(y_i\\) at each time \\(x_i,\\) for \\(i=1,\\ldots, n\\), where the observation locations are ordered: \\(x_1 &lt; x_2 &lt; \\ldots &lt;x_n\\). We imagine that the \\(y\\)’s are noisy versions of a smooth function of \\(x\\), say \\(f(x)\\). That is, \\[\ny_i = f(x_i) + \\epsilon_i,\n\\tag{1.1}\\] where the \\(\\{\\epsilon_i\\}\\) are i.i.d: \\[\n\\epsilon_i \\sim \\mathrm{N}(0,\\sigma^2).\n\\tag{1.2}\\] We suppose we do not know the correct form of function \\(f\\): how can we estimate it?\nIt is useful to divide modelling approaches into two broad types: parametric and non-parametric.\n\nParametric models\nBy far the most common parametric model is simple linear regression, for example, \\(f(x) = \\alpha + \\beta x\\), where parameters \\(\\alpha\\) and \\(\\beta\\) are to be estimated. This is, of course, the simplest example of the polynomial model family, \\(f(x) = \\alpha + \\beta x + \\gamma x^2 +\\cdots + \\omega \\; x^p\\), where \\(p\\) is the order of the polynomial and where all of \\(\\alpha, \\beta, \\gamma, \\dots, \\omega\\) are to be estimated. This has as special cases: quadratic, cubic, quartic, and quintic polynomials models. Also common are exponential models, for example \\(f(x) = \\alpha e^{-\\beta x}\\), where \\(\\alpha,\\beta\\) are to be estimated – do not confuse this with the exponential probability density function.\nNote that the polynomial models are all linear functions of the parameters. They are standard forms in regression modelling, as studied in MATH3714 (Linear regression and Robustness) and MATH3823 (Generalised linear models). The exponential model, however, is an example of a model which is non-linearly in the parameters – it is an example of a non-linear regression model.\nAlthough very many parametric models exist, they are all somewhat inflexible in their description of \\(f\\). They cannot accommodate arbitrary fluctuations in \\(f(x)\\) over \\(x\\) because they contain only a small number of parameters (degrees-of-freedom).\n\n\nNon-parametric models\nIn such models, \\(f\\) is assumed to be a smooth function of \\(x\\), but otherwise we do not know what \\(f\\) looks like. A smooth function \\(f\\) is such that \\(f(x_i)\\) is close to \\(f(x_j)\\) whenever \\(x_i\\) is close to \\(x_j\\). To characterise and fit \\(f\\) we will use an approach based on splines. In practice, different approaches to characterizing and fitting smooth \\(f\\) lead to similar fits to the data. The spline approach fits neatly with normal and generalised linear models (NLMs and GLMs), but so do other approaches (for example, kernel smoothing and wavelets). Methods of fitting \\(f\\) based on kernel smoothing and the Nadaraya–Watson estimator are studied in the Level 5 component of MATH5714 (Linear regression, robustness and smoothing) where the choice of bandwidth in kernel methods is analogous to the choice of smoothing parameter value in spline smoothing.\n\n\nPiecewise polynomial models\nA common problem with low-order polynomials is that they can often fit well for part of the data but have unappealing features elsewhere. For example, although none of the models in Figure 1.1 fit the data at all well, we might imagine that three short linear segments might be a good fit to the coal-seam data. Also, the piecewise linear model was a good description of the data in Figure 1.2 (c). This suggests that local polynomial models might be useful. In some situation, for example when we know that the function \\(f\\) is continuous, jumps in the fitted model, as in Figure 1.2 (c), are unacceptable. Alternatively, we may require differentiability of \\(f\\). Such technical issues lead to the use of splines, which is introduced in the next chapter."
  },
  {
    "objectID": "1_introduction.html#sec-exercises1",
    "href": "1_introduction.html#sec-exercises1",
    "title": "1  Non-parametric Modelling",
    "section": "1.4 Exercises",
    "text": "1.4 Exercises\n\n1.1 Consider the first three models fitted in Figure 1.1 and let the data be denoted, \\(\\{(x_i, y_i): i=1,2,\\dots, n\\}\\). These three models can be written \\[\\begin{align*}\n(a) \\quad &  y = \\alpha +\\epsilon\\\\\n(b) \\quad &  y = \\alpha +\\beta x+\\epsilon\\\\\n(c) \\quad &  y = \\alpha +\\beta x + \\gamma x^2 +\\epsilon\n\\end{align*}\\] where \\(\\epsilon\\) represents normally distributed random error. Use the principle of least squares, or otherwise, to obtain estimates of the model parameters.\n\n\nClick here to see hints.\n\nFor each, start by defining the residual sum of squares (RSS), \\(RSS=\\sum (y_i - \\hat{y}_i)^2\\) where, in turn (a) \\(\\hat{y}_i= \\alpha\\), (b) \\(\\hat{y}_i= \\alpha+\\beta x_i\\), (c) \\(\\hat{y}_i= \\alpha+\\beta x_i+\\gamma x_i^2\\). Then, find he parameter values which minimise the RSS by (possibly partial) differentiation.\n\n1.2 Discuss possible approaches to fitting an exponential model, \\(y=\\alpha e^{\\beta x},\\) to data. Note that no actual algebraic derivation, nor numerical coded algorithm is expect.\n\n\nClick here to see hints.\n\nThere is more than one approach. Can least squares be used? Could a simple transformation of the data and the fitted model make solving the problem easier? Is there an algebraic solution? Is there a purely numerical solution?\n\n1.3 In Figure 1.2 (c), discuss how you might fit a two-part linear model for the case where the change-point is unknown. Note that no actual algebraic derivation, nor numerical coded algorithm is expect.\n\n\nClick here to see hints.\n\nWith many similar problems, imaging breaking the problem down into steps. If you know the location of the change-point then what should you do? Can you then try different possible change-point locations?\n\n1.4 Discuss the four fitted models in Figure 1.3. Can you give positive and negative properties of each model? Which do you think is best and which worst? Do you think which is best/worst, depends on the data? Justify your answers.\n\n\nClick here to see hints.\n\nDon’t get too stuck on the data used here, but think of general issues: ease of use, reliability of the data, is there error with the data or is it very reliable? What if the response it discrete?\n\n\n\n\n\n\n\n\nNote\n\n\n\nExercise 1 Solutions can be found here."
  },
  {
    "objectID": "2_basicdefinitions.html#basic-definitions",
    "href": "2_basicdefinitions.html#basic-definitions",
    "title": "2  Introducing Splines",
    "section": "2.1 Basic definitions",
    "text": "2.1 Basic definitions\nLet \\(t_1 &lt; t_2 &lt; \\ldots &lt; t_m\\) be a fixed set of sites or knots which need not correspond to observation locations, as in Figure 2.1.\n\n\n\n\n\nFigure 2.1: Diagram of knots and data points.\n\n\n\n\nNote that we use the symbol \\(t\\), rather than \\(x\\), so that we do not confuse knots and observation locations.\nA spline of order \\(p\\geq 1\\) is a piecewise-polynomial of order \\(p\\) which is \\((p-1)\\) times differentiable at the knots. Thus there are coefficients \\(\\{a_{k\\ell},\\; k=0,\\ldots,m, \\; \\ell=0,\\ldots,p\\}\\) such that \\[\nf(t) = \\sum_{\\ell=0}^p a_{k\\ell} \\; t^\\ell,\\qquad \\text{for}~t_k \\leq t &lt;t_{k+1},\n\\tag{2.1}\\] where we take \\(t_0 = -\\infty\\) and \\(t_{m+1} = +\\infty\\).\nIf we are using cubic polynomials, (\\(p=3\\)), then \\(f\\) is given by the following equations: \\[\nf(t) = a_{00} + a_{01} t + a_{02}t^2 +a_{03}t^3, \\quad\nt_0 \\le t &lt; t_1\n\\] to the left of the first knot, \\[\nf(t) = a_{10} + a_{11} t + a_{12}t^2 +a_{13}t^3, \\quad\nt_1 \\le t &lt; t_2\n\\] between the first and second knots, and so on until \\[\nf(t) = a_{m0} + a_{m1} t + a_{m2}t^2 +a_{m3}t^3, \\quad\nt_m \\le t &lt; t_{m+1}\n\\] to the right of the final knot. This is illustrated in Figure 2.2 (a) with \\(m=2.\\)\n\n\n\n\n\n\n\n(a) No smoothness constraints\n\n\n\n\n\n\n\n(b) With smoothness constraints\n\n\n\n\nFigure 2.2: Piecewise-cubic functions in three intervals with knot positions indicated with vertical lines."
  },
  {
    "objectID": "2_basicdefinitions.html#focus-on-splines-quiz",
    "href": "2_basicdefinitions.html#focus-on-splines-quiz",
    "title": "2  Introducing Splines",
    "section": "Focus on splines quiz",
    "text": "Focus on splines quiz\nTest your knowledge recall and comprehension to reinforce basic ideas about splines.\n\n\n\nWhich of the following best describes the relationship between knots and data locations.\n\n Knots are the response and data locations are the explanatory variable There must be fewer knots than data locations Knots and data locations are both marked on the x-axis Knots and data points are exactly the same\n\nHow many polynomial equations are need to define a cubic spline with four knots? 34512None of the above\nWhat is the highest power possible in a cubic spline? 0123None of the above\nHow many times can a spline of order 5 be differentiated at the knots? Cannot be differentiatedTwice differentiableFour times differentiableInfinitly differentiable\nWhich of the following is NOT a property of cubic splines? Piecewise cubic polynomialTwice differentiableMay contain change-pointsContinuous at the knots"
  },
  {
    "objectID": "2_basicdefinitions.html#imposing-smoothness",
    "href": "2_basicdefinitions.html#imposing-smoothness",
    "title": "2  Introducing Splines",
    "section": "2.2 Imposing smoothness",
    "text": "2.2 Imposing smoothness\nBecause of the use of polynomials, \\(f\\) is smooth between each successive pair of knots. At the knots, however, \\(f\\) might not be continuous and it might not be differentiable – in such cases we would say that the function is not smooth.\nTo ensure that \\(f\\) is also smooth at each of the knots, we impose smoothness constraints which control continuity of the function and its derivatives at the knots.\nLet \\(f^{(\\ell)}\\) be the \\(\\ell\\)-th order derivative, with \\(f^{(0)}=f\\) being the function itself, \\(f^{(1)}=f'\\) is the first derivative and \\(f^{(2)}=f''\\) the second derivative. Further, let \\(f^{(\\ell)}(t-\\epsilon)\\) and \\(f^{(\\ell)}(t+\\epsilon)\\), for \\(\\epsilon\\ge 0\\), denote evaluation of the function or its derivative at points just below and just above \\(t\\) – we will be interested in their relative values as \\(\\epsilon \\rightarrow 0\\).\nTo impose smoothness, we require that\n\\[\n\\lim _{\\epsilon\\rightarrow 0} f^{(\\ell)}(t_k-\\epsilon)\n=\n\\lim _{\\epsilon\\rightarrow 0} f^{(\\ell)}(t_k+\\epsilon),\n\\tag{2.2}\\] for all \\(k=1,\\ldots,m\\) and for \\(\\ell = 0,\\ldots,(p-1)\\).\nIn other words we say that \\(f\\) is smooth if the limits, from below and from above, of the function and its \\((p-1)\\) derivatives exist and are equal.\nThe meaning of these smoothness constraints is illustrated in Figure 2.2. In Figure 2.2 (a), a piecewise cubic function with two knots has been plotted. The first derivative, \\(f'\\), is discontinuous at the first knot and the function itself, \\(f\\), is discontinuous at the second knot. Figure 2.2 (b) shows a similar shaped cubic spline with two knots. This time, the function \\(f\\) and its first two derivatives are continuous at both knots.\nThe smoothness conditions in Equation 2.2 induce constraints on the coefficients \\(\\{a_{k\\ell}\\}\\). A polynomial of order \\(p\\) has \\(p+1\\) coefficients, and there are \\(m+1\\) intervals when we have \\(m\\) knots. This leads to \\((p+1)\\times(m+1)\\) coefficients but there are \\(p\\) constraints at each of the \\(m\\) knots. Thus the total degrees of freedom of the system is \\[\n\\text{df}_{\\text{spline}}   = (p+1)(m+1) - pm = m+p+1.\n\\tag{2.3}\\]\nThese degrees of freedom provide the necessary flexibility in the spline.\nNote that \\(f\\) is infinitely differentiable everywhere, except at the knots where it is \\(p-1\\) times differentiable. In particular, for \\(p=1\\), \\(f\\) is a linear spline comprising linear pieces constrained to be continuous at the knots, although the slope of \\(f\\) is discontinuous at the knots. Also, for \\(p=3\\), \\(f\\) is a cubic spline comprising cubic polynomial pieces continuous at the knots; where the first and second derivatives of \\(f\\) are also continuous, but the third derivative is discontinuous at the knots."
  },
  {
    "objectID": "2_basicdefinitions.html#focus-on-smoothness-quiz",
    "href": "2_basicdefinitions.html#focus-on-smoothness-quiz",
    "title": "2  Introducing Splines",
    "section": "Focus on smoothness quiz",
    "text": "Focus on smoothness quiz\nTest your knowledge recall and comprehension to reinforce basic ideas about smoothness.\n\n\n\nWhich of the following best describes the motivation for using smooth fitted models? Makes model fitting easierCalculations are easy to do in RIt produces nice graphsReduces the effect of measurement errorNone of the above\nWhich of the following best describes the motivation for using piecewise polynomial components? Can involve change-pointsWell understood and easy to useCan model jumps wellThey lead to normally distributed errorsNone of the above\nIs the following a true statement? ‘The higher the order of the polynomial components the smoother the spline’ TRUEFALSE\nIf a model fitted to a particular data set has zero degrees of freedom, then which of the following statements about the solution is most likely to be true? Does not fit the data wellNo solutionUnique solutionMultiple solutions\nWhich of the following best describes spline modeling? The only non-parametric method availableIt is a parametric approachRequires high-level codingA flexible non-parametric approach"
  },
  {
    "objectID": "2_basicdefinitions.html#exercises",
    "href": "2_basicdefinitions.html#exercises",
    "title": "2  Introducing Splines",
    "section": "2.3 Exercises",
    "text": "2.3 Exercises\n\n2.1 Why is it not sensible to define a smooth function made-up of constant components? Similarly, why is not sensible to create a differentiable function from linear splines?\n\n\nClick here to see hints.\n\nFor each case, think about the number of parameters for each component and the implications of any constraints.\n\n2.2 In the situation illustrated in Figure 2.2 (b), where \\(p=3\\) and \\(m=2\\), clearly identify the \\((p+1)\\times (m+1)=12\\) model parameters and the \\(pm=6\\) smoothness constraints in terms of the cubic polynomials and their derivatives.\n\n\nClick here to see hints.\n\nDefine a cubic polynomial for each interval and consider continuity and differentiability.\n\n2.3 Further consider the situation illustrated in Figure 2.2 (b). Suppose now that we require the splines to pass through specified coordinates \\((t_1, f(t_1))\\) and \\((t_2, f(t_2))\\). What is the degrees of freedom for this model? How many such cubic splines would satisfy these constraints? Discuss potential additional constraints which would lead to a unique fitted model. Do you think having a unique solution is a positive or negative property?\n\n\nClick here to see hints.\n\nThink about the degrees of freedom, that is the total number of parameters and the number of constraints, including forcing the spine to pass through two points. Think about the implications of having zero and non-zero degrees of freedom. There are very many (infinitely many?) potential additional constraints, but suggest one or two which sound a good idea.\n\n2.4 For a general problem, what would be the effect of requiring additional constraints of the form of Equation 2.2 but with \\(\\ell = p\\)? Would this lead to an acceptable fitted cubic spline model? Justify your answer.\n\n\nClick here to see hints.\n\nThink about the implication of this on the curvature of neighbouring components, and hence on overall curvature.\n\n\n\n\n\n\n\n\nNote\n\n\n\nExercise 2 Solutions can be found here."
  },
  {
    "objectID": "3_interpolating.html#overview",
    "href": "3_interpolating.html#overview",
    "title": "3  Interpolating Splines",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nChapter 1 considered general limitations of parametric models, and polynomial regression in particular (see Figure 1.1), which motivated the use of the more flexible spline models (see Figure 1.3) – though at that stage no mathematical details were presented. In Chapter 2, basic spline definitions were given, including the notation of smoothness constraints, and these ideas were further explored in the Exercises in Section 3.6. This chapter will now give mathematical details of the interpolating spline problem and consider application to data. A feature of all these interpolation methods is that they fit the data exactly and that the fitted functions are smooth. Figure 3.1, is cubic spline interpolation, which fits a cubic polynomial between successive data points such that the function, gradient and the curvature are all continuous at each data point. The solid line shows the fitted values within the range of the data, whereas the dashed line shows the fitted values outside the range of the data – extrapolation.\n\n\nCode\npar(mar=c(3,3,1,1), mgp=c(2,1,0))\n\nlocation=0:10\ndepth=c(-90,-95,-140,-120,-100,-75,NA,-130,-110,-105,-50)\n\nplot(location, depth, \n     xlim=c(-0.5,10.5), ylim=c(-150,0), pch=4)\n\nmysplinefit = splinefun(location, depth, method=\"natural\")\ncurve(mysplinefit,-0.75,10.5, add=T, lty=2)\ncurve(mysplinefit,   0,   10, add=T)\n\n\n\n\n\nFigure 3.1: A cubic interpolating spline fitted to the coal-seam data, with the dashed line showing extrapolation."
  },
  {
    "objectID": "3_interpolating.html#natural-splines",
    "href": "3_interpolating.html#natural-splines",
    "title": "3  Interpolating Splines",
    "section": "3.2 Natural splines",
    "text": "3.2 Natural splines\nSuppose we have \\(n\\) observations \\(\\{y_1,\\dots,y_m\\}\\) at locations \\(\\{t_1,\\dots,x_m\\}.\\) We can construct a cubic spline (that is with \\(p=3\\)) to pass through (interpolate) all the points \\((t_i,y_i),\\ i=1,\\dots,m\\). In fact, for any given set of points, there is an infinite number of cubic splines which interpolate them, see Figure 3.2 for examples. Exactly one of these splines has the property that, in the leftmost and rightmost intervals, it is a straight line. Such a spline is called a natural cubic spline.\n\n\n\n\n\nFigure 3.2: Cubic interpolating splines fitted to the coal-seam data, with the dashed lines showing extrapolation – the natural spline is shown in solid black.\n\n\n\n\nNote that natural splines are not the only choice for the spline method – see the R help page for other options – with perhaps the most useful other being \\(\\texttt{periodic}\\) which can be considered if we expect the unknown function to also be periodic. Figure 3.3 shows the fitted natural and periodic spline fitted to a simulated cosine function. The only noticeable difference is outside the range of the data, that is for extrapolation. Great care should be used, however, as imposing such additional restrictions on the fitted function can lead to unforeseen modelling errors when we do not observe the full range of \\(\\texttt{x}\\) values.\n\n\nCode\nset.seed(15340) # for reproducibility\n\npar(mar=c(3,3,1,1), mgp=c(2,1,0))\n\n# Generate some artificial data \nn = 12\nx = seq(0, 2*pi, length.out=n); y = cos(x+1)+rnorm(n,0,0.1)\ny = c(y[-n],y[1]) # make sure first/last point equal\n\n# Fit splines but with different \"methods\"\n# ... using natural splines\nplot(x, y, xlim=c(-pi/2,7*pi/3), ylim=c(-3,3), pch=16)\n\nmysplinefit2 = splinefun(x, y, method=\"natural\")\ncurve(mysplinefit2,     0,   2*pi, add=T)\ncurve(mysplinefit2, -pi/2, 7*pi/3, add=T, lty=2)\n\n\n# ... using periodic splines\nplot(x, y, xlim=c(-pi/2,7*pi/3), ylim=c(-3,3), pch=16)\n\nmysplinefit2 = splinefun(x, y, method=\"periodic\")\ncurve(mysplinefit2, 0, 2*pi, add=T)\ncurve(mysplinefit2, -pi/2, 7*pi/3, add=T, lty=2)\n\n\n\n\n\n\n\n\n(a) Using method=“natural”\n\n\n\n\n\n\n\n(b) Using method=“periodic”\n\n\n\n\nFigure 3.3: Example of different interpolating splines."
  },
  {
    "objectID": "3_interpolating.html#properties-of-natural-splines",
    "href": "3_interpolating.html#properties-of-natural-splines",
    "title": "3  Interpolating Splines",
    "section": "3.3 Properties of natural splines",
    "text": "3.3 Properties of natural splines\nNatural splines are a special case of polynomial splines of odd order \\(p\\). Thus we have natural linear splines (\\(p=1\\)), natural cubic splines (\\(p=3\\)), etc. A spline is said to be natural if, beyond the boundary knots \\(t_1\\) and \\(t_{m}\\), its \\((p+1)/2\\) higher-order derivatives are zero: \\[\nf^{(j)}(t) = 0,  \n\\tag{3.1}\\] for \\(j={(p+1)}/{2},\\ldots,p\\) and either \\(t \\leq t_1\\) or \\(t \\geq t_m\\).\nThus a natural spline of order \\(p\\) has the following \\(p+1\\) constraints, in addition to those of Equation 2.2 : \\[\n\\lim _{\\epsilon\\rightarrow 0} f^{(\\ell)}(t_1-\\epsilon)\n=\n\\lim _{\\epsilon\\rightarrow 0} f^{(\\ell)}(t_m+\\epsilon) =0,\n\\tag{3.2}\\] for \\(\\ell={(p+1)}/{2},\\ldots,p\\).\nIn particular,\n\na natural linear spline has \\(p+1=2\\) additional constraints: \\[\n\\lim _{\\epsilon\\rightarrow 0} f^{(1)}(t_1-\\epsilon)\n=\n\\lim _{\\epsilon\\rightarrow 0} f^{(1)}(t_m+\\epsilon) =0,\n\\tag{3.3}\\] implying that \\(f(t)\\) is constant in the outer intervals of a natural linear spline,\na natural cubic spline has \\(p+1=4\\) additional constraints: \\[\n\\lim _{\\epsilon\\rightarrow 0} f^{(2)}(t_1-\\epsilon) = \\lim _{\\epsilon\\rightarrow 0} f^{(2)}(t_m+\\epsilon) =0,\n\\] \\[\n\\lim _{\\epsilon\\rightarrow 0} f^{(3)}(t_1-\\epsilon) = \\lim _{\\epsilon\\rightarrow 0} f^{(3)}(t_m+\\epsilon) =0,\n\\tag{3.4}\\] implying that \\(f(t)\\) is linear in the outer intervals of a natural cubic spline.\n\nThe total degrees of freedom of a natural spline is, starting from Equation 2.3, but taking into account the additional \\(p+1\\) additional constraints is \\[\n\\text{df}_{\\text{nat.spline}} = m+p+1 - (p+1) = m.\n\\tag{3.5}\\] That is the degrees of freedom for natural splines equals \\(m\\) whatever the value of \\(p\\).\n\nProposition 3.1 Linear and cubic natural splines have the following representations:\n\nLinear natural splines: \\[\nf(t) = a_0 + \\sum_{i=1}^m b_i \\left| t - t_i \\right|;  \\quad  \\sum_{i=1}^m b_i = 0\n\\tag{3.6}\\]\nCubic natural splines: \\[\nf(t) = a_0 + a_1 t + \\sum_{i=1}^m b_i \\left| t - t_i \\right|^3;  \\quad  \\sum_{i=1}^m b_i = \\sum_{i=1}^m b_i t_i = 0.\n\\tag{3.7}\\]\n\n\nProof: Not covered here (but may be included later in the module if time allows)."
  },
  {
    "objectID": "3_interpolating.html#focus-on-natural-splines-quiz",
    "href": "3_interpolating.html#focus-on-natural-splines-quiz",
    "title": "3  Interpolating Splines",
    "section": "Focus on natural splines quiz",
    "text": "Focus on natural splines quiz\nTest your knowledge recall and comprehension to reinforce basic ideas about natural splines.\n\n\n\nWhich of the following is a key property of every spline? Has jump discontinuitiesIs a smooth non-parametric functionIs an example of a parametric modelAlways interpolate dataAlways extrapolate data\nWhich of the following is NOT a key property of a cubic spline function? Is always continuousIs always infinitely differentiableIs a smooth functionIs a piecewise polynomialHas extra contraints at knots\nWhich of the following statements is true ? Natual splines are a special case of polynomial splinesPolynomial splines are a special case of natural splinesPolynomial splines are identical to natural splinesNatural splines have fewer constraints than polynomial splinesNatural splines are smoother than polynomial splines\nWhich of the following statements is a true ? Natural splines have fewer constraints in the outer intervalsNatural splines have additional constraint in the internal intervalsNatural splines have additional constraints in the outer intervalsNatural splines are not smooth functionsNatural splines have fewer constraint in the internal intervals\n\n5 What determines the degrees of freedom of a natural spline? Both the number of knots and the polynomial orderThe sample sizeThe number of knotsBoth the number of knots and the sample sizeThe order of the polynomial used"
  },
  {
    "objectID": "3_interpolating.html#roughness-penalties",
    "href": "3_interpolating.html#roughness-penalties",
    "title": "3  Interpolating Splines",
    "section": "3.4 Roughness penalties",
    "text": "3.4 Roughness penalties\nAn aim of spline models is to describe an unknown function using piecewise-polynomials which are smooth. In the previous section, smoothness was imposed by explicitly constraining specified high-order derivatives. An alternative approach is to measure and control the degree of smoothness of the splines. In practice the roughness of the spline is usually measured and one definition of roughness is: \\[\nJ_\\nu(f) = \\int_{-\\infty}^\\infty \\left[ f^{(\\nu)}(t) \\right]^2 \\text{d}t\n\\tag{3.8}\\] where \\(\\nu\\geq 1\\) is an integer and \\(f^{(\\nu)}\\) denotes the \\(\\nu\\)th derivative of \\(f\\). Thus \\(f^{(1)}(t)\\) denotes the first derivative and \\(f^{(2)}(t)\\) denotes the second derivative of \\(f\\).\nIntuitively, roughness measures the “wiggliness” of a function.\nAim might be to find the smoothest function which interpolates the data points. Hence, an alternative approach to that in previous sections is to find the function \\(f\\) which minimizes Equation 3.8 and satisfies \\(f(t_i)=y_i\\) for \\(i=1,\\dots,m\\). We refer to the solutions of this problem as the optimal interpolating function.\nIt turns out that there is a very close link between \\(J_\\nu(\\cdot)\\) and \\(p\\)th-order natural splines, where \\(p=2\\nu-1\\) (so \\(p\\) is odd). Important special cases are: \\(\\nu=1\\) and \\(p=1\\), and \\(\\nu=2\\) and \\(p=3\\). This relationship is defined in the following proposition.\n\nProposition 3.2 The optimal interpolating function is a \\(p\\)th-order natural spline, where \\(p = 2ν − 1\\). That is, the natural spline \\(f\\) is the unique minimizer of \\(J_\\nu(f)\\).\n\nProof: Not covered here (but may be included later in the module if time allows).\nComments\n\nLinear and cubic interpolating splines are also of interest in numerical analysis, for example to interpolate tables of numbers.\nThe linear interpolating spline is simply the piecewise-linear path connecting the data points.\nOf course, in the linear spline case, knot points are clearly visible as kinks in the interpolating function. But, in the cubic spline case, knots points are invisible to the naked eye. Hence, in general, there is little motivation to use higher-order splines.\nNumerical considerations: the interpolating spline solutions involve matrix inversion. The inversion of an \\(n \\times n\\) matrix involves \\(O(n^3)\\) operations – hence it is time consuming if \\(n\\) is large (for example, \\(n=1000\\) or \\(10000\\)). Fortunately there are tricks to reduce the computation to \\(O(n)\\)."
  },
  {
    "objectID": "3_interpolating.html#fitting-interpolating-splines-in-r",
    "href": "3_interpolating.html#fitting-interpolating-splines-in-r",
    "title": "3  Interpolating Splines",
    "section": "3.5 Fitting interpolating splines in R",
    "text": "3.5 Fitting interpolating splines in R\nThere are two main function within \\(\\mathbf{R}\\) for fitting interpolating splines to data, \\(\\texttt{spline}\\) which outputs fitted values for specified points or \\(\\texttt{splinefun}\\) which returns an \\(\\mathbf{R}\\) function which can be used directly by other commands, such as \\(\\texttt{curve}\\). The following illustrates the two approaches.\n\n\nCode\nset.seed(15342)\n\npar(mar=c(3,3,1,1), mgp=c(2,1,0))\n\nx = 1:9; y = rnorm(9)\n\n# spline deals with points\nplot(x, y, xlim=c(0,10), ylim=c(-3,3), pch=4)\n\nmysplinefit1 = spline(x, y, method=\"natural\")\nlines(mysplinefit1)\n\n\n# splinefun produces a function\nplot(x, y, xlim=c(0,10), ylim=c(-3,3), pch=4)\n\nmysplinefit2 = splinefun(x, y, method=\"natural\")\ncurve(mysplinefit2, 0, 10, add=T)\n\n\n\n\n\n\n\n\n(a) Using the spline command\n\n\n\n\n\n\n\n(b) Using the splinefun command\n\n\n\n\nFigure 3.4: R code for cubic interpolating splines.\n\n\n\nThe following, illustrates the different ways to draw the spline and to calculated fitted values.\n\n\nCode\nset.seed(15342)\n\nx = 1:9; y = rnorm(9)\n\n# spline deals with points\nmysplinefit1 = spline(x, y, method=\"natural\")\nspline(x, y, xout=c(2.5, 7.5), method=\"natural\")\n\n\n$x\n[1] 2.5 7.5\n\n$y\n[1]  0.08785792 -0.78655273\n\n\nCode\n# splinefun produces a function\nmysplinefit2 = splinefun(x, y, method=\"natural\")\nmysplinefit2(c(2.5, 7.5))\n\n\n[1]  0.08785792 -0.78655273\n\n\n\n\nBefore finishing, as we saw in Lecture 2, let us consider the derivatives of the fitted spline function. Figure 3.5 shows (a) the fitted natural spline, along with its first three derivatives in (b)-(d). Note that the function and the first two derivatives are continuous everywhere, but that the third derivatives is not continuous but has jumps at the knot locations. Note also that the first derivative, and higher derivatives, are all constant outside the range of the interior knots.\n\n\nCode\nset.seed(15342)\nx = 1:9; y = rnorm(9)\n\nmysplinefit2 = splinefun(x, y, method=\"natural\")\n\nplot(x, y, pch=16)\ncurve(mysplinefit2, 0, 10, add=T, lwd=1.5)\nabline(v=x, col=\"grey\")\n\n# Plot the first derivative\ncurve(mysplinefit2(x,deriv=1), 0, 10, lwd=1.5,  \n      xlab=\"x\", ylab=\"First derivative\")\nabline(v=x, col=\"grey\"); abline(h=0, col=\"grey\")\n\n# Plot the second derivative\ncurve(mysplinefit2(x,deriv=2), 0, 10, lwd=1.5, \n      xlab=\"x\", ylab=\"Second derivative\")\nabline(v=x, col=\"grey\"); abline(h=0, col=\"grey\")\n\n# Plot the third derivative\ncurve(mysplinefit2(x,deriv=3), 0, 10, lwd=1.5, n=10001,\n      xlab=\"x\", ylab=\"Third derivative\")\nabline(v=x, col=\"grey\"); abline(h=0, col=\"grey\")\n\n\n\n\n\n\n\n\n(a) Data and spline function\n\n\n\n\n\n\n\n(b) First derivative\n\n\n\n\n\n\n\n\n\n(c) Second derivative\n\n\n\n\n\n\n\n(d) Third derivative\n\n\n\n\nFigure 3.5: Derivatives of cubic interpolating splines."
  },
  {
    "objectID": "3_interpolating.html#focus-on-fitting-splines-quiz",
    "href": "3_interpolating.html#focus-on-fitting-splines-quiz",
    "title": "3  Interpolating Splines",
    "section": "Focus on fitting splines quiz",
    "text": "Focus on fitting splines quiz\nTest your knowledge recall and comprehension to reinforce basic ideas about R commands for splines.\n\n\n\nWhich of the following should be first in every data analysis? Fit a spline modelCalculate the correlationAdd the fitted curve to a plot of the dataFit a linear modelDraw a scatter plot\nWhich of the following is NOT part of a data analysis using interpolating splines? Read the data descriptionExamine model residualsAdd the fitted curve to a plot of the dataFit a spline modelDraw a scatter plot of the data\nWhat is the main difference between the R commands spline and splinefun? splinefun produces a functionspline interpolates whereas splinefun smoothsspline is more boringOnly spline has knotsThey are identical, only the name is different\nWhich of the following is NOT an optional method for the R command spline? fmmperiodicnaturalhymansmooth\nWhen using the output from splinefun, which of the following CANNOT be plotted? Fourth derivativeFunction valueSecond derivativeFirst derivativeThird derivative"
  },
  {
    "objectID": "3_interpolating.html#sec-exercises2",
    "href": "3_interpolating.html#sec-exercises2",
    "title": "3  Interpolating Splines",
    "section": "3.6 Exercises",
    "text": "3.6 Exercises\n\n3.1 For the situation shown in Figure 2.2, but taking \\(p=1\\), write-down the linear functions for the three intervals and clearly identify all the \\(6\\) model parameters. Next, write down the constraints required to make the functions pass through the \\(m=2\\) data points, and the two constraints which impose continuity of function. What additional constraints are needed to fix the first derivative at zero for the outer two intervals?\n\n\nClick here to see hints.\n\nYou might get two redundant constraints but think carefully about which can be removed.\n\n3.2 Continuing the problem described in Exercise 3.1, write the constraints as a system of 6 linear equations in the 6 unknown model parameters. How might you solve this system to give the parameter values which solve the interpolation problem?\n\n\nClick here to see hints.\n\nYou need to write the six equations as a set of simultaneous equations. A simple matrix inversion is all that is needed for the solution – but don’t try to actually do the inversion!\n\n3.3 Continuing the linear system described in Exercise 3.2, create a synthetic problem by choosing two data response values. Then solve the system in \\({\\mathbf R}\\), or otherwise, and plot the fitted spline interpolating function.\n\n\nClick here to see hints.\n\nThe choice of the two data point locations (x-values) and function value (y-value) is completely your choice. Then, define the two vectors and the matrix defined in the previous question. R has a function “solve” which will invert the design matrix – look at “help(solve)”\n\n3.4 Again, considering the situation shown in Figure 2.2, but taking \\(p=1.\\) Using the alternative representation in Equation 3.6, write down two constraints involving the data points and the additional constraint on the \\(b_i\\) parameters. Write this linear system of 3 equations in three unknowns in matrix form.\n\n\nClick here to see hints.\n\nThe first part only requires checking the definition of the alternative form in the notes. Then, write it in vector/matrix form.\n\n3.5 Continuing the linear system described in Exercise 3.4, using the same points created in Exercise 3.4, calculate the parameter values in this new parameterization. Check that your two fitted interpolating spline give the same answers. Which approach do you prefer? Justify you answer.\n\n\nClick here to see hints.\n\nAgain, in R, define the vectors and matrix, and use “solve” to find the parameter estimates. You can check for equality by any approach, for example just plot both solutions and see if they match. The choice of which it preferable is yours, no wrong answer, but it’s the justification that matters.\n\n3.6 Create you own version of the R code used to produce Figure 3.4 and experiment with the two alternative spline fitting commands. Remove the \\(\\texttt{set.seed(15342)}\\) command so that you produce different data each time and comment on the similarities and differences when using different data sets.\n\n\nClick here to see hints.\n\nSimply copy the code from the referred to figure and remove the set.seed command. Then, each time you run the code you will get a different answer.\n\n3.7 Let \\[\\begin{equation*}\n     f(t) = 3 + 2t + 4|t|^3 + |t-1|^3.\n\\end{equation*}\\] Write \\(f\\) as a cubic polynomial in each of the intervals \\((-\\infty,0)\\), \\((0,1)\\) and \\((1, \\infty)\\). Verify that \\(f\\) and its first two derivatives are continuous at the knots.\nIs \\(f\\) a spline? Is \\(f\\) a natural spline?\n\n\nClick here to see hints.\n\nTo show this is a spline, you need to re-write the equations as polynomials. That is consider what happens to the absolute values in each interval. Then, substitute in the knot locations (0 and 1) into the equations for function, derivative and second derivative and make sure all matches. There is also a solution based on the equation of the alternative form – see if you can argue it this way also.\n\n3.8 Let \\[\\begin{equation*}\n     f(t) = 3 + |t| - |t-2|.\n\\end{equation*}\\]\nShow by direct calculation that \\[\\begin{equation*}\n     \\int_{-\\infty}^{\\infty} \\{f'(t)\\}^2 \\, dt = 8.\n   \\end{equation*}\\]\nShow that this integral can also be written in the form \\(-2 \\mathbf b^T K \\mathbf b\\), where you should define \\(\\mathbf b\\) and \\(K\\).\n\n\nClick here to see hints.\n\nUse a similar approach as in the previous equation to “remove” the absolute value. You should then see that the integral is trivial. For the matrix form, go back to the definitions of each part. Multiplying out will then give the same answer as the integral.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nExercise 3 Solutions can be found here."
  },
  {
    "objectID": "4_smoothing.html#overview",
    "href": "4_smoothing.html#overview",
    "title": "4  Smoothing Splines",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nIn Section 1.3 we described a general statistical model with a response variable \\(y\\) and an explanatory variable \\(x\\). We observe \\(y_i\\) at each location \\(x_i,\\) for \\(i=1,\\ldots, n\\). We imagined that the \\(y\\)’s are noisy versions of a smooth function of \\(t\\), say \\(f(\\cdot)\\) where the errors follow a normal distribution with constant variance. That is \\[\ny_i = f(x_i) + \\epsilon_i, \\quad\n\\epsilon_i \\sim \\mathrm{N}(0,\\sigma^2),\n\\] for \\(i=1,\\ldots,n\\), where \\(f\\) is smooth, the \\(\\epsilon_i\\) are i.i.d., and \\(f\\) and \\(\\sigma^2\\) are unknown.\nThe log-likelihood for this situation is: \\[\nl(f; \\mathbf{y})\n= -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2 - n \\log \\sigma\n\\tag{4.1}\\] and we wish to estimate \\(f\\) for a given data set \\(\\mathbf{y}=\\{y_1,\\dots, y_n\\}\\). With no constraints on \\(f\\), the log-likelihood would be maximised by setting \\(f(x_i)=y_i\\) for all \\(i\\), and we would estimate the noise variance as \\(\\hat{\\sigma}^2=0\\). This takes no account of randomness in the data and \\(f\\) would in general need to be quite wiggly to achieve this fit.\nFigure 4.1 (a) shows such an interpolation of noisy data. This would be of little use for explanation, interpolation or prediction.\n\n\n\n\n\n\n\n(a) Interpolating spline\n\n\n\n\n\n\n\n(b) Smoothing spline\n\n\n\n\nFigure 4.1: Comparison of interpolating and smoothing methods applied to a noisy dat set.\n\n\nWe do not expect, or even want, the fitted function \\(f\\) to pass exactly through the data points \\(\\{(x_i,y_i), i=1,\\dots,n\\}\\), but merely to lie close to them. We would rather trade-off goodness-of-fit against smoothness of \\(f\\). Figure 4.1 (b) shows a smoothing spline fit to the same data. This is much better as there is a clear explanation of the relationship, it could be used reasonably well for interpolation and prediction."
  },
  {
    "objectID": "4_smoothing.html#the-penalized-least-squares-criterion",
    "href": "4_smoothing.html#the-penalized-least-squares-criterion",
    "title": "4  Smoothing Splines",
    "section": "4.2 The penalized least-squares criterion",
    "text": "4.2 The penalized least-squares criterion\nNoting that maximizing Equation 4.1 is equivalent to minimizing the residual sum of squares: \\(\\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2\\), we can achieve this trade-off by minimizing a penalized sum of squared residuals: \\[\nR_\\nu(f,\\lambda) = \\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2 + \\lambda J_\\nu(f),\n\\tag{4.2}\\] where \\(J_\\nu(f)\\), as first defined in Equation 3.8, penalizes the roughness of \\(f\\). The smoothing parameter \\(\\lambda\\geq 0\\) controls the severity of this penalty. For now we will assume \\(\\lambda\\), which absorbs the \\(\\sigma^2\\) in Equation 4.1, is known.\nFigure 4.2 shows example smoothing spline fits using a range of smoothing parameters, \\(\\lambda\\). In Figure 4.2 (a) the fit is essentially a straight line, perhaps Figure 4.2 (b) and Figure 4.2 (c) show acceptable fits. Figure 4.2 (d), with a very small \\(\\lambda\\) value is close to an interpolating spline fit and is clearly unacceptable.\n\n\n\n\n\n\n\n(a) Very strong smoothing\n\n\n\n\n\n\n\n(b) Moderate smoothing\n\n\n\n\n\n\n\n\n\n(c) Weak smoothing\n\n\n\n\n\n\n\n(d) Very weak smoothing\n\n\n\n\nFigure 4.2: Comparison of interpolating and smoothing methods applied to a noisy dat set.\n\n\nAs the smoothing parameter \\(\\lambda\\) increases, the optimal \\(f\\) becomes smoother. In particular, it can be shown that as \\(\\lambda \\rightarrow \\infty\\), the vector of coefficients \\({\\mathbf b} \\rightarrow {\\mathbf 0}\\), and \\({\\mathbf a}\\) tends to the usual least-squares estimate of the regression parameters. Thus, the smoothing spline converges to the sample mean \\(f(x)=\\bar y\\) when \\(\\nu=1\\), and to the ordinary least squares fitted line, \\(f(x)=\\hat\\alpha+\\hat\\beta t\\), when \\(\\nu=2\\). In the other direction, as \\(\\lambda \\rightarrow 0\\) the smoothing solution converges to the interpolating spline."
  },
  {
    "objectID": "4_smoothing.html#focus-on-smoothing-splines-quiz",
    "href": "4_smoothing.html#focus-on-smoothing-splines-quiz",
    "title": "4  Smoothing Splines",
    "section": "Focus on smoothing splines quiz",
    "text": "Focus on smoothing splines quiz\nTest your knowledge recall and comprehension to reinforce basic ideas about smoothing splines.\n\n\n\nWhich of the following statements is NOT true?\n\n The smoothing spline parameter λ controls the amount of smoothing Interpolating splines pass through all data points Smoothing splines should be used for all problems Smoothing splines are better for noisy data Both interpolating and smoothing splines are piecewise polynomial functions\n\n\n2 Which of the following statements is true about splines? The roughness measures the wiggliness of a functionThe roughness can be integrated to give the smoothnessThe roughness defines the likelihood functionThe roughness is a measure of goodness of fitThe roughness measures the lack of fit\n\nWhich of the following best describes the result of using a very large value of λ in a cubic smoothing spline? The spline is approximately quadraticThe spline is approximately constantThe spline is approximately linearThe spline is approximately cubicThe spline interpolates the data\nWhich of the following best describes the result of using a very small value of λ in a linear smoothing spline? The spline is approximately quadraticThe spline is approximately cubicThe spline is approximately linearThe spline is approximately constantThe spline interpolates the data\nWhich of the following statements best describes how to choose the value of λ? To balance closeness to data and roughnessTo minimise the roughnessTo maximise the likelihoodTo maximise the roughnessTo better fit the data with a high roughness"
  },
  {
    "objectID": "4_smoothing.html#sec:relationto",
    "href": "4_smoothing.html#sec:relationto",
    "title": "4  Smoothing Splines",
    "section": "4.3 Relation to interpolating splines",
    "text": "4.3 Relation to interpolating splines\nWe show in the following proposition that the function \\(f\\) which minimizes Equation 4.2 is the interpolating spline of its fitted values.\n\nProposition 4.1 Suppose \\(\\hat{f}\\) minimizes \\(R_\\nu(f,\\lambda)\\) and let \\(\\hat{y}_i = \\hat{f}(x_i), i=1,\\dots,n\\) denote the corresponding fitted values. Then, \\(\\hat{f}\\) solves the interpolation problem for the artificial data set \\((x_i,\\hat{y}_i), i=1,\\dots , n\\). That is, \\(\\hat{f}\\) minimizes \\(J_\\nu(f)\\) over functions \\(f\\) satisfying \\(\\hat f(x_i)=\\hat{y}_i\\), \\(i=1,\\ldots,n\\). Consequently, \\(\\hat{f}\\) is a \\(p^{\\text{th}}\\)-order natural spline, where \\(p=2\\nu-1\\). This means that, when solving the smoothing problem, we only need consider spline functions given by the representations in Proposition 3.1.\n\n\n\n\n\n\n\n\n(a) Smoothing spline of data shown as dots and with fitted values shown as crosses\n\n\n\n\n\n\n\n(b) Interpolating spline of fitted values from smoothing problem\n\n\n\n\nFigure 4.3: Illustration of proposition showing that solution of the smothing problem is a natural interpolating spline.\n\n\nProof: Suppose the assertion is not true. In this case, we must be able to find a function \\({\\hat{f}^\\star}\\), say, which also interpolates the artificial data \\((x_i,\\hat{y}_i), i=1,\\dots ,n\\), but which has a smaller roughness penalty. That is \\[\nJ_\\nu({\\hat{f}^\\star}) &lt; J_\\nu(\\hat{f})\n  \\mbox{ with }\n\\hat{f}^\\star(x_i) = \\hat{y}_i, \\ i=1,\\ldots,n.\n\\] Note that the fitted values from the function \\(\\hat f^\\star\\) are also equal to \\(\\hat y_i, i=1,\\dots,n\\) as it interpolates the same artificial data as \\(\\hat f\\).\nNow, from Equation 4.2, \\[\\begin{align*}\n  R_\\nu(\\hat{f}^\\star, \\lambda) &= \\sum_i (y_i-\\hat{y}_i)^2 + \\lambda J_\\nu(\\hat{f}^\\star)\\\\\n&&lt; \\sum_i (y_i-\\hat{y}_i)^2 + \\lambda J_\\nu(\\hat{f})\n= R_\\nu(\\hat{f}, \\lambda).\n\\end{align*}\\] Hence \\(R_\\nu(\\hat{f}^\\star, \\lambda) &lt; R_\\nu(\\hat{f}, \\lambda)\\). But, by construction \\(\\hat{f}\\) minimizes \\(R_\\nu(f, \\lambda),\\) which is a contradiction. Hence, it must not be possible to find a function \\({\\hat{f}^\\star}\\) which also interpolates the artificial data but which has a smaller roughness penalty.\nWe have shown that \\(\\hat{f}\\) is the optimal interpolant of the fitted values \\(\\hat{y}_i,\\ i=1,\\dots,n,\\) so it follows from Proposition 4.1 that \\(\\hat{f}\\) is a natural spline of order \\(p=2\\nu-1\\)."
  },
  {
    "objectID": "4_smoothing.html#sec-smoothingmatrix",
    "href": "4_smoothing.html#sec-smoothingmatrix",
    "title": "4  Smoothing Splines",
    "section": "4.4 The smoothing problem in matrix notation",
    "text": "4.4 The smoothing problem in matrix notation\nWe have just proved that the function \\(\\hat{f}\\) that minimizes \\(R_\\nu (f, \\lambda)\\) must be a natural spline (linear if \\(\\nu=1\\), cubic if \\(\\nu=2\\)) with knots at \\(\\{t_i,\\,i=1,\\ldots,n\\}.\\) That is, as in Proposition 3.1, we can write:\n\\[\n\\hat{f}(t) = \\sum_{i=1}^n b_i\\left|t-t_i\\right|^p +\n\\begin{cases}\na_0, &\\nu=1\\\\\na_o + a_1t, &\\nu=2,\n\\end{cases}  \n\\tag{4.3}\\] where \\(p=2\\nu-1\\) and constraints \\[\n\\sum_{i=1}^n b_i = 0 \\, \\mbox{ for }\\, \\nu=1 \\quad\n\\sum_{i=1}^n b_i = \\sum_{i=1}^n b_i \\, t_i = 0\n\\, \\mbox{ for }\\, \\nu=2.\n\\tag{4.4}\\]\nHowever, we have not yet figured out how to calculate the parameter values \\(\\hat{a}_0,\\dots,\\hat{a}_{\\nu-1},\\ \\hat{b}_1,\\dots,\\hat{b}_n\\) in Equation 4.3 which optimally fit the data \\(y_1,\\dots,y_n\\). For this, it is convenient to re-express the penalised sum of squared residuals Equation 4.2 in matrix notation.\n\nProposition 4.2 The roughness of a natural linear spline \\((\\nu=1\\), i.e. \\(p=1)\\) is \\[\nJ_1(f)  \n= -2\\sum_{i=1}^n \\sum_{k=1}^n b_i b_k \\left| t_i - t_k \\right|\n= c_1\\,{\\mathbf b}^T \\, K_1 \\, {\\mathbf b},\n\\tag{4.5}\\] and the roughness of a natural cubic spline (\\(\\nu=2\\), i.e. \\(p=3\\)) is \\[\nJ_2(f)  = 12\\sum_{i=1}^n \\sum_{k=1}^n b_i b_k \\left| t_i - t_k \\right|^3\n= c_2\\,{\\mathbf b}^T \\, K_2 \\, {\\mathbf b},\n\\tag{4.6}\\] where the constants are given by \\[\nc_1=-2, \\quad c_2=12.\n\\tag{4.7}\\] Here \\({\\mathbf b}=(b_1,\\ldots,b_n)^{T}\\) is the vector of spline coefficients and \\(K_\\nu\\) is the \\(n\\times n\\) matrix whose \\((i,k)\\)th element is \\(\\left| t_i - t_k \\right|^p\\), where \\(p=2\\nu-1\\) that is \\[\nK_\\nu\n=\n\\begin{bmatrix}\n|t_1-t_1|^p & |t_1-t_2|^p & \\dots & |t_1-t_n|^p \\\\\n|t_2-t_1|^p & |t_2-t_2|^p & \\dots & |t_2-t_n|^p \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n|t_n-t_1|^p & |t_n-t_2|^p & \\dots & |t_n-t_n|^p.\n\\end{bmatrix}\n\\]\n\nProof: To be covered later if there is sufficient time.\nFrom Proposition 4.2 the roughness penalty satisfies: \\[\\begin{align}\nJ_\\nu  (\\hat{f}) = c_\\nu   \\, {\\mathbf b}^T K_\\nu   {\\mathbf b} \\label{eq:Jnufhat}\n\\end{align}\\] where \\(c_1=-2\\); \\(c_2= 12\\); \\({\\mathbf b}=(b_1,\\ldots,b_n)^T\\); and \\(K_\\nu\\) is the \\(n \\times n\\) matrix whose \\((i,k)\\)th element is \\(\\left|t_i-t_k\\right|^p\\).\nFrom Equation 4.3, the value of \\(\\hat{f}\\) at knot \\(t_k\\) is \\[\n\\hat{f}(t_k) = \\sum_{i=1}^n b_i\\left|t_k-t_i\\right|^p + \\begin{cases}a_0, &\\nu=1\\\\ a_0 + a_1t_k, &\\nu=2 \\end{cases}.\n\\tag{4.8}\\]\nIn matrix form, this is \\[\n\\mathbf{\\hat{f}} = \\left[\n\\begin{array}{c}\\hat{f}(t_1) \\\\\n\\vdots  \\\\\n\\hat{f}(t_n)\\end{array}\n\\right]\n= K_\\nu   {\\mathbf b} + L_\\nu   \\mathbf{a}_{\\,\\nu}\n\\tag{4.9}\\] where \\[\nL_1 =\n\\left[\n\\begin{array}{c} 1\\\\ \\vdots \\\\ 1\\end{array}\\right], \\quad \\mathbf{a}_1 = a_0,\n\\quad \\quad \\mbox{and} \\quad \\quad\nL_2 =\n\\left[\n\\begin{array}{cc} 1 & t_1 \\\\\n\\vdots & \\vdots\n\\\\ 1 & t_n\n\\end{array}\n\\right],\n\\quad \\mathbf{a}_2 = \\left[\\begin{array}{l}a_0\\\\a_1\\end{array}\n\\right] .\n\\]\nThus, from Equation 4.4 – Equation 4.9, the penalised least-squares criterion Equation 4.2 reduces to a quadratic function of the parameters \\(\\mathbf{a}\\) and \\({\\mathbf b}\\):\n\\[\nR_\\nu(f, \\lambda) = (\\mathbf{y} - K_\\nu  \\, {\\mathbf b} - L_\\nu   \\, \\mathbf{a})^T  (\\mathbf{y} - K_\\nu  \\, {\\mathbf b} - L_\\nu   \\, \\mathbf{a})\n                            + \\lambda \\,c_\\nu  \\,{\\mathbf b}^T K_\\nu   \\,{\\mathbf b}\n\\tag{4.10}\\] subject to \\[\nL_\\nu^T\\,{\\mathbf b} = 0,\n\\tag{4.11}\\] where \\(\\mathbf{y} = (y_1,\\ldots,y_n)^T\\).\nTo find the explicit values for \\(\\mathbf{b}\\) and \\(\\mathbf{a}\\), we must minimize the quadratic function Equation 4.10 subject to the linear constraints Equation 4.11.\n\nProposition 4.3 The solution to the smoothing spline problem is given by \\[\n\\begin{bmatrix}\n\\hat{\\mathbf a} \\\\ \\hat{\\mathbf b}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 & L_\\nu \\\\\nL_\\nu^T & K_\\nu+\\lambda^* I_n\n\\end{bmatrix}^{-1}\n\\begin{bmatrix}\n{\\mathbf 0} \\\\ {\\mathbf y}\n\\end{bmatrix}\n\\tag{4.12}\\] and \\(\\lambda^* = c_\\nu \\lambda\\).\n\nProof: Omitted."
  },
  {
    "objectID": "4_smoothing.html#smoothing-splines-in-r",
    "href": "4_smoothing.html#smoothing-splines-in-r",
    "title": "4  Smoothing Splines",
    "section": "4.5 Smoothing splines in R",
    "text": "4.5 Smoothing splines in R\nFitting a smoothing spline to data involved estimating values of \\(\\mathbf a\\) and \\(\\mathbf b\\) to minimize the penalised roughness measure in Equation 4.10 subject to the constraints in Equation 4.11 which yields the explicit equation in Equation 4.12.\nOf course, it is possible to code the solution of this matrix system, but it is usual instead to use in-built commands in software such as R.\nIn R the basic command is:\n\n\nCode\nsmooth.spline(x, y, lambda)\n\n\nwhere \\(\\texttt{x}\\) and \\(\\texttt{y}\\) are vectors of data coordinates, and \\(\\texttt{lambda}\\) specifies the value of the smoothing parameter to use – note that the effect of a given value of \\(\\lambda\\) depends on the problem considered.\nFor example, consider the synthetic data set which contains a break-point at \\(x=2/3\\) first met in Figure 1.2. Fitted spline curves with \\(\\lambda=0.0001\\) and \\(\\lambda=1\\), shown in Figure 4.4, are based on the following commands:\n\n\nCode\nmyfit1 = smooth.spline(x, y, lambda = 0.00001)\nmyfit2 = smooth.spline(x, y, lambda = 1)\n\n\n\n\nCode\npar(mar=c(3,3,1,1), mgp=c(2,1,0))\nset.seed(12341)\n\nx = runif(40)\ny = 0.75*(x &gt;= 0.67) + 0.6*x*(x&lt;0.67) + rnorm(40,0,0.1)\n\nplot(x,y, xlim=c(0,1), ylim=c(0,1), pch=16)\nmyfit1 = smooth.spline(x, y, lambda = 0.00001)\nfit.locations = seq(0,10,0.01)\nfitted = predict(myfit1, fit.locations)\nlines(fitted, col=\"blue\")\n\nplot(x,y, xlim=c(0,1), ylim=c(0,1), pch=16)\nmyfit2 = smooth.spline(x, y, lambda = 1)\nfit.locations = seq(0,10,0.01)\nfitted = predict(myfit2, fit.locations)\nlines(fitted, col=\"blue\")\n\n\n\n\n\n\n\n\n(a) Weak smoothing\n\n\n\n\n\n\n\n(b) Strong smoothing\n\n\n\n\nFigure 4.4: Simulated data superimposed with fitted smoothing splines.\n\n\n\nIf no value of the smoothing parameter is specified, the the optimal value is calculated using generalised cross-validation as discussed in the next chapter."
  },
  {
    "objectID": "4_smoothing.html#exercies",
    "href": "4_smoothing.html#exercies",
    "title": "4  Smoothing Splines",
    "section": "4.6 Exercies",
    "text": "4.6 Exercies\n\n4.1 Recall, from MATH3823 Chapter 3, the tropical cyclone data recording the number of cyclones in each of 13 seasons:\n\n\nTable 4.1: Numbers of tropical cyclones in \\(n = 13\\) successive seasons1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeason\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n\n\nNo of cyclones\n6\n5\n4\n6\n6\n3\n12\n7\n4\n2\n6\n7\n4\n\n\n\n\nIt is reasonable to allow the average number of cyclones to vary with season, rather than assuming a constant rate.\nFit a cubic smoothing spline to these data, using a range of values for the smoothing parameter \\(\\lambda\\) to estimate the time-varying cyclone rate. By eye, suggest a suitable value for \\(\\lambda\\) and justify your choice. Compare the smoothed estimate of rate with the value obtain by assuming a constant rate.\n\n\nClick here to see hints.\n\nFollow the R code used in Section 4.5 but don’t worry about finding an exact value for the smoothing parameter, only the order of magnitude.\n\n4.2 Consider the Old Faithful data set on geyser eruptions available in R. Use the following commands to learn more about and visualize the data:\ndata(faithful)   \nhelp(faithful)  \nplot(faithful) \nFit a cubic smoothing spline to these data, using a range of values for the smoothing parameter \\(\\lambda\\). By eye, suggest a suitable value for \\(\\lambda\\).\n\n\nClick here to see hints.\n\nTaking waiting as the explanatory and eruption as the response, follow the R code used in Section 4.5 but don’t worry about finding an exact value for the smoothing parameter, only the order of magnitude.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nExercise 4 Solutions can be found here."
  },
  {
    "objectID": "4_smoothing.html#footnotes",
    "href": "4_smoothing.html#footnotes",
    "title": "4  Smoothing Splines",
    "section": "",
    "text": "Dobson and Barnett, 3rd edn, Table 1.2↩︎"
  },
  {
    "objectID": "5_smoothingparameter.html#overview",
    "href": "5_smoothingparameter.html#overview",
    "title": "5  Choosing the Smoothing Parameter",
    "section": "5.1 Overview",
    "text": "5.1 Overview\nSuppose we are given data \\(D=\\{(t_i,y_i),\\,i=1,\\ldots,n\\}\\) and that our model is: \\[\ny_i = f(t_i) + \\epsilon_i,\\qquad \\epsilon_i\\sim\\text{N}(0,\\sigma^2)\n\\tag{5.1}\\] where the \\(\\epsilon_i\\) are i.i.d. \\(\\sim\\text{N}(0,\\sigma^2)\\) and \\(f(t)\\) is assumed to be smooth. Given knot positions \\(\\{t_i,\\; i=1,\\dots,n\\}\\), we can estimate \\(f(t)\\) with a smoothing spline \\(\\hat{f}_\\lambda(t)\\).\nHow then should we choose the value of the smoothing parameter \\(\\lambda\\)? By setting \\(\\lambda \\rightarrow 0\\), we obtain exactly the interpolating spline \\(\\hat{f}_0(t)\\) and a perfect fit to the data. However, this tends to overfit the data: applying it to to a new sample of data where model Equation 5.1 still applies would produce a poor fit. Conversely, by setting \\(\\lambda \\rightarrow \\infty\\), we get:\n\\[\\begin{align*}\nf_\\infty(t) &=\n\\begin{cases} \\hat{a}_0, & \\nu=1,\\ p=1\\\\\n\\hat{a}_0 + \\hat{a}_1 t, & \\nu=2,\\ p=3.\n\\end{cases}\n\\end{align*}\\] Here, \\(\\hat{a}_0=\\bar{y}\\), for the \\(\\nu=1, p=1\\) case, and \\(\\{\\hat{a}_0,\\ \\hat{a}_1\\}\\), for the \\(v=2, p=3\\) case, are the OLS linear regression parameters.\nIf the true \\(f(t)\\) was constant or linear, this solution would be reasonable, but often we are interested in less regular functions."
  },
  {
    "objectID": "5_smoothingparameter.html#trainingtest-approach",
    "href": "5_smoothingparameter.html#trainingtest-approach",
    "title": "5  Choosing the Smoothing Parameter",
    "section": "5.2 Training/test approach",
    "text": "5.2 Training/test approach\nOne way to approach estimation of \\(\\lambda\\) is to partition the set of indices \\(I= \\{1,\\dots,n\\}\\) into two subsets \\(I_1\\) and \\(I_2\\), where \\(I_1\\cup I_2 = I\\) and \\(I_1 \\cap I_2 = \\phi\\). Thus we obtain two datasets:\n\nTraining dataset: \\(D_1 = \\{(t_{i},y_{i}),\\ i\\in I_1\\}\\),\nTest dataset: \\(D_2 = \\{(t_{i},y_{i}),\\ i\\in I_2\\}\\).\n\nWe fit a smoothing spline \\(\\hat{f}_{\\lambda,I_1}(t)\\) to the training dataset, and judge the quality of the fit using the test dataset: \\[\nQ_{I_1:I_2}(\\lambda) = \\sum_{i\\in I_2} \\left(y_i - \\hat{f}_{\\lambda,I_1}(t_i)\\right)^2.\n\\tag{5.2}\\] We choose \\(\\lambda\\) to minimise \\(Q_{I_1:I_2}(\\lambda)\\). Many algorithms exist for such minimization, for example through evaluation on a fine grid of \\(\\lambda\\) values, although many more computationally efficient algorithms exist."
  },
  {
    "objectID": "5_smoothingparameter.html#sec-cv",
    "href": "5_smoothingparameter.html#sec-cv",
    "title": "5  Choosing the Smoothing Parameter",
    "section": "5.3 Cross-validation or leave-one-out",
    "text": "5.3 Cross-validation or leave-one-out\nThis is an extreme form of the above principle. The test dataset \\(D_2\\) comprises a single observation, \\((t_{j},y_{j})\\), for a given value of \\(j\\). The training set \\(D_1\\) is then \\(D_{-j} = \\{(t_{i},y_{i}),\\ i \\in I_{-j}\\}\\), where \\(I_{-j}\\) denotes the full set \\(I\\) excluding \\(j\\). Then in a slightly amended notation we can write \\[\nQ_{-j:j}(\\lambda) = \\left(y_j - \\hat{f}_{\\lambda,-j}(t_j)\\right)^2\n\\] to assess the quality of fit. Of course, \\(j\\) is arbitrary, so we repeat this process for each \\(j\\in\\{1,\\dots,n\\}\\) then average the assessments to form the ordinary cross-validation criterion: \\[\nQ_{OCV}(\\lambda) = \\frac{1}{n} \\sum_{j=1}^n \\left(y_j - \\hat{f}_{\\lambda,-j}(t_j)\\right)^2.\n\\tag{5.3}\\]\nWe then choose the value \\(\\hat{\\lambda}\\) which minimises \\(Q_{OCV}(\\lambda)\\). Hopefully, a plot of \\(Q_{OCV}(\\lambda)\\) will appear as in Figure 5.1, but there is no theoretical guarantee that this curve will have a unique turning point, making it difficult to locate the minimum.\n\n\n\n\n\nFigure 5.1: Ordinary cross validation plot\n\n\n\n\nAt first sight, evaluation of \\(Q_{OCV}(\\lambda)\\) for a given \\(\\lambda\\) appears computationally intensive: we must compute \\(n\\) different smoothing solutions, each corresponding to one of the left-out data points. Fortunately, there is a computational trick which enables us to compute \\(Q_{OCV}(\\lambda)\\) directly from the smoothing spline solution constructed from the whole dataset"
  },
  {
    "objectID": "5_smoothingparameter.html#the-smoothing-matrix",
    "href": "5_smoothingparameter.html#the-smoothing-matrix",
    "title": "5  Choosing the Smoothing Parameter",
    "section": "5.4 The smoothing matrix",
    "text": "5.4 The smoothing matrix\nHere we show, for a given value of the smoothing parameter \\(\\lambda\\) and the index \\(\\nu \\geq 1\\), that the fitted value \\(\\hat{f}_\\lambda(t_k)\\) at each knot \\(t_k\\) may be written as a linear combination of the observations, \\(y_1,\\dots,y_n\\).\nRecall from Proposition 4.3 that the smoothing spline \\(\\hat{f}_\\lambda(t)\\), which minimises the penalised sum of squares criterion Equation 4.2 for a given value of the smoothing parameter \\(\\lambda\\), has coefficients \\(\\hat{\\mathbf{a}}, \\hat{\\mathbf{b}}\\) where \\[\n\\begin{bmatrix}\n\\hat{\\mathbf a} \\\\ \\hat{\\mathbf b}\n\\end{bmatrix}\n=\nM_{\\lambda}^{-1}\n\\begin{bmatrix}\n{\\mathbf 0} \\\\ {\\mathbf y}\n\\end{bmatrix}\n\\quad \\text{where} \\quad\nM_{\\lambda} =\n\\begin{bmatrix}\n0 & L_\\nu^T \\\\\nL_\\nu & K_\\nu+\\lambda^* I_n\n\\end{bmatrix}\n\\]\nThe fitted values of the smoothing spline at the knots can be represented in matrix form as \\[\n\\mathbf{\\hat{f}} = \\left[\n\\begin{array}{c}\\hat{f}(t_1) \\\\\n\\vdots  \\\\\n\\hat{f}(t_n)\\end{array}\n\\right]\n= K  \\, {\\hat {\\mathbf b}} + L \\, {\\hat {\\mathbf a}}\n=\n\\begin{bmatrix}\nK & L\n\\end{bmatrix}\n\\begin{bmatrix} M_\\lambda^{12} \\\\ M_\\lambda^{22}\n\\end{bmatrix}\n\\mathbf{y},\n\\] where \\(M_\\lambda^{-1}\\) has been partitioned in the form \\[\nM_\\lambda^{-1} = \\begin{bmatrix} M_\\lambda^{11} & M_\\lambda^{12} \\\\ M_\\lambda^{21} & M_\\lambda^{22}\n\\end{bmatrix},\n\\] where \\(M_\\lambda^{12}\\) is \\(\\nu \\times \\nu\\) and \\(M_\\lambda^{22}\\) is \\(n \\times n\\) .\nIt can be shown that the matrix \\[\nS_\\lambda =\n\\begin{bmatrix} K & L\n\\end{bmatrix}\n\\begin{bmatrix}\nM_\\lambda^{12} \\\\ M_\\lambda^{22}\n\\end{bmatrix}\n\\] is a symmetric positive definite matrix for \\(\\lambda &gt; 0\\) called the smoothing matrix. Then, \\(S_\\lambda\\) connects the data \\(\\mathbf{y}\\) to the fitted values through \\[\n\\mathbf{\\hat f} = S_\\lambda \\, \\mathbf{y}.\n\\tag{5.4}\\] That is, the fitted values are simple linear function of the data."
  },
  {
    "objectID": "5_smoothingparameter.html#sec-edf",
    "href": "5_smoothingparameter.html#sec-edf",
    "title": "5  Choosing the Smoothing Parameter",
    "section": "5.5 Effective degrees of freedom",
    "text": "5.5 Effective degrees of freedom\nHow many degrees of freedom are there in the smoothing spline? There are altogether \\(n+\\nu\\) parameters, \\(\\nu\\) in \\(\\mathbf{a}\\) and \\(n\\) in \\(\\mathbf{b}\\), but these are not completely free.\nWe showed earlier that as the smoothing parameter \\(\\lambda\\rightarrow \\infty\\), the smoothing spline \\(\\hat{f}(t)\\) becomes the least-squares regression solution for model formula \\(y \\sim 1\\) when \\(\\nu=1\\), or model formula \\(y \\sim 1+t\\) when \\(\\nu=2\\). Thus, when \\(\\lambda= \\infty\\), the degrees of freedom in the spline is \\(\\nu\\). We also showed that when \\(\\lambda=0\\), the smoothing spline \\(\\hat{f}(t)\\) becomes the interpolating spline, for which the degrees of freedom is the number of observations, \\(n\\).\nThus, intuitively, for values of \\(\\lambda\\) between the extremes of \\(0\\) and \\(\\infty\\), the spline degrees of freedom should lie somewhere between \\(\\nu\\) and \\(n\\); the greater the smoothing, the fewer the degrees of freedom. How can we capture this notion precisely?\nA clue comes from Ordinary Least Squares (OLS) regression, in which the fitted values are given by \\[\n\\hat{\\mathbf{y}} = X (X^T X)^{-1} X^T \\mathbf{y} = H \\mathbf{y},\n\\] where \\(X\\) is the \\(n \\times p\\) design matrix, where \\(p\\) is the number of model parameters. Here, \\[\nH = X (X^T X)^{-1} X^T\n\\] is called the hat matrix, which linearly maps the data \\(\\mathbf y\\) onto the fitted values \\(\\hat{\\mathbf{y}}\\). Using the property that \\(\\text{trace}(QR) = \\text{trace}(RQ)\\) for matrices \\(Q,R\\) of conformable dimensions, the trace of the hat matrix is: \\[\\begin{align*}\n\\text{trace}(X(X^T X)^{-1} X^T) &= \\text{trace}((X^T X)^{-1} X^T X)\\\\\n&= \\text{trace}(I_p)\\\\\n&= p,\n\\end{align*}\\] where \\(I_p\\) is the \\(p \\times p\\) identity matrix. Thus, for OLS regression, we see that the trace of the hat matrix equals the number of model parameters.\nNow in Equation 5.4 we see that the smoothing matrix \\(S_\\lambda\\) takes the role of a hat matrix, since it linearly maps the data onto the fitted values. This suggests that, for the smoothing spline, we can calculate an effective number of degrees of freedom as: \\[\n\\text{edf}_\\lambda = \\text{trace} \\, S_\\lambda.\n\\tag{5.5}\\] It can be shown from the limiting behaviour of the smoothing splines as \\(\\lambda \\rightarrow \\infty\\) and \\(\\lambda \\rightarrow 0\\) that \\(\\text{edf}_\\infty = \\nu\\) (the number of parameters in the OLS solution) and \\(\\text{edf}_0 = n\\) (the number of parameters in the interpolating spline)."
  },
  {
    "objectID": "5_smoothingparameter.html#sec-gcv",
    "href": "5_smoothingparameter.html#sec-gcv",
    "title": "5  Choosing the Smoothing Parameter",
    "section": "5.6 Generalised Cross Validation",
    "text": "5.6 Generalised Cross Validation\nThe cross-validation criterion \\(Q_{OCV}(\\lambda)\\), defined in Equation 5.2, is used to set the smoothing parameter, \\(\\lambda\\). However, as noted in Section 5.3, using Equation 5.2 to compute \\(Q_{OCV}(\\lambda)\\) would be impractical for large \\(n,\\) since it would require fitting a new smoothing spline \\(\\hat{f}_{\\lambda,-j}\\) for each leave-one-out dataset \\(I_{-j}\\).\nFortunately, it is possible to compute \\(Q_{OCV}(\\lambda)\\) directly from the spline \\(\\hat{f}_{\\lambda}\\) fitted to the full dataset. It can be shown that Equation 5.2 can be rewritten: \\[\nQ_{OCV}(\\lambda) = \\frac{1}{n} \\sum_{j=1}^n \\left(\\frac{y_j - \\hat{f}_{\\lambda}(t_j)}{1-s_{jj}}\\right)^2,\n\\tag{5.6}\\] where \\(\\hat{f}_{\\lambda}(t_j)\\) is the full-data fitted spline value at \\(t_j\\) given by Equation 5.4, and \\(s_{jj}\\) is the \\(j\\)th diagonal element of the smoothing (hat) matrix \\(S_\\lambda\\).\nPrior to the discovery of algorithm to compute Equation 5.6 quickly, a computationally efficient approximation to the cross-validation criterion Equation 5.2 was proposed, called the Generalised Cross-Validation criterion (GCV): \\[\nQ_{GCV}(\\lambda) = \\frac{ \\frac{1}{n} \\sum_{j=1}^n \\left(y_j - \\hat{f}_{\\lambda}(t_j)\\right)^2}{\\left(1-\\tfrac{1}{n}\\text{trace}(S_\\lambda)\\right)^2}.\n\\tag{5.7}\\] Thus Equation 5.7 replaces \\(s_{jj}\\) in Equation 5.6 with the average of the diagonal elements of \\(S_\\lambda\\), which equals \\(\\tfrac{1}{n}\\text{edf}_\\lambda\\). Thus a low value of \\(\\text{edf}_\\lambda\\) will deflate \\(Q_{GCV}(\\lambda)\\), making that value of \\(\\lambda\\) more favourable.\nIn principle, OCV supplants GCV, but GCV is still used as it is numerically more stable. In particular, GCV is used in the \\(\\texttt{mgcv}\\) package – this will be discussed in the next chapter."
  },
  {
    "objectID": "6_smoothinginpractice.html#overview",
    "href": "6_smoothinginpractice.html#overview",
    "title": "6  General Additive Models",
    "section": "6.1 Overview",
    "text": "6.1 Overview\nSo far, we have considered the modelling of a response variable \\(y\\) in terms of a single response variable \\(x\\). In particular, we have assumed that the form of this relationship is unknown but can be written as \\[\ny_i = f(t_i) + \\epsilon_i,\\qquad \\epsilon_i\\sim\\text{N}(0,\\sigma^2)\n\\] where the \\(\\epsilon_i\\) are i.i.d. \\(\\sim\\text{N}(0,\\sigma^2)\\) and \\(f(t)\\) is assumed to be smooth. Given knot positions \\(\\{t_i,\\; i=1,\\dots,n\\}\\), we can estimate \\(f(t)\\) with a smoothing spline \\(\\hat{f}_\\lambda(t)\\) for given smoothing parameter \\(\\lambda\\) and, further, we can estimate \\(\\lambda\\) using ordinary or generalised cross validation. This approach is in contrast to simple linear regression where \\(y\\) is expressed as a linear function of the explanatory variable \\(y_i=\\alpha + \\beta x_i +\\epsilon_i\\) which enforces a very inflexible relationship.\nIn this chapter, we generalise the modelling to describe the dependence of the response variable \\(y\\) on a set of explanatory variables \\(\\mathbf{x}=(x_1,x_2,\\dots, x_p)\\) where, conditionally on \\(\\mathbf{x}\\), observation \\(y\\) has a distribution which is not necessarily normal.\nJust as with a generalised linear model, the general additive model relates a continuous or discrete response variable \\(Y\\) to a set of explanatory variables \\(\\mathbf{x}=(x_1,x_2,\\dots, x_p)\\) and, again, the model contains three parts:\nRandom part: The probability (mass or density) function of \\(Y\\) is assumed to belong to the two-parameter exponential family of distributions with parameters \\(\\theta\\) and \\(\\phi\\).\nSystematic part: This is a non-linear predictor equation: \\[\n\\eta = \\sum_{j=1}^p f_j(x_j).\n\\tag{6.1}\\]\nLink function: This is a one-to-one function providing the link between the predictor equation \\(\\eta\\) and the mean \\(\\mu = \\mbox{E}[Y]\\):\n\\[\n\\eta = g(\\mu), \\quad \\mbox{and} \\quad \\mu  = g^{-1}(\\eta) = h(\\eta).\n\\tag{6.2}\\]\nHere, \\(g(\\mu)\\) is called the link function, and \\(h(\\eta)\\) is called the inverse link function."
  },
  {
    "objectID": "6_smoothinginpractice.html#penalized-deviance",
    "href": "6_smoothinginpractice.html#penalized-deviance",
    "title": "6  General Additive Models",
    "section": "6.2 Penalized deviance",
    "text": "6.2 Penalized deviance\nThe spline theory in the previous chapters has assumed Gaussian (normally distributed) data and the identity link function. For non-Gaussian data and/or a non-identity link function, we need to replace the penalized least-squares criterion of Equation 4.2 with a penalized deviance: \\[\nR_{\\nu}(f, \\lambda, \\boldsymbol{\\beta})\n=\nD({\\mathbf y}, f,\\boldsymbol{\\beta})  + \\lambda \\, J_\\nu(f),\n\\tag{6.3}\\] where \\(D({\\mathbf y}, f,\\boldsymbol{\\beta})\\) is the deviance for the vector \\(\\mathbf{y}\\) of observations modeled by a linear predictor that comprises a spline function \\(f(t)\\) of order \\(\\nu\\) and possibly also covariate main-effects and interactions. The penalized deviance is then minimized with respect to the spline coefficients \\(\\mathbf{a}\\), \\(\\mathbf{b}\\) and regression parameters \\(\\boldsymbol{\\beta}\\), if any. Note that the fitted values for \\(y\\) are obtained by applying the inverse link function to the linear predictor \\(f(t)\\), for example the logistic function when the link is logit.\nWhen there are several smooth terms of order \\(\\nu\\) in the model, \\({\\mathbf f} = \\{f_1,\\dots,f_m\\}\\), each may be assigned its own roughness penalty, \\(\\lambda_h\\), and Equation 6.3 becomes \\[\nR_{\\nu}(f_1,\\dots,f_m, \\lambda_1,\\dots, \\lambda_m, \\boldsymbol{\\beta}) =\nD({\\mathbf y}, f_1,\\dots,f_m, \\boldsymbol{\\beta})  + \\sum_{h=1}^m \\lambda_h J_\\nu(f_h)\n\\tag{6.4}\\] or we may choose to write this as \\[\nR_{\\nu}({\\mathbf f}, {\\boldsymbol \\lambda}, \\boldsymbol{\\beta}) =\nD({\\mathbf y}, {\\mathbf f}, \\boldsymbol{\\beta})  + \\sum_{h=1}^m \\lambda_h J_\\nu(f_h).\n\\]"
  },
  {
    "objectID": "6_smoothinginpractice.html#gams-in-r",
    "href": "6_smoothinginpractice.html#gams-in-r",
    "title": "6  General Additive Models",
    "section": "6.3 GAMs in R",
    "text": "6.3 GAMs in R\nFitting smoothing splines is straightforward in practice using R. At the beginning of each R session, the first step is to load the package \\(\\texttt{mgcv}\\) which makes available a set of routines written by Simon Wood of the University of Bath.\nThe main command is \\(\\texttt{gam}\\) which fits a smoothing spline (or, more generally, a general additive model). The \\(\\texttt{gam}\\) function is an extension to the \\(\\texttt{glm}\\) command for fitting generalised linear models, to allow nonparametric functions of explanatory variables.\nThe syntax of the \\(\\texttt{gam}\\) command is similar to that of \\(\\texttt{glm}\\). Suppose \\(\\texttt{y}\\) is a vector of length containing observations of a dependent variable and is another vector of length containing the times of those observations. Then each of the commands\n\n\nCode\nout=gam(y~s(tt,fx=FALSE,k=6,sp=3.5))\nout=gam(y~s(tt,fx=FALSE,k=6))\n\n\nfits a cubic smoothing spline to the dependent variable \\(\\texttt{y}\\). In the first version above, the user explicitly sets the smoothing parameter \\(\\lambda\\) (here denoted \\(\\texttt{sp}\\)) to the value \\(3.5\\). In the second version, the optimal value of \\(\\lambda\\) is chosen by the routine to minimise the Generalised Cross-Validation criterion, GCV. The notation \\(\\texttt{s(tt)}\\) means a smooth function (a cubic smoothing spline in this setting) of the explanatory variable \\(\\texttt{tt}\\). This notation can be viewed as an extension of the model notation used by the \\(\\texttt{glm}\\) function. Setting \\(\\texttt{fx=FALSE}\\) in function \\(\\texttt{s}\\) specifies that the dimensionality of the spline should be free (see later notes). Parameter \\(\\texttt{k}\\) of function \\(\\texttt{s}\\) specifies the maximum dimensionality of the spline, and should be set according to the problem and data at hand since the default value will not be appropriate in general. For example, if \\(\\texttt{tt}\\) contains only 6 distinct values, then it would be appropriate to set \\(\\texttt{k=6}\\).\nIn each of the above examples, output from \\(\\texttt{gam}\\) is stored in an object called \\(\\texttt{out}\\). This object contains several components of interest:\n\n\\(\\texttt{out\\$fitted.values}\\) is a vector of length \\(\\texttt{n}\\) containing the fitted values of the smoothing spline at the data values.\n\\(\\texttt{out\\$gcv.ubre}\\) contains the value of the GCV criterion.\n\\(\\texttt{out\\$hat}\\) contains the diagonal values of the smoothing matrix, also called the hat matrix. The total degrees of freedom in the model (including the intercept term) is given by \\(\\texttt{sum(out\\$hat)}\\).\n\\(\\texttt{out\\$sp}\\) contains the smoothing parameter value.\n\\(\\texttt{summary.gam(out)}\\) provides a summary of the smoothed model fit.\n\\(\\texttt{anova.gam(out)}\\) provides an analysis of deviance for the model.\n\nThus, if \\(\\texttt{gam}\\) is called without an explicit choice of \\(\\texttt{sp}\\) (as in the second example above), the output from \\(\\texttt{gam}\\) gives the optimal smoothing parameter value and the corresponding value of the GCV criterion.\nTo plot a smoothing spline:\n\ndefine a dense set of times spanning the data, at which to plot the smooth curve;\nuse the \\(\\texttt{predict.gam}\\) function to compute the cubic spline at these values.\n\nFor example, suppose \\(\\texttt{y}\\) is a vector of length 20 containing the responses at times \\(1,\\dots,20\\).\n\n\nCode\n# observation times\ntt=1:20\n# compute a dense set of numbers between 0 and 21: 0,0.1,...,20.9,21.0\nttnew = (0:210)/10\n# compute predicted values at each of 0,0.1,...,20.9,21.0\npred=predict.gam(out,newdata=list(tt=ttnew))\n# plot the original data\nplot(tt,y)\n# superimpose the smoothing spline\nlines(ttnew,pred)"
  },
  {
    "objectID": "6_smoothinginpractice.html#coronary-heart-disease-chd-in-south-africa",
    "href": "6_smoothinginpractice.html#coronary-heart-disease-chd-in-south-africa",
    "title": "6  General Additive Models",
    "section": "6.4 Coronary heart disease (CHD) in South Africa",
    "text": "6.4 Coronary heart disease (CHD) in South Africa\nThe textbook Elements of Statistical Learning, by Hastie, Tibshirani and Friedman (2nd Edn, 2011), refers to a case–control study of coronary heart disease (CHD) in South Africa. \n\n\nCode\nhr &lt;- read.table(\"https://richardpmann.com/MATH5824/Datasets/SAheart.txt\", sep=\",\",head=T,row.names=1)\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\n\\(\\texttt{tobacco}\\)\ncumulative tobacco consumption (kg)\n\n\n\\(\\texttt{famhist}\\)\nfamily history of heart disease (Present, Absent)\n\n\n\\(\\texttt{age}\\)\nage at onset of the disease (years)\n\n\n\\(\\texttt{chd}\\)\ncase–control status (1 \\(\\Rightarrow\\) CHD; 0 \\(\\Rightarrow\\) no CHD).\n\n\n\nThe dependent variable in our models will be \\(\\texttt{chd}\\). We can consider the CHD status of each individual to be the result of an experiment in which the outcome is either CHD (success) or no CHD (failure). Thus we can model these data using the Binomial distribution, where the Binomial index is 1. The \\(\\texttt{glm}\\) function allows such binary (0/1) dependent variables to be specified directly in the model formula, as in the example below, instead of via the usual two-column matrix of successes and failures.\nWe can examine CHD in relation to tobacco consumption, age and family history of heart disease, with the following R commands:\n\n\nCode\n# make variables in dataframe hr directly available\nattach(hr)\n\n# fit a generalised linear model with chd as dependent variable\nglm1 = glm(formula='chd~tobacco+age+famhist',family='binomial')\n# examine the results\nsummary(glm1)\n\n\n\nCall:\nglm(formula = \"chd~tobacco+age+famhist\", family = \"binomial\")\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -3.620593   0.444576  -8.144 3.83e-16 ***\ntobacco         0.083004   0.025712   3.228  0.00125 ** \nage             0.048812   0.009452   5.164 2.42e-07 ***\nfamhistPresent  0.974791   0.220023   4.430 9.41e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 596.11  on 461  degrees of freedom\nResidual deviance: 495.39  on 458  degrees of freedom\nAIC: 503.39\n\nNumber of Fisher Scoring iterations: 4\n\n\nAll variables in this model are statistically significant: disease is positively related to the amount of tobacco consumed, age and family history of heart disease. However, this model assumes that the logit of the probability of disease is linearly related to both tobacco consumption and age (logit being the default link for Binomial). We can explore more flexible tobacco-consumption and age trends with the following generalised additive model:\n\n\nCode\nlibrary(mgcv)\n\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-0. For overview type 'help(\"mgcv-package\")'.\n\n\nCode\ngam1 = gam(chd ~ s(tobacco,k=20)+s(age,k=20)+famhist, family='binomial')\nsummary.gam(gam1)\n\n\n\nFamily: binomial \nLink function: logit \n\nFormula:\nchd ~ s(tobacco, k = 20) + s(age, k = 20) + famhist\n\nParametric coefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.2379     0.1631  -7.592 3.15e-14 ***\nfamhistPresent   0.9628     0.2233   4.311 1.62e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n             edf Ref.df Chi.sq  p-value    \ns(tobacco) 6.080  7.573  17.89   0.0179 *  \ns(age)     1.002  1.003  24.11 9.53e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.212   Deviance explained = 19.1%\nUBRE = 0.083268  Scale est. = 1         n = 462\n\n\nHere, the \\(\\texttt{s}\\) function specifies a smooth (cubic spline) dependence. Parameter \\(\\texttt{k}\\) of the \\(\\texttt{s}\\) function specifies the maximum number of degrees of freedom to be allocated to this dependence. Setting \\(\\texttt{k}\\) too small would restrict the set of basis functions used to construct the splines; setting \\(\\texttt{k}\\) too large would increase the computational burden unnecessarily.\nThese results show that the fitted smoothing spline of the dependence of CHD on tobacco consumption has an effective degrees of freedom (edf) of 6.080, while the dependence on age has edf of only 1.002, implying an almost linear age-dependence because a linear age term would have exactly 1 degree of freedom. See Section Section 5.5 for further details on the calculation of edf.\nThe significance of each of these smooth terms is given by the \\(p\\)-value column in the above table. These \\(p\\)-values are computed from the \\(\\chi^2\\) statistics in the previous column whose approximate degrees of freedom are given in the column headed \\(\\texttt{Ref.df}\\). For example, the \\(p\\)-value of 0.0196 for \\(\\texttt{s(tobacco)}\\) is computed by referring 17.62 to the \\(\\chi^2\\) distribution on 7.573 degrees of freedom. Note that this compares the above model with the model which omits \\(\\texttt{tobacco}\\) completely.\nThe following R code was used to plot the fitted smooth functions of age and tobacco consumption:\n\n\nCode\n# for larger axis labels\npar(cex.lab=1.6)\n# create synthetic data set for ages 15 to 65 having consumed\n# no tobacco and with no family history of heart disease\nnewdat1 = data.frame(age = seq(from=15,to=65,by=0.1),tobacco=0,\n          famhist=\"Absent\")\n# predict logit probability of CHD in these synthetic individuals\npred1 = predict.gam(gam1,newdata=newdat1)\n# plot the predicted logit probability of CHD by age in these\n# synthetic individuals\nplot(newdat1$age,pred1,xlab=\"Age\",\n    ylab=\"Predicted logit probability of CHD\", type=\"l\")\n\n\n\n\n\nCode\n# create synthetic data set for age 40 having consumed tobacco\n# ranging from 0 to 32kg and with no family history  of heart disease\nnewdat2 = data.frame(age = 40,tobacco=seq(from=0,to=32,by=0.1),\n          famhist=\"Absent\")\n# predict logit probability of CHD in these synthetic individuals\npred2 = predict.gam(gam1,newdata=newdat2)\n# plot the predicted logit probability of CHD by tobacco in these imaginary individuals\nplot(newdat2$tobacco,pred2,xlab=\"Tobacco consumption\",\n    ylab=\"logit probability of CHD\", type=\"l\")\n\n\n\n\n\nThe predicted dependence of the logit probability of CHD on smooth functions of age and tobacco consumption is shown above. We see an almost linear predicted age-dependence, in agreement with its edf. The curious predicted dependence on tobacco consumption might be explained by other factors correlated with tobacco consumption."
  },
  {
    "objectID": "4_smoothing.html#the-penalised-least-squares-criterion",
    "href": "4_smoothing.html#the-penalised-least-squares-criterion",
    "title": "4  Smoothing Splines",
    "section": "4.2 The penalised least-squares criterion",
    "text": "4.2 The penalised least-squares criterion\nNoting that maximizing Equation 4.1 is equivalent to minimizing the residual sum of squares: \\(\\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2\\), we can achieve this trade-off by minimizing a penalised sum of squared residuals: \\[\nR_\\nu(f,\\lambda) = \\sum_{i=1}^n \\left(y_i - f(x_i)\\right)^2 + \\lambda J_\\nu(f),\n\\tag{4.2}\\] where \\(J_\\nu(f)\\), as first defined in Equation 3.8, penalizes the roughness of \\(f\\). The smoothing parameter \\(\\lambda\\geq 0\\) controls the severity of this penalty. For now we will assume \\(\\lambda\\), which absorbs the \\(\\sigma^2\\) in Equation 4.1, is known.\nFigure 4.2 shows example smoothing spline fits using a range of smoothing parameters, \\(\\lambda\\). In Figure 4.2 (a) the fit is essentially a straight line, perhaps Figure 4.2 (b) and Figure 4.2 (c) show acceptable fits. Figure 4.2 (d), with a very small \\(\\lambda\\) value is close to an interpolating spline fit and is clearly unacceptable.\n\n\n\n\n\n\n\n(a) Very strong smoothing\n\n\n\n\n\n\n\n(b) Moderate smoothing\n\n\n\n\n\n\n\n\n\n(c) Weak smoothing\n\n\n\n\n\n\n\n(d) Very weak smoothing\n\n\n\n\nFigure 4.2: Comparison of interpolating and smoothing methods applied to a noisy dat set.\n\n\nAs the smoothing parameter \\(\\lambda\\) increases, the optimal \\(f\\) becomes smoother. In particular, it can be shown that as \\(\\lambda \\rightarrow \\infty\\), the vector of coefficients \\({\\mathbf b} \\rightarrow {\\mathbf 0}\\), and \\({\\mathbf a}\\) tends to the usual least-squares estimate of the regression parameters. Thus, the smoothing spline converges to the sample mean \\(f(x)=\\bar y\\) when \\(\\nu=1\\), and to the ordinary least squares fitted line, \\(f(x)=\\hat\\alpha+\\hat\\beta t\\), when \\(\\nu=2\\). In the other direction, as \\(\\lambda \\rightarrow 0\\) the smoothing solution converges to the interpolating spline."
  },
  {
    "objectID": "6_smoothinginpractice.html#penalised-deviance",
    "href": "6_smoothinginpractice.html#penalised-deviance",
    "title": "6  General Additive Models",
    "section": "6.2 Penalised deviance",
    "text": "6.2 Penalised deviance\nThe spline theory in the previous chapters has assumed Gaussian (normally distributed) data and the identity link function. For non-Gaussian data and/or a non-identity link function, we need to replace the penalised least-squares criterion of Equation 4.2 with a penalised deviance: \\[\nR_{\\nu}(f, \\lambda, \\boldsymbol{\\beta})\n=\nD({\\mathbf y}, f,\\boldsymbol{\\beta})  + \\lambda \\, J_\\nu(f),\n\\tag{6.3}\\] where \\(D({\\mathbf y}, f,\\boldsymbol{\\beta})\\) is the deviance for the vector \\(\\mathbf{y}\\) of observations modeled by a linear predictor that comprises a spline function \\(f(t)\\) of order \\(\\nu\\) and possibly also covariate main-effects and interactions. The penalised deviance is then minimised with respect to the spline coefficients \\(\\mathbf{a}\\), \\(\\mathbf{b}\\) and regression parameters \\(\\boldsymbol{\\beta}\\), if any. Note that the fitted values for \\(y\\) are obtained by applying the inverse link function to the linear predictor \\(f(t)\\), for example the logistic function when the link is logit.\nWhen there are several smooth terms of order \\(\\nu\\) in the model, \\({\\mathbf f} = \\{f_1,\\dots,f_m\\}\\), each may be assigned its own roughness penalty, \\(\\lambda_h\\), and Equation 6.3 becomes \\[\nR_{\\nu}(f_1,\\dots,f_m, \\lambda_1,\\dots, \\lambda_m, \\boldsymbol{\\beta}) =\nD({\\mathbf y}, f_1,\\dots,f_m, \\boldsymbol{\\beta})  + \\sum_{h=1}^m \\lambda_h J_\\nu(f_h)\n\\tag{6.4}\\] or we may choose to write this as \\[\nR_{\\nu}({\\mathbf f}, {\\boldsymbol \\lambda}, \\boldsymbol{\\beta}) =\nD({\\mathbf y}, {\\mathbf f}, \\boldsymbol{\\beta})  + \\sum_{h=1}^m \\lambda_h J_\\nu(f_h).\n\\]"
  }
]